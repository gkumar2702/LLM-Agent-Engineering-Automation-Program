{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 4: LangGraph\n",
        "\n",
        "## Applied AI Scientist Field Notes - Expanded Edition\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 4: LangGraph - Stateful Workflows and Agent Graphs\n",
        "\n",
        "### Topics\n",
        "1. State management\n",
        "2. Graph-based workflows\n",
        "3. Conditional routing\n",
        "4. Retry and error handling\n",
        "5. Human-in-the-loop (HITL)\n",
        "6. Production deployment\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q langgraph\n",
        "%pip install -q typing-extensions\n",
        "\n",
        "print('LangGraph installed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Stateful Agent Graphs\n",
        "\n",
        "LangGraph enables:\n",
        "- **State persistence** across steps\n",
        "- **Conditional routing** based on state\n",
        "- **Retry logic** for failed steps\n",
        "- **Human approval** gates\n",
        "- **Observability** through state inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Callable, Dict, Any, List\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[str]\n",
        "    current_step: str\n",
        "    attempts: int\n",
        "    data: Dict[str, Any]\n",
        "\n",
        "class StatefulWorkflow:\n",
        "    '''Simplified LangGraph-style workflow engine'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "        self.edges = {}\n",
        "        self.state = AgentState(messages=[], current_step='start', attempts=0, data={})\n",
        "    \n",
        "    def add_node(self, name: str, func: Callable):\n",
        "        self.nodes[name] = func\n",
        "    \n",
        "    def add_edge(self, from_node: str, to_node: str, condition: Callable = None):\n",
        "        if from_node not in self.edges:\n",
        "            self.edges[from_node] = []\n",
        "        self.edges[from_node].append({'to': to_node, 'condition': condition})\n",
        "    \n",
        "    def run(self, initial_input: str, max_steps=10):\n",
        "        self.state['messages'].append(initial_input)\n",
        "        steps = 0\n",
        "        \n",
        "        while self.state['current_step'] != 'end' and steps < max_steps:\n",
        "            current = self.state['current_step']\n",
        "            \n",
        "            if current in self.nodes:\n",
        "                print(f'Step {steps+1}: {current}')\n",
        "                self.nodes[current](self.state)\n",
        "            \n",
        "            next_step = 'end'\n",
        "            if current in self.edges:\n",
        "                for edge in self.edges[current]:\n",
        "                    if edge['condition'] is None or edge['condition'](self.state):\n",
        "                        next_step = edge['to']\n",
        "                        break\n",
        "            \n",
        "            self.state['current_step'] = next_step\n",
        "            steps += 1\n",
        "        \n",
        "        return self.state\n",
        "\n",
        "# Example: Customer Support Workflow\n",
        "def classify(state):\n",
        "    msg = state['messages'][-1].lower()\n",
        "    if 'refund' in msg:\n",
        "        state['data']['intent'] = 'refund'\n",
        "    elif 'track' in msg:\n",
        "        state['data']['intent'] = 'tracking'\n",
        "    else:\n",
        "        state['data']['intent'] = 'general'\n",
        "\n",
        "def handle_refund(state):\n",
        "    state['attempts'] += 1\n",
        "    if state['attempts'] > 2:\n",
        "        state['data']['escalate'] = True\n",
        "    else:\n",
        "        state['data']['processed'] = True\n",
        "\n",
        "def respond(state):\n",
        "    if state['data'].get('escalate'):\n",
        "        state['messages'].append('Escalated to human')\n",
        "    else:\n",
        "        state['messages'].append('Request processed')\n",
        "\n",
        "wf = StatefulWorkflow()\n",
        "wf.add_node('start', classify)\n",
        "wf.add_node('refund', handle_refund)\n",
        "wf.add_node('respond', respond)\n",
        "wf.add_edge('start', 'refund', lambda s: s['data'].get('intent') == 'refund')\n",
        "wf.add_edge('start', 'respond', lambda s: s['data'].get('intent') != 'refund')\n",
        "wf.add_edge('refund', 'respond')\n",
        "wf.add_edge('respond', 'end')\n",
        "\n",
        "result = wf.run('I want a refund')\n",
        "print(f'\\nFinal state: {result[\"messages\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Production Patterns\n",
        "\n",
        "**Best practices:**\n",
        "- Max step limits to prevent infinite loops\n",
        "- State checkpointing for recovery\n",
        "- Observability through structured logging\n",
        "- Retry with exponential backoff\n",
        "- Circuit breakers for external services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Advanced State Management with Checkpointing\n",
        "\n",
        "Production LangGraph systems need:\n",
        "- **State persistence**: Save/restore state across runs\n",
        "- **Checkpointing**: Resume from failures\n",
        "- **State versioning**: Track state evolution\n",
        "- **State inspection**: Debug complex workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import TypedDict, Dict, Any, List, Optional\n",
        "import pickle\n",
        "\n",
        "class CheckpointableState(TypedDict):\n",
        "    '''State with checkpointing support'''\n",
        "    messages: List[Dict[str, str]]\n",
        "    current_step: str\n",
        "    attempts: int\n",
        "    data: Dict[str, Any]\n",
        "    checkpoint_id: str\n",
        "    created_at: str\n",
        "\n",
        "class StateManager:\n",
        "    '''Production state management with persistence'''\n",
        "    \n",
        "    def __init__(self, checkpoint_dir='./checkpoints'):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.state_history = []\n",
        "    \n",
        "    def create_checkpoint(self, state: CheckpointableState) -> str:\n",
        "        '''Save state checkpoint'''\n",
        "        checkpoint_id = hashlib.md5(\n",
        "            f\"{datetime.utcnow().isoformat()}{state['current_step']}\".encode()\n",
        "        ).hexdigest()[:16]\n",
        "        \n",
        "        checkpoint = {\n",
        "            'id': checkpoint_id,\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "            'state': state.copy(),\n",
        "        }\n",
        "        \n",
        "        # Save to disk\n",
        "        filename = f\"{self.checkpoint_dir}/{checkpoint_id}.json\"\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "        \n",
        "        self.state_history.append(checkpoint)\n",
        "        return checkpoint_id\n",
        "    \n",
        "    def restore_checkpoint(self, checkpoint_id: str) -> Optional[CheckpointableState]:\n",
        "        '''Restore state from checkpoint'''\n",
        "        filename = f\"{self.checkpoint_dir}/{checkpoint_id}.json\"\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                checkpoint = json.load(f)\n",
        "            return checkpoint['state']\n",
        "        except FileNotFoundError:\n",
        "            return None\n",
        "    \n",
        "    def get_state_history(self) -> List[dict]:\n",
        "        '''Get complete state evolution'''\n",
        "        return self.state_history\n",
        "    \n",
        "    def rollback_to_step(self, step_name: str) -> Optional[CheckpointableState]:\n",
        "        '''Rollback to specific step'''\n",
        "        for checkpoint in reversed(self.state_history):\n",
        "            if checkpoint['state']['current_step'] == step_name:\n",
        "                return checkpoint['state']\n",
        "        return None\n",
        "\n",
        "class ProductionGraph:\n",
        "    '''LangGraph with production features'''\n",
        "    \n",
        "    def __init__(self, enable_checkpointing=True):\n",
        "        self.nodes = {}\n",
        "        self.edges = {}\n",
        "        self.state_manager = StateManager() if enable_checkpointing else None\n",
        "        self.execution_log = []\n",
        "    \n",
        "    def add_node(self, name: str, func: Callable):\n",
        "        '''Add node with error handling'''\n",
        "        self.nodes[name] = func\n",
        "    \n",
        "    def add_conditional_edge(self, from_node: str, routing_func: Callable):\n",
        "        '''Add edge with dynamic routing'''\n",
        "        self.edges[from_node] = {'type': 'conditional', 'router': routing_func}\n",
        "    \n",
        "    def add_edge(self, from_node: str, to_node: str):\n",
        "        '''Add fixed edge'''\n",
        "        self.edges[from_node] = {'type': 'fixed', 'to': to_node}\n",
        "    \n",
        "    def run(self, initial_state: CheckpointableState, max_steps=20) -> CheckpointableState:\n",
        "        '''Execute graph with checkpointing'''\n",
        "        state = initial_state\n",
        "        \n",
        "        for step_num in range(max_steps):\n",
        "            current_node = state['current_step']\n",
        "            \n",
        "            # Checkpoint before execution\n",
        "            if self.state_manager:\n",
        "                checkpoint_id = self.state_manager.create_checkpoint(state)\n",
        "                state['checkpoint_id'] = checkpoint_id\n",
        "            \n",
        "            # Execute node\n",
        "            try:\n",
        "                if current_node in self.nodes:\n",
        "                    print(f'Executing: {current_node}')\n",
        "                    self.nodes[current_node](state)\n",
        "                    \n",
        "                    self.execution_log.append({\n",
        "                        'step': step_num + 1,\n",
        "                        'node': current_node,\n",
        "                        'status': 'success',\n",
        "                        'timestamp': datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f'Error in {current_node}: {e}')\n",
        "                self.execution_log.append({\n",
        "                    'step': step_num + 1,\n",
        "                    'node': current_node,\n",
        "                    'status': 'error',\n",
        "                    'error': str(e),\n",
        "                    'timestamp': datetime.utcnow().isoformat()\n",
        "                })\n",
        "                \n",
        "                # Retry or escalate\n",
        "                state['attempts'] += 1\n",
        "                if state['attempts'] >= 3:\n",
        "                    state['current_step'] = 'human_review'\n",
        "                continue\n",
        "            \n",
        "            # Determine next step\n",
        "            if current_node in self.edges:\n",
        "                edge = self.edges[current_node]\n",
        "                if edge['type'] == 'conditional':\n",
        "                    state['current_step'] = edge['router'](state)\n",
        "                else:\n",
        "                    state['current_step'] = edge['to']\n",
        "            else:\n",
        "                state['current_step'] = 'end'\n",
        "            \n",
        "            # Check termination\n",
        "            if state['current_step'] == 'end':\n",
        "                break\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def visualize_execution(self) -> str:\n",
        "        '''Generate execution trace for debugging'''\n",
        "        trace = '\\nExecution Trace:\\n' + '=' * 60 + '\\n'\n",
        "        for entry in self.execution_log:\n",
        "            status_icon = '✓' if entry['status'] == 'success' else '✗'\n",
        "            trace += f\"{status_icon} Step {entry['step']}: {entry['node']}\"\n",
        "            if entry['status'] == 'error':\n",
        "                trace += f\" - Error: {entry['error']}\"\n",
        "            trace += '\\n'\n",
        "        return trace\n",
        "\n",
        "# Example: Complex workflow with checkpointing\n",
        "print('PRODUCTION LANGGRAPH WITH CHECKPOINTING')\n",
        "print('=' * 90)\n",
        "\n",
        "def analyze_input(state):\n",
        "    '''Analyze user input'''\n",
        "    message = state['messages'][-1]['content']\n",
        "    state['data']['complexity'] = 'high' if len(message) > 100 else 'low'\n",
        "    state['data']['requires_tools'] = 'code' in message.lower()\n",
        "\n",
        "def route_by_complexity(state) -> str:\n",
        "    '''Route based on complexity'''\n",
        "    if state['data']['complexity'] == 'high':\n",
        "        return 'detailed_processing'\n",
        "    else:\n",
        "        return 'quick_processing'\n",
        "\n",
        "def detailed_processing(state):\n",
        "    '''Handle complex queries'''\n",
        "    state['data']['processed'] = True\n",
        "    state['data']['detail_level'] = 'comprehensive'\n",
        "\n",
        "def quick_processing(state):\n",
        "    '''Handle simple queries'''\n",
        "    state['data']['processed'] = True\n",
        "    state['data']['detail_level'] = 'basic'\n",
        "\n",
        "def generate_response(state):\n",
        "    '''Generate final response'''\n",
        "    detail = state['data'].get('detail_level', 'basic')\n",
        "    state['messages'].append({\n",
        "        'role': 'assistant',\n",
        "        'content': f'Response generated with {detail} detail level'\n",
        "    })\n",
        "\n",
        "# Build graph\n",
        "graph = ProductionGraph(enable_checkpointing=True)\n",
        "graph.add_node('analyze', analyze_input)\n",
        "graph.add_node('detailed_processing', detailed_processing)\n",
        "graph.add_node('quick_processing', quick_processing)\n",
        "graph.add_node('generate', generate_response)\n",
        "\n",
        "graph.add_edge('analyze', 'routing')\n",
        "graph.add_conditional_edge('routing', route_by_complexity)\n",
        "graph.add_edge('detailed_processing', 'generate')\n",
        "graph.add_edge('quick_processing', 'generate')\n",
        "graph.add_edge('generate', 'end')\n",
        "\n",
        "# Test with different complexity\n",
        "for test_input in ['Simple question', 'Complex query about implementing a distributed system with fault tolerance and high availability']:\n",
        "    print(f'\\n--- Testing: {test_input[:50]}... ---')\n",
        "    \n",
        "    initial_state = CheckpointableState(\n",
        "        messages=[{'role': 'user', 'content': test_input}],\n",
        "        current_step='analyze',\n",
        "        attempts=0,\n",
        "        data={},\n",
        "        checkpoint_id='',\n",
        "        created_at=datetime.utcnow().isoformat()\n",
        "    )\n",
        "    \n",
        "    final_state = graph.run(initial_state, max_steps=10)\n",
        "    \n",
        "    print(f'Complexity: {final_state[\"data\"].get(\"complexity\")}')\n",
        "    print(f'Detail level: {final_state[\"data\"].get(\"detail_level\")}')\n",
        "    print(f'Checkpoints created: {len(graph.state_manager.state_history)}')\n",
        "\n",
        "print('\\n' + graph.visualize_execution())\n",
        "print('=' * 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Human-in-the-Loop (HITL) Patterns\n",
        "\n",
        "Critical for production:\n",
        "- **Approval gates**: Require human approval for risky actions\n",
        "- **Review queues**: Batch similar requests for efficient review\n",
        "- **Escalation logic**: Auto-escalate based on confidence/risk\n",
        "- **Async approval**: Don't block on human review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class ApprovalStatus(Enum):\n",
        "    PENDING = 'pending'\n",
        "    APPROVED = 'approved'\n",
        "    REJECTED = 'rejected'\n",
        "    TIMEOUT = 'timeout'\n",
        "\n",
        "class HITLRequest:\n",
        "    '''Human-in-the-loop approval request'''\n",
        "    def __init__(self, request_id: str, action: str, context: dict, risk_level: str):\n",
        "        self.request_id = request_id\n",
        "        self.action = action\n",
        "        self.context = context\n",
        "        self.risk_level = risk_level\n",
        "        self.status = ApprovalStatus.PENDING\n",
        "        self.reviewer_notes = ''\n",
        "        self.created_at = datetime.utcnow()\n",
        "\n",
        "class HITLManager:\n",
        "    '''Manage human-in-the-loop approvals'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.pending_requests = {}\n",
        "        self.approval_queue = queue.Queue()\n",
        "        self.auto_approve_threshold = 0.9  # Confidence threshold\n",
        "    \n",
        "    def request_approval(self, \n",
        "                        action: str, \n",
        "                        context: dict, \n",
        "                        risk_level: str,\n",
        "                        confidence: float = 0.5,\n",
        "                        timeout_seconds: int = 300) -> ApprovalStatus:\n",
        "        '''Request human approval for action'''\n",
        "        \n",
        "        # Auto-approve low-risk, high-confidence actions\n",
        "        if risk_level == 'low' and confidence >= self.auto_approve_threshold:\n",
        "            print(f'Auto-approved: {action} (confidence: {confidence:.2f})')\n",
        "            return ApprovalStatus.APPROVED\n",
        "        \n",
        "        # Create approval request\n",
        "        request_id = hashlib.md5(f\"{action}{time.time()}\".encode()).hexdigest()[:16]\n",
        "        request = HITLRequest(request_id, action, context, risk_level)\n",
        "        \n",
        "        self.pending_requests[request_id] = request\n",
        "        self.approval_queue.put(request)\n",
        "        \n",
        "        print(f'Awaiting human approval for: {action} (risk: {risk_level})')\n",
        "        \n",
        "        # In production, this would:\n",
        "        # 1. Send to review queue/UI\n",
        "        # 2. Wait for human response via webhook/polling\n",
        "        # 3. Timeout if no response\n",
        "        \n",
        "        # Simulated approval (instant for demo)\n",
        "        return self._simulate_human_decision(request, timeout_seconds)\n",
        "    \n",
        "    def _simulate_human_decision(self, request: HITLRequest, timeout: int) -> ApprovalStatus:\n",
        "        '''Simulate human reviewer decision'''\n",
        "        # In production, this would wait for actual human input\n",
        "        \n",
        "        # Simulation logic\n",
        "        if request.risk_level == 'high':\n",
        "            # High risk requires careful review\n",
        "            time.sleep(0.1)  # Simulate thinking\n",
        "            request.status = ApprovalStatus.APPROVED if 'safe' in request.action.lower() else ApprovalStatus.REJECTED\n",
        "        elif request.risk_level == 'medium':\n",
        "            request.status = ApprovalStatus.APPROVED\n",
        "        else:\n",
        "            request.status = ApprovalStatus.APPROVED\n",
        "        \n",
        "        return request.status\n",
        "    \n",
        "    def get_pending_reviews(self) -> List[HITLRequest]:\n",
        "        '''Get all pending approval requests'''\n",
        "        return [r for r in self.pending_requests.values() if r.status == ApprovalStatus.PENDING]\n",
        "\n",
        "class HITLGraph:\n",
        "    '''LangGraph with human-in-the-loop approval gates'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "        self.edges = {}\n",
        "        self.hitl_manager = HITLManager()\n",
        "    \n",
        "    def add_node(self, name: str, func: Callable, requires_approval: bool = False, risk_level: str = 'low'):\n",
        "        '''Add node with optional approval requirement'''\n",
        "        self.nodes[name] = {\n",
        "            'func': func,\n",
        "            'requires_approval': requires_approval,\n",
        "            'risk_level': risk_level\n",
        "        }\n",
        "    \n",
        "    def run(self, state: dict, max_steps=10):\n",
        "        '''Execute graph with HITL gates'''\n",
        "        for step_num in range(max_steps):\n",
        "            current_node = state['current_step']\n",
        "            \n",
        "            if current_node == 'end':\n",
        "                break\n",
        "            \n",
        "            if current_node not in self.nodes:\n",
        "                print(f'Unknown node: {current_node}')\n",
        "                break\n",
        "            \n",
        "            node_config = self.nodes[current_node]\n",
        "            \n",
        "            # Check if approval required\n",
        "            if node_config['requires_approval']:\n",
        "                approval = self.hitl_manager.request_approval(\n",
        "                    action=current_node,\n",
        "                    context=state.get('data', {}),\n",
        "                    risk_level=node_config['risk_level'],\n",
        "                    confidence=state.get('confidence', 0.5)\n",
        "                )\n",
        "                \n",
        "                if approval != ApprovalStatus.APPROVED:\n",
        "                    print(f'Action {current_node} was {approval.value}')\n",
        "                    state['current_step'] = 'rejected'\n",
        "                    break\n",
        "            \n",
        "            # Execute node\n",
        "            print(f'Executing: {current_node}')\n",
        "            node_config['func'](state)\n",
        "            \n",
        "            # Move to next step (simplified routing)\n",
        "            if current_node in self.edges:\n",
        "                state['current_step'] = self.edges[current_node]\n",
        "            else:\n",
        "                state['current_step'] = 'end'\n",
        "        \n",
        "        return state\n",
        "\n",
        "# Example: Data deletion workflow with HITL\n",
        "print('\\nHUMAN-IN-THE-LOOP WORKFLOW')\n",
        "print('=' * 90)\n",
        "\n",
        "def validate_request(state):\n",
        "    state['data']['validated'] = True\n",
        "    state['confidence'] = 0.8\n",
        "\n",
        "def execute_deletion(state):\n",
        "    '''High-risk action: delete data'''\n",
        "    state['data']['deleted'] = True\n",
        "    print('  [CRITICAL] Data deleted')\n",
        "\n",
        "def send_confirmation(state):\n",
        "    state['data']['confirmation_sent'] = True\n",
        "\n",
        "hitl_graph = HITLGraph()\n",
        "\n",
        "# Low-risk node\n",
        "hitl_graph.add_node('validate', validate_request, requires_approval=False)\n",
        "\n",
        "# High-risk node requiring approval\n",
        "hitl_graph.add_node('delete', execute_deletion, requires_approval=True, risk_level='high')\n",
        "\n",
        "# Low-risk node\n",
        "hitl_graph.add_node('confirm', send_confirmation, requires_approval=False)\n",
        "\n",
        "hitl_graph.edges = {\n",
        "    'validate': 'delete',\n",
        "    'delete': 'confirm',\n",
        "    'confirm': 'end'\n",
        "}\n",
        "\n",
        "# Test workflow\n",
        "initial_state = {\n",
        "    'current_step': 'validate',\n",
        "    'data': {'user_id': '12345'},\n",
        "    'confidence': 0.8\n",
        "}\n",
        "\n",
        "final_state = hitl_graph.run(initial_state)\n",
        "\n",
        "print(f'\\nFinal state: {final_state[\"current_step\"]}')\n",
        "print(f'Data deleted: {final_state[\"data\"].get(\"deleted\", False)}')\n",
        "print('=' * 90)\n",
        "print('\\nKEY INSIGHT: HITL is essential for high-stakes actions')\n",
        "print('  - Auto-approve low-risk, high-confidence actions')\n",
        "print('  - Always require review for irreversible operations')\n",
        "print('  - Set appropriate timeouts')\n",
        "print('  - Track approval metrics (approval rate, review time)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Questions: LangGraph Production Systems\n",
        "\n",
        "### For Senior/Staff Engineers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "langgraph_interview_questions = [\n",
        "    {\n",
        "        'level': 'Senior',\n",
        "        'question': 'Your LangGraph workflow fails 5% of the time at step 3 (of 7) due to external API timeouts. Users have to restart from the beginning. Design a robust recovery system.',\n",
        "        'answer': '''\n",
        "**Problem Analysis:**\n",
        "- Step 3 failure rate: 5%\n",
        "- No checkpointing → restart from beginning\n",
        "- Wasted computation: Steps 1-2 repeated\n",
        "- Poor UX: User frustration\n",
        "- Cost: Unnecessary LLM calls\n",
        "\n",
        "**Solution Architecture:**\n",
        "\n",
        "**1. Checkpoint-Based Recovery**\n",
        "```python\n",
        "import pickle\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class CheckpointManager:\n",
        "    def __init__(self, storage_backend='redis'):\n",
        "        self.storage = Redis() if storage_backend == 'redis' else {}\n",
        "        self.checkpoint_ttl = 3600  # 1 hour\n",
        "    \n",
        "    def save_checkpoint(self, workflow_id: str, step: str, state: dict) -> str:\n",
        "        '''Save state at each step'''\n",
        "        checkpoint = {\n",
        "            'workflow_id': workflow_id,\n",
        "            'step': step,\n",
        "            'state': state,\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "        }\n",
        "        \n",
        "        key = f'checkpoint:{workflow_id}:{step}'\n",
        "        \n",
        "        # Serialize state\n",
        "        serialized = pickle.dumps(checkpoint)\n",
        "        \n",
        "        # Save with TTL\n",
        "        self.storage.setex(key, self.checkpoint_ttl, serialized)\n",
        "        \n",
        "        return key\n",
        "    \n",
        "    def load_checkpoint(self, workflow_id: str, step: str = None) -> dict:\n",
        "        '''Restore state from checkpoint'''\n",
        "        if step:\n",
        "            key = f'checkpoint:{workflow_id}:{step}'\n",
        "            checkpoint = self.storage.get(key)\n",
        "            if checkpoint:\n",
        "                return pickle.loads(checkpoint)\n",
        "        else:\n",
        "            # Find latest checkpoint\n",
        "            pattern = f'checkpoint:{workflow_id}:*'\n",
        "            keys = self.storage.keys(pattern)\n",
        "            if keys:\n",
        "                # Get most recent\n",
        "                latest = max(keys, key=lambda k: self.storage.ttl(k))\n",
        "                return pickle.loads(self.storage.get(latest))\n",
        "        \n",
        "        return None\n",
        "\n",
        "class ResilientGraph:\n",
        "    '''LangGraph with automatic recovery'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.checkpoint_mgr = CheckpointManager()\n",
        "        self.nodes = {}\n",
        "        self.max_retries = 3\n",
        "    \n",
        "    def add_node(self, name: str, func: Callable, \n",
        "                 idempotent: bool = True, \n",
        "                 retry_strategy: str = 'exponential_backoff'):\n",
        "        '''Add node with retry configuration'''\n",
        "        self.nodes[name] = {\n",
        "            'func': func,\n",
        "            'idempotent': idempotent,\n",
        "            'retry_strategy': retry_strategy\n",
        "        }\n",
        "    \n",
        "    def execute_node_with_retry(self, node_name: str, state: dict, workflow_id: str):\n",
        "        '''Execute node with automatic retry and checkpointing'''\n",
        "        node = self.nodes[node_name]\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                # Save checkpoint before execution\n",
        "                self.checkpoint_mgr.save_checkpoint(workflow_id, node_name, state)\n",
        "                \n",
        "                # Execute\n",
        "                result = node['func'](state)\n",
        "                \n",
        "                # Success - update state\n",
        "                state.update(result or {})\n",
        "                \n",
        "                return state, True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f'Attempt {attempt + 1} failed: {e}')\n",
        "                \n",
        "                # Last attempt\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return state, False\n",
        "                \n",
        "                # Retry with backoff\n",
        "                if node['retry_strategy'] == 'exponential_backoff':\n",
        "                    wait_time = 2 ** attempt  # 1s, 2s, 4s\n",
        "                    time.sleep(wait_time)\n",
        "                \n",
        "                # Check if idempotent\n",
        "                if not node['idempotent']:\n",
        "                    # Non-idempotent operations need special handling\n",
        "                    # Check if operation actually succeeded\n",
        "                    if self._verify_operation(node_name, state):\n",
        "                        print(f'Operation succeeded despite error (idempotency check)')\n",
        "                        return state, True\n",
        "        \n",
        "        return state, False\n",
        "    \n",
        "    def run_with_recovery(self, workflow_id: str, initial_state: dict, resume: bool = False):\n",
        "        '''Execute workflow with automatic recovery'''\n",
        "        \n",
        "        if resume:\n",
        "            # Attempt to restore from checkpoint\n",
        "            checkpoint = self.checkpoint_mgr.load_checkpoint(workflow_id)\n",
        "            if checkpoint:\n",
        "                print(f\"Resuming from checkpoint: {checkpoint['step']}\")\n",
        "                state = checkpoint['state']\n",
        "                start_step = checkpoint['step']\n",
        "            else:\n",
        "                print('No checkpoint found, starting from beginning')\n",
        "                state = initial_state\n",
        "                start_step = None\n",
        "        else:\n",
        "            state = initial_state\n",
        "            start_step = None\n",
        "        \n",
        "        # Execute workflow\n",
        "        step_sequence = ['step1', 'step2', 'step3', 'step4', 'step5', 'step6', 'step7']\n",
        "        \n",
        "        # Skip to resume point\n",
        "        if start_step:\n",
        "            step_sequence = step_sequence[step_sequence.index(start_step):]\n",
        "        \n",
        "        for step in step_sequence:\n",
        "            print(f'\\nExecuting: {step}')\n",
        "            state, success = self.execute_node_with_retry(step, state, workflow_id)\n",
        "            \n",
        "            if not success:\n",
        "                print(f'Step {step} failed after {self.max_retries} retries')\n",
        "                print(f'Workflow can be resumed with workflow_id: {workflow_id}')\n",
        "                return {'status': 'failed', 'last_step': step, 'state': state}\n",
        "        \n",
        "        # Clean up checkpoints on success\n",
        "        self._cleanup_checkpoints(workflow_id)\n",
        "        \n",
        "        return {'status': 'success', 'state': state}\n",
        "```\n",
        "\n",
        "**2. Compensating Transactions (for non-idempotent operations)**\n",
        "```python\n",
        "class CompensatingTransaction:\n",
        "    '''Handle rollback for failed workflows'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.compensation_log = []\n",
        "    \n",
        "    def record_action(self, step: str, action: dict, compensation: Callable):\n",
        "        '''Record action with its compensation function'''\n",
        "        self.compensation_log.append({\n",
        "            'step': step,\n",
        "            'action': action,\n",
        "            'compensation': compensation,\n",
        "            'timestamp': datetime.utcnow()\n",
        "        })\n",
        "    \n",
        "    def rollback(self, from_step: str = None):\n",
        "        '''Rollback actions from failed step backwards'''\n",
        "        print('Starting rollback...')\n",
        "        \n",
        "        # Reverse order\n",
        "        for entry in reversed(self.compensation_log):\n",
        "            if from_step and entry['step'] == from_step:\n",
        "                break\n",
        "            \n",
        "            try:\n",
        "                print(f\"Compensating: {entry['step']}\")\n",
        "                entry['compensation'](entry['action'])\n",
        "            except Exception as e:\n",
        "                print(f\"Compensation failed for {entry['step']}: {e}\")\n",
        "                # Log for manual intervention\n",
        "```\n",
        "\n",
        "**3. Progress Tracking for UX**\n",
        "```python\n",
        "class ProgressTracker:\n",
        "    '''Track and display workflow progress to users'''\n",
        "    \n",
        "    def __init__(self, total_steps: int):\n",
        "        self.total_steps = total_steps\n",
        "        self.current_step = 0\n",
        "        self.step_details = {}\n",
        "    \n",
        "    def update(self, step_name: str, status: str, message: str = ''):\n",
        "        '''Update progress'''\n",
        "        self.current_step += 1\n",
        "        self.step_details[step_name] = {\n",
        "            'status': status,\n",
        "            'message': message,\n",
        "            'timestamp': datetime.utcnow().isoformat()\n",
        "        }\n",
        "        \n",
        "        # Send to frontend\n",
        "        progress_pct = (self.current_step / self.total_steps) * 100\n",
        "        self._emit_progress_event({\n",
        "            'step': step_name,\n",
        "            'progress': progress_pct,\n",
        "            'status': status,\n",
        "            'message': message\n",
        "        })\n",
        "    \n",
        "    def _emit_progress_event(self, data: dict):\n",
        "        '''Emit progress to frontend (WebSocket, SSE, polling)'''\n",
        "        # In production: send via WebSocket\n",
        "        print(f\"Progress: {data['progress']:.0f}% - {data['step']} - {data['status']}\")\n",
        "```\n",
        "\n",
        "**4. Complete Solution**\n",
        "```python\n",
        "class ProductionWorkflow:\n",
        "    '''Production workflow with full recovery support'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.graph = ResilientGraph()\n",
        "        self.progress = ProgressTracker(total_steps=7)\n",
        "        self.compensator = CompensatingTransaction()\n",
        "    \n",
        "    def execute(self, workflow_id: str, initial_state: dict, resume: bool = False):\n",
        "        '''Execute with recovery and progress tracking'''\n",
        "        \n",
        "        try:\n",
        "            # Run workflow\n",
        "            result = self.graph.run_with_recovery(workflow_id, initial_state, resume)\n",
        "            \n",
        "            if result['status'] == 'success':\n",
        "                self.progress.update('complete', 'success', 'Workflow completed')\n",
        "                return result\n",
        "            else:\n",
        "                # Failed but recoverable\n",
        "                self.progress.update(result['last_step'], 'failed', 'Step failed, can resume')\n",
        "                return result\n",
        "                \n",
        "        except Exception as e:\n",
        "            # Unrecoverable error - rollback\n",
        "            print(f'Unrecoverable error: {e}')\n",
        "            self.compensator.rollback()\n",
        "            raise\n",
        "```\n",
        "\n",
        "**Results:**\n",
        "- **Retry success rate**: 5% failure → 0.125% failure (97.5% improvement)\n",
        "  - With 3 retries and 5% failure rate per attempt: 0.05^3 = 0.000125 = 0.0125%\n",
        "- **User experience**: Resume from failure point, not restart\n",
        "- **Cost savings**: No repeated work for steps 1-2\n",
        "- **Observability**: Progress tracking, checkpoint history\n",
        "\n",
        "**Production Checklist:**\n",
        "- [x] Checkpoint every step\n",
        "- [x] Retry with exponential backoff\n",
        "- [x] Idempotency checks\n",
        "- [x] Compensating transactions\n",
        "- [x] Progress tracking\n",
        "- [x] TTL for checkpoints (prevent storage bloat)\n",
        "- [x] Monitoring/alerting on high retry rates\n",
        "        ''',\n",
        "    },\n",
        "    {\n",
        "        'level': 'Staff',\n",
        "        'question': 'Design a LangGraph system that can handle 10,000 concurrent workflows, with state persistence, distributed execution, and sub-second routing decisions. Include architectural diagrams and trade-offs.',\n",
        "        'answer': '''\n",
        "**High-Scale LangGraph Architecture:**\n",
        "\n",
        "**1. Distributed State Store**\n",
        "```python\n",
        "import redis\n",
        "from redis.cluster import RedisCluster\n",
        "\n",
        "class DistributedStateStore:\n",
        "    '''Scalable state storage with sharding'''\n",
        "    \n",
        "    def __init__(self, redis_cluster_nodes: List[dict]):\n",
        "        # Redis Cluster for horizontal scaling\n",
        "        self.cluster = RedisCluster(\n",
        "            startup_nodes=redis_cluster_nodes,\n",
        "            decode_responses=False,\n",
        "            skip_full_coverage_check=True\n",
        "        )\n",
        "        \n",
        "        # Local cache for hot states\n",
        "        self.local_cache = LRUCache(maxsize=1000)\n",
        "    \n",
        "    def save_state(self, workflow_id: str, state: dict):\n",
        "        '''Save state with automatic sharding'''\n",
        "        # Serialize\n",
        "        state_bytes = pickle.dumps(state)\n",
        "        \n",
        "        # Compress for large states\n",
        "        if len(state_bytes) > 1024:  # 1KB\n",
        "            import zlib\n",
        "            state_bytes = zlib.compress(state_bytes)\n",
        "        \n",
        "        # Save to cluster (auto-sharded by workflow_id)\n",
        "        self.cluster.setex(\n",
        "            f'state:{workflow_id}',\n",
        "            3600,  # 1 hour TTL\n",
        "            state_bytes\n",
        "        )\n",
        "        \n",
        "        # Update local cache\n",
        "        self.local_cache.put(workflow_id, state)\n",
        "    \n",
        "    def load_state(self, workflow_id: str) -> dict:\n",
        "        '''Load state with cache'''\n",
        "        # Check local cache first\n",
        "        cached = self.local_cache.get(workflow_id)\n",
        "        if cached:\n",
        "            return cached\n",
        "        \n",
        "        # Load from cluster\n",
        "        state_bytes = self.cluster.get(f'state:{workflow_id}')\n",
        "        if not state_bytes:\n",
        "            return None\n",
        "        \n",
        "        # Decompress if needed\n",
        "        try:\n",
        "            state = pickle.loads(state_bytes)\n",
        "        except:\n",
        "            import zlib\n",
        "            state = pickle.loads(zlib.decompress(state_bytes))\n",
        "        \n",
        "        # Update cache\n",
        "        self.local_cache.put(workflow_id, state)\n",
        "        \n",
        "        return state\n",
        "```\n",
        "\n",
        "**2. Async Execution Engine**\n",
        "```python\n",
        "import asyncio\n",
        "import aiokafka\n",
        "\n",
        "class AsyncGraphExecutor:\n",
        "    '''Async execution for high concurrency'''\n",
        "    \n",
        "    def __init__(self, state_store: DistributedStateStore):\n",
        "        self.state_store = state_store\n",
        "        self.semaphore = asyncio.Semaphore(100)  # Limit concurrent LLM calls\n",
        "        self.metrics = defaultdict(int)\n",
        "    \n",
        "    async def execute_node(self, workflow_id: str, node_name: str, state: dict):\n",
        "        '''Execute single node asynchronously'''\n",
        "        async with self.semaphore:\n",
        "            start = time.time()\n",
        "            \n",
        "            try:\n",
        "                # Execute node function\n",
        "                result = await self._run_node_func(node_name, state)\n",
        "                \n",
        "                # Update state\n",
        "                state.update(result)\n",
        "                \n",
        "                # Save to distributed store\n",
        "                await asyncio.to_thread(\n",
        "                    self.state_store.save_state,\n",
        "                    workflow_id,\n",
        "                    state\n",
        "                )\n",
        "                \n",
        "                latency = (time.time() - start) * 1000\n",
        "                self.metrics['success'] += 1\n",
        "                self.metrics['total_latency_ms'] += latency\n",
        "                \n",
        "                return state, True\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.metrics['errors'] += 1\n",
        "                return state, False\n",
        "    \n",
        "    async def _run_node_func(self, node_name: str, state: dict):\n",
        "        '''Run node function (may call LLM)'''\n",
        "        # Simulated async LLM call\n",
        "        await asyncio.sleep(0.1)\n",
        "        return {'processed': True}\n",
        "    \n",
        "    async def execute_batch(self, workflows: List[tuple]):\n",
        "        '''Execute multiple workflows concurrently'''\n",
        "        tasks = [\n",
        "            self.execute_workflow(workflow_id, state)\n",
        "            for workflow_id, state in workflows\n",
        "        ]\n",
        "        \n",
        "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "        \n",
        "        return results\n",
        "```\n",
        "\n",
        "**3. Message Queue for Distributed Execution**\n",
        "```python\n",
        "class WorkflowOrchestrator:\n",
        "    '''Distribute workflow execution across workers'''\n",
        "    \n",
        "    def __init__(self, kafka_brokers: List[str]):\n",
        "        self.producer = aiokafka.AIOKafkaProducer(\n",
        "            bootstrap_servers=kafka_brokers,\n",
        "            value_serializer=lambda v: json.dumps(v).encode()\n",
        "        )\n",
        "        \n",
        "        self.consumer = aiokafka.AIOKafkaConsumer(\n",
        "            'workflow-tasks',\n",
        "            bootstrap_servers=kafka_brokers,\n",
        "            group_id='graph-workers',\n",
        "            value_deserializer=lambda v: json.loads(v.decode())\n",
        "        )\n",
        "    \n",
        "    async def submit_workflow(self, workflow_id: str, initial_state: dict):\n",
        "        '''Submit workflow for distributed execution'''\n",
        "        message = {\n",
        "            'workflow_id': workflow_id,\n",
        "            'state': initial_state,\n",
        "            'current_step': 'start',\n",
        "            'submitted_at': datetime.utcnow().isoformat()\n",
        "        }\n",
        "        \n",
        "        await self.producer.send('workflow-tasks', value=message)\n",
        "    \n",
        "    async def worker_loop(self, executor: AsyncGraphExecutor):\n",
        "        '''Worker that consumes and executes workflows'''\n",
        "        async for message in self.consumer:\n",
        "            workflow_data = message.value\n",
        "            \n",
        "            # Execute current step\n",
        "            state, success = await executor.execute_node(\n",
        "                workflow_data['workflow_id'],\n",
        "                workflow_data['current_step'],\n",
        "                workflow_data['state']\n",
        "            )\n",
        "            \n",
        "            if success:\n",
        "                # Determine next step (routing logic)\n",
        "                next_step = self._route_next_step(state)\n",
        "                \n",
        "                if next_step != 'end':\n",
        "                    # Submit next step to queue\n",
        "                    await self.submit_workflow(\n",
        "                        workflow_data['workflow_id'],\n",
        "                        state\n",
        "                    )\n",
        "```\n",
        "\n",
        "**4. Fast Routing with Decision Trees**\n",
        "```python\n",
        "class OptimizedRouter:\n",
        "    '''Sub-second routing decisions'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Pre-compiled routing logic\n",
        "        self.routing_tree = self._build_routing_tree()\n",
        "        self.cache = LRUCache(maxsize=10000)\n",
        "    \n",
        "    def _build_routing_tree(self):\n",
        "        '''Build decision tree for fast routing'''\n",
        "        # Routing rules compiled into efficient structure\n",
        "        return {\n",
        "            'complexity': {\n",
        "                'high': {'confidence': {'high': 'generate', 'low': 'review'}},\n",
        "                'low': 'generate'\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def route(self, state: dict) -> str:\n",
        "        '''Route in < 1ms'''\n",
        "        # Check cache first\n",
        "        cache_key = self._state_fingerprint(state)\n",
        "        cached = self.cache.get(cache_key)\n",
        "        if cached:\n",
        "            return cached\n",
        "        \n",
        "        # Traverse decision tree\n",
        "        node = self.routing_tree\n",
        "        \n",
        "        # Fast lookup\n",
        "        complexity = state.get('complexity', 'low')\n",
        "        if complexity in node:\n",
        "            node = node[complexity]\n",
        "            if isinstance(node, dict):\n",
        "                confidence = state.get('confidence', 'low')\n",
        "                next_step = node.get('confidence', {}).get(confidence, 'generate')\n",
        "            else:\n",
        "                next_step = node\n",
        "        else:\n",
        "            next_step = 'generate'\n",
        "        \n",
        "        # Cache result\n",
        "        self.cache.put(cache_key, next_step)\n",
        "        \n",
        "        return next_step\n",
        "    \n",
        "    def _state_fingerprint(self, state: dict) -> str:\n",
        "        '''Fast state hashing for cache keys'''\n",
        "        # Hash only routing-relevant fields\n",
        "        key_fields = f\"{state.get('complexity')}_{state.get('confidence')}\"\n",
        "        return key_fields\n",
        "```\n",
        "\n",
        "**5. Complete Architecture**\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                        Load Balancer                         │\n",
        "│                      (10K req/sec)                          │\n",
        "└────────────────────┬────────────────────────────────────────┘\n",
        "                     │\n",
        "        ┌────────────┴────────────┐\n",
        "        │                         │\n",
        "   ┌────▼─────┐            ┌─────▼─────┐\n",
        "   │  API      │            │   API     │\n",
        "   │  Server 1 │            │  Server N │\n",
        "   └────┬──────┘            └─────┬─────┘\n",
        "        │                         │\n",
        "        └────────────┬────────────┘\n",
        "                     │\n",
        "            ┌────────▼────────┐\n",
        "            │   Kafka Cluster │\n",
        "            │  (workflow-tasks)│\n",
        "            └────────┬─────────┘\n",
        "                     │\n",
        "     ┌───────────────┼───────────────┐\n",
        "     │               │               │\n",
        "┌────▼─────┐   ┌────▼─────┐   ┌────▼─────┐\n",
        "│ Worker 1 │   │ Worker 2 │   │ Worker N │\n",
        "│ (Async   │   │ (Async   │   │ (Async   │\n",
        "│ Executor)│   │ Executor)│   │ Executor)│\n",
        "└────┬─────┘   └────┬─────┘   └────┬─────┘\n",
        "     │              │              │\n",
        "     └──────────────┼──────────────┘\n",
        "                    │\n",
        "          ┌─────────▼──────────┐\n",
        "          │   Redis Cluster    │\n",
        "          │ (Distributed State)│\n",
        "          └────────────────────┘\n",
        "```\n",
        "\n",
        "**Performance Characteristics:**\n",
        "- **Throughput**: 10,000 workflows/sec\n",
        "- **Routing latency**: < 1ms (cached), < 5ms (uncached)\n",
        "- **State save/load**: < 10ms (Redis Cluster)\n",
        "- **Node execution**: 100-500ms (LLM calls)\n",
        "- **Total latency**: P95 < 2s for 5-node workflow\n",
        "\n",
        "**Trade-offs:**\n",
        "\n",
        "| Aspect | Choice | Trade-off |\n",
        "|--------|--------|----------|\n",
        "| State Storage | Redis Cluster | Fast but memory-intensive vs. DB (durable but slower) |\n",
        "| Execution Model | Async + Message Queue | Complex but scalable vs. sync (simple but limited) |\n",
        "| Routing | Decision Tree | Fast but rigid vs. LLM routing (flexible but slow) |\n",
        "| Caching | Aggressive LRU | Higher memory vs. lower latency |\n",
        "| Serialization | Pickle + Compression | Fast but language-specific vs. JSON (universal but slower) |\n",
        "\n",
        "**Cost Analysis (10K workflows/sec):**\n",
        "- Redis Cluster (30 nodes): $3,000/month\n",
        "- Kafka (9 brokers): $2,000/month  \n",
        "- Workers (50 instances): $10,000/month\n",
        "- LLM API calls: $50,000/month (dominant cost)\n",
        "- **Total**: ~$65,000/month\n",
        "\n",
        "**Optimization Opportunities:**\n",
        "1. Model caching (save 30% LLM cost)\n",
        "2. Batched inference (2x throughput)\n",
        "3. Smart routing (avoid expensive models when possible)\n",
        "4. State compression (50% storage reduction)\n",
        "        ''',\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, qa in enumerate(langgraph_interview_questions, 1):\n",
        "    print(f'\\n{'=' * 100}')\n",
        "    print(f'Q{i} [{qa[\"level\"]} Level]')\n",
        "    print('=' * 100)\n",
        "    print(f'\\n{qa[\"question\"]}\\n')\n",
        "    print('ANSWER:')\n",
        "    print(qa['answer'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4: Advanced Routing Patterns\n",
        "\n",
        "Production routing needs:\n",
        "- **Conditional logic**: Route based on state\n",
        "- **Parallel execution**: Multiple paths simultaneously\n",
        "- **Dynamic routing**: LLM decides next step\n",
        "- **Fallback paths**: Handle edge cases\n",
        "- **Loop detection**: Prevent infinite cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Callable, Set\n",
        "import time\n",
        "\n",
        "class AdvancedGraphState(TypedDict):\n",
        "    '''Enhanced state for complex workflows'''\n",
        "    messages: List[dict]\n",
        "    current_node: str\n",
        "    visited_nodes: Set[str]\n",
        "    loop_count: Dict[str, int]\n",
        "    data: Dict[str, Any]\n",
        "    confidence: float\n",
        "    errors: List[dict]\n",
        "\n",
        "class AdvancedRouter:\n",
        "    '''Sophisticated routing with loop detection and fallbacks'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.routing_rules = {}\n",
        "        self.fallback_routes = {}\n",
        "        self.max_loops = 3\n",
        "    \n",
        "    def add_conditional_route(self, from_node: str, conditions: Dict[str, Callable]):\n",
        "        '''Add multi-way conditional routing\n",
        "        \n",
        "        Example:\n",
        "            conditions = {\n",
        "                'high_confidence': lambda s: s['confidence'] > 0.8,\n",
        "                'medium_confidence': lambda s: 0.5 < s['confidence'] <= 0.8,\n",
        "                'low_confidence': lambda s: s['confidence'] <= 0.5,\n",
        "            }\n",
        "        '''\n",
        "        self.routing_rules[from_node] = conditions\n",
        "    \n",
        "    def add_fallback(self, from_node: str, fallback_node: str):\n",
        "        '''Add fallback route if all conditions fail'''\n",
        "        self.fallback_routes[from_node] = fallback_node\n",
        "    \n",
        "    def route(self, state: AdvancedGraphState) -> str:\n",
        "        '''Determine next node with loop detection'''\n",
        "        current = state['current_node']\n",
        "        \n",
        "        # Check for infinite loops\n",
        "        if state['loop_count'].get(current, 0) >= self.max_loops:\n",
        "            print(f'Loop detected at {current}, escaping to fallback')\n",
        "            return self.fallback_routes.get(current, 'human_review')\n",
        "        \n",
        "        # Try conditional routes\n",
        "        if current in self.routing_rules:\n",
        "            for next_node, condition in self.routing_rules[current].items():\n",
        "                if condition(state):\n",
        "                    # Update loop counter\n",
        "                    state['loop_count'][next_node] = state['loop_count'].get(next_node, 0) + 1\n",
        "                    return next_node\n",
        "        \n",
        "        # Use fallback if all conditions fail\n",
        "        if current in self.fallback_routes:\n",
        "            return self.fallback_routes[current]\n",
        "        \n",
        "        # Default: end workflow\n",
        "        return 'end'\n",
        "\n",
        "class ParallelExecutionGraph:\n",
        "    '''Execute multiple branches in parallel'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "        self.parallel_branches = {}\n",
        "    \n",
        "    def add_parallel_branches(self, fork_node: str, branches: List[str], join_node: str):\n",
        "        '''Define parallel execution branches\n",
        "        \n",
        "        Example:\n",
        "            fork_node: 'start'\n",
        "            branches: ['analyze_style', 'analyze_security', 'analyze_performance']\n",
        "            join_node: 'aggregate_results'\n",
        "        '''\n",
        "        self.parallel_branches[fork_node] = {\n",
        "            'branches': branches,\n",
        "            'join': join_node\n",
        "        }\n",
        "    \n",
        "    async def execute_parallel(self, state: dict, branches: List[str]) -> dict:\n",
        "        '''Execute branches in parallel'''\n",
        "        import asyncio\n",
        "        \n",
        "        # Create tasks for each branch\n",
        "        tasks = []\n",
        "        for branch in branches:\n",
        "            if branch in self.nodes:\n",
        "                task = asyncio.create_task(self._execute_node_async(branch, state.copy()))\n",
        "                tasks.append((branch, task))\n",
        "        \n",
        "        # Wait for all branches\n",
        "        results = {}\n",
        "        for branch, task in tasks:\n",
        "            try:\n",
        "                branch_result = await task\n",
        "                results[branch] = branch_result\n",
        "            except Exception as e:\n",
        "                results[branch] = {'error': str(e)}\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    async def _execute_node_async(self, node_name: str, state: dict) -> dict:\n",
        "        '''Execute single node asynchronously'''\n",
        "        await asyncio.sleep(0.1)  # Simulate async work\n",
        "        if node_name in self.nodes:\n",
        "            return self.nodes[node_name](state)\n",
        "        return {}\n",
        "\n",
        "# Example: Parallel code analysis\n",
        "print('ADVANCED ROUTING DEMONSTRATION')\n",
        "print('=' * 90)\n",
        "\n",
        "def analyze_style(state):\n",
        "    state['style_issues'] = ['inconsistent_naming', 'missing_docstrings']\n",
        "    return state\n",
        "\n",
        "def analyze_security(state):\n",
        "    state['security_issues'] = ['sql_injection_risk']\n",
        "    return state\n",
        "\n",
        "def analyze_performance(state):\n",
        "    state['performance_issues'] = ['n_squared_complexity']\n",
        "    return state\n",
        "\n",
        "def aggregate_analysis(state):\n",
        "    all_issues = (\n",
        "        state.get('style_issues', []) + \n",
        "        state.get('security_issues', []) + \n",
        "        state.get('performance_issues', [])\n",
        "    )\n",
        "    state['total_issues'] = len(all_issues)\n",
        "    state['severity'] = 'high' if 'security' in str(all_issues) else 'medium'\n",
        "    return state\n",
        "\n",
        "# Mock async execution\n",
        "import asyncio\n",
        "\n",
        "async def demo_parallel():\n",
        "    graph = ParallelExecutionGraph()\n",
        "    graph.nodes = {\n",
        "        'analyze_style': analyze_style,\n",
        "        'analyze_security': analyze_security,\n",
        "        'analyze_performance': analyze_performance,\n",
        "        'aggregate': aggregate_analysis,\n",
        "    }\n",
        "    \n",
        "    initial_state = {'code': 'sample code here'}\n",
        "    \n",
        "    print('\\nExecuting parallel branches...')\n",
        "    start = time.time()\n",
        "    \n",
        "    # Execute in parallel\n",
        "    branch_results = await graph.execute_parallel(\n",
        "        initial_state,\n",
        "        ['analyze_style', 'analyze_security', 'analyze_performance']\n",
        "    )\n",
        "    \n",
        "    elapsed = (time.time() - start) * 1000\n",
        "    \n",
        "    print(f'All branches completed in {elapsed:.0f}ms')\n",
        "    print('\\nResults:')\n",
        "    for branch, result in branch_results.items():\n",
        "        issues = result.get('style_issues') or result.get('security_issues') or result.get('performance_issues', [])\n",
        "        print(f'  {branch}: {len(issues)} issues found')\n",
        "    \n",
        "    # Aggregate\n",
        "    final_state = initial_state.copy()\n",
        "    for result in branch_results.values():\n",
        "        final_state.update(result)\n",
        "    \n",
        "    final_state = aggregate_analysis(final_state)\n",
        "    print(f'\\nTotal issues: {final_state[\"total_issues\"]}')\n",
        "    print(f'Severity: {final_state[\"severity\"]}')\n",
        "\n",
        "# Run demo\n",
        "asyncio.run(demo_parallel())\n",
        "\n",
        "print('\\n' + '=' * 90)\n",
        "print('KEY BENEFITS OF PARALLEL EXECUTION:')\n",
        "print('  - Reduced latency: 3 × 100ms sequential = 300ms')\n",
        "print('                    vs max(100ms, 100ms, 100ms) = 100ms parallel')\n",
        "print('  - Better resource utilization')\n",
        "print('  - Independent failures don\\'t block other branches')\n",
        "print('  - Can aggregate multiple perspectives')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 5: Observability and Debugging\n",
        "\n",
        "Production graphs need deep observability:\n",
        "- **State inspection**: View state at any point\n",
        "- **Execution traces**: Full history of decisions\n",
        "- **Performance profiling**: Per-node latency\n",
        "- **Error tracking**: Capture and analyze failures\n",
        "- **Replay capability**: Reproduce issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "class ObservableGraph:\n",
        "    '''LangGraph with comprehensive observability'''\n",
        "    \n",
        "    def __init__(self, workflow_name: str):\n",
        "        self.workflow_name = workflow_name\n",
        "        self.nodes = {}\n",
        "        self.traces = []\n",
        "        self.node_metrics = defaultdict(lambda: {\n",
        "            'calls': 0,\n",
        "            'successes': 0,\n",
        "            'failures': 0,\n",
        "            'total_latency_ms': 0,\n",
        "            'latency_samples': []\n",
        "        })\n",
        "    \n",
        "    def add_node(self, name: str, func: Callable):\n",
        "        '''Add node with automatic instrumentation'''\n",
        "        self.nodes[name] = self._instrument_function(name, func)\n",
        "    \n",
        "    def _instrument_function(self, node_name: str, func: Callable) -> Callable:\n",
        "        '''Wrap function with observability'''\n",
        "        def instrumented(state: dict) -> dict:\n",
        "            trace_id = state.get('trace_id', str(uuid.uuid4()))\n",
        "            span_id = str(uuid.uuid4())[:8]\n",
        "            \n",
        "            # Start span\n",
        "            span_start = time.time()\n",
        "            \n",
        "            # Record entry\n",
        "            self.traces.append({\n",
        "                'trace_id': trace_id,\n",
        "                'span_id': span_id,\n",
        "                'workflow': self.workflow_name,\n",
        "                'node': node_name,\n",
        "                'event': 'start',\n",
        "                'timestamp': datetime.utcnow().isoformat(),\n",
        "                'state_snapshot': state.copy(),\n",
        "            })\n",
        "            \n",
        "            try:\n",
        "                # Execute node\n",
        "                result = func(state)\n",
        "                \n",
        "                # Record success\n",
        "                latency_ms = (time.time() - span_start) * 1000\n",
        "                \n",
        "                self.traces.append({\n",
        "                    'trace_id': trace_id,\n",
        "                    'span_id': span_id,\n",
        "                    'workflow': self.workflow_name,\n",
        "                    'node': node_name,\n",
        "                    'event': 'end',\n",
        "                    'status': 'success',\n",
        "                    'latency_ms': latency_ms,\n",
        "                    'timestamp': datetime.utcnow().isoformat(),\n",
        "                })\n",
        "                \n",
        "                # Update metrics\n",
        "                metrics = self.node_metrics[node_name]\n",
        "                metrics['calls'] += 1\n",
        "                metrics['successes'] += 1\n",
        "                metrics['total_latency_ms'] += latency_ms\n",
        "                metrics['latency_samples'].append(latency_ms)\n",
        "                \n",
        "                return result\n",
        "                \n",
        "            except Exception as e:\n",
        "                # Record failure\n",
        "                latency_ms = (time.time() - span_start) * 1000\n",
        "                \n",
        "                self.traces.append({\n",
        "                    'trace_id': trace_id,\n",
        "                    'span_id': span_id,\n",
        "                    'workflow': self.workflow_name,\n",
        "                    'node': node_name,\n",
        "                    'event': 'error',\n",
        "                    'error': str(e),\n",
        "                    'latency_ms': latency_ms,\n",
        "                    'timestamp': datetime.utcnow().isoformat(),\n",
        "                })\n",
        "                \n",
        "                # Update metrics\n",
        "                metrics = self.node_metrics[node_name]\n",
        "                metrics['calls'] += 1\n",
        "                metrics['failures'] += 1\n",
        "                \n",
        "                raise\n",
        "        \n",
        "        return instrumented\n",
        "    \n",
        "    def get_trace(self, trace_id: str) -> List[dict]:\n",
        "        '''Get complete trace by ID'''\n",
        "        return [t for t in self.traces if t['trace_id'] == trace_id]\n",
        "    \n",
        "    def get_node_metrics(self, node_name: str = None) -> dict:\n",
        "        '''Get performance metrics for nodes'''\n",
        "        if node_name:\n",
        "            metrics = self.node_metrics[node_name]\n",
        "            return {\n",
        "                'node': node_name,\n",
        "                'calls': metrics['calls'],\n",
        "                'success_rate': metrics['successes'] / metrics['calls'] if metrics['calls'] > 0 else 0,\n",
        "                'avg_latency_ms': metrics['total_latency_ms'] / metrics['calls'] if metrics['calls'] > 0 else 0,\n",
        "                'p50_latency_ms': np.percentile(metrics['latency_samples'], 50) if metrics['latency_samples'] else 0,\n",
        "                'p95_latency_ms': np.percentile(metrics['latency_samples'], 95) if metrics['latency_samples'] else 0,\n",
        "            }\n",
        "        else:\n",
        "            # All nodes\n",
        "            return {name: self.get_node_metrics(name) for name in self.node_metrics.keys()}\n",
        "    \n",
        "    def visualize_trace(self, trace_id: str) -> str:\n",
        "        '''Generate ASCII visualization of execution trace'''\n",
        "        trace_events = self.get_trace(trace_id)\n",
        "        \n",
        "        if not trace_events:\n",
        "            return 'No trace found'\n",
        "        \n",
        "        viz = f'\\nExecution Trace: {trace_id}\\n'\n",
        "        viz += '=' * 80 + '\\n'\n",
        "        \n",
        "        for event in trace_events:\n",
        "            indent = '  ' * (1 if event['event'] == 'start' else 2)\n",
        "            \n",
        "            if event['event'] == 'start':\n",
        "                viz += f\"{indent}→ {event['node']} (starting...)\\n\"\n",
        "            elif event['event'] == 'end':\n",
        "                viz += f\"{indent}✓ {event['node']} ({event['latency_ms']:.0f}ms)\\n\"\n",
        "            elif event['event'] == 'error':\n",
        "                viz += f\"{indent}✗ {event['node']} - Error: {event['error'][:50]}...\\n\"\n",
        "        \n",
        "        viz += '=' * 80\n",
        "        return viz\n",
        "    \n",
        "    def export_trace_json(self, trace_id: str, filename: str):\n",
        "        '''Export trace for analysis'''\n",
        "        trace = self.get_trace(trace_id)\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(trace, f, indent=2)\n",
        "\n",
        "# Demo observable graph\n",
        "print('OBSERVABLE GRAPH WITH TRACING')\n",
        "print('=' * 90)\n",
        "\n",
        "graph = ObservableGraph('customer_support_flow')\n",
        "\n",
        "# Add nodes\n",
        "def classify_query(state):\n",
        "    state['query_type'] = 'refund' if 'refund' in state['messages'][-1]['content'].lower() else 'general'\n",
        "    state['confidence'] = 0.85\n",
        "    return state\n",
        "\n",
        "def process_refund(state):\n",
        "    time.sleep(0.05)  # Simulate work\n",
        "    state['refund_processed'] = True\n",
        "    return state\n",
        "\n",
        "def process_general(state):\n",
        "    time.sleep(0.03)  # Simulate work\n",
        "    state['response_generated'] = True\n",
        "    return state\n",
        "\n",
        "graph.add_node('classify', classify_query)\n",
        "graph.add_node('process_refund', process_refund)\n",
        "graph.add_node('process_general', process_general)\n",
        "\n",
        "# Execute multiple workflows\n",
        "for i in range(3):\n",
        "    trace_id = str(uuid.uuid4())\n",
        "    state = {\n",
        "        'trace_id': trace_id,\n",
        "        'messages': [{'role': 'user', 'content': f'Test query {i}'}],\n",
        "        'current_node': 'classify',\n",
        "        'visited_nodes': set(),\n",
        "        'loop_count': {},\n",
        "        'data': {},\n",
        "        'confidence': 0.0,\n",
        "        'errors': []\n",
        "    }\n",
        "    \n",
        "    # Execute nodes\n",
        "    for node in ['classify', 'process_general']:\n",
        "        state['current_node'] = node\n",
        "        state = graph.nodes[node](state)\n",
        "\n",
        "print('\\nNode Performance Metrics:')\n",
        "print('-' * 80)\n",
        "for node_name, metrics in graph.get_node_metrics().items():\n",
        "    print(f'\\n{node_name}:')\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f'  {key}: {value:.2f}')\n",
        "        else:\n",
        "            print(f'  {key}: {value}')\n",
        "\n",
        "# Show trace visualization\n",
        "if graph.traces:\n",
        "    sample_trace_id = graph.traces[0]['trace_id']\n",
        "    print('\\n' + graph.visualize_trace(sample_trace_id))\n",
        "\n",
        "print('\\n' + '=' * 90)\n",
        "print('OBSERVABILITY BEST PRACTICES:')\n",
        "print('  - Trace every execution with unique trace_id')\n",
        "print('  - Capture state snapshots at each node')\n",
        "print('  - Measure latency per node (identify bottlenecks)')\n",
        "print('  - Track success/failure rates per node')\n",
        "print('  - Export traces for offline analysis')\n",
        "print('  - Use distributed tracing in production (Jaeger, Zipkin)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
