{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 6: CrewAI\n",
        "\n",
        "## Applied AI Scientist Field Notes - Expanded Edition\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 6: CrewAI - Role-Based Task Orchestration\n",
        "\n",
        "### Topics\n",
        "1. Agent roles and expertise\n",
        "2. Task delegation\n",
        "3. Sequential vs hierarchical processes\n",
        "4. Memory management\n",
        "5. Tool integration\n",
        "6. Output validation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q crewai crewai-tools\n",
        "\n",
        "print('CrewAI installed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Role-Based Agents\n",
        "\n",
        "CrewAI concepts:\n",
        "- **Agent**: Has role, goal, backstory, tools\n",
        "- **Task**: Work unit with expected output\n",
        "- **Crew**: Orchestrates agents and tasks\n",
        "- **Process**: Sequential or hierarchical\n",
        "- **Memory**: Shared context across agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrewAgent:\n",
        "    def __init__(self, role: str, goal: str, backstory: str, tools: list = None):\n",
        "        self.role = role\n",
        "        self.goal = goal\n",
        "        self.backstory = backstory\n",
        "        self.tools = tools or []\n",
        "    \n",
        "    def execute_task(self, task: str) -> str:\n",
        "        return f'[{self.role}] completed: {task[:50]}...'\n",
        "\n",
        "class Task:\n",
        "    def __init__(self, description: str, expected_output: str, agent):\n",
        "        self.description = description\n",
        "        self.expected_output = expected_output\n",
        "        self.agent = agent\n",
        "        self.result = None\n",
        "    \n",
        "    def execute(self):\n",
        "        self.result = self.agent.execute_task(self.description)\n",
        "        return self.result\n",
        "\n",
        "class Crew:\n",
        "    def __init__(self, agents: list, tasks: list, process='sequential'):\n",
        "        self.agents = agents\n",
        "        self.tasks = tasks\n",
        "        self.process = process\n",
        "    \n",
        "    def kickoff(self):\n",
        "        results = []\n",
        "        for task in self.tasks:\n",
        "            result = task.execute()\n",
        "            results.append({\n",
        "                'task': task.description[:50],\n",
        "                'agent': task.agent.role,\n",
        "                'result': result\n",
        "            })\n",
        "        return {'results': results, 'process': self.process}\n",
        "\n",
        "# Example: Research Crew\n",
        "print('CrewAI Workflow Demo')\n",
        "print('=' * 80)\n",
        "\n",
        "researcher = CrewAgent('Researcher', 'Find information', 'Expert researcher', ['web_search'])\n",
        "writer = CrewAgent('Writer', 'Create reports', 'Technical writer', ['markdown'])\n",
        "editor = CrewAgent('Editor', 'Refine content', 'Quality editor', ['grammar_check'])\n",
        "\n",
        "task1 = Task('Research LLM agentic trends', '5 key trends', researcher)\n",
        "task2 = Task('Write technical report', '2-page report', writer)\n",
        "task3 = Task('Edit and polish', 'Final report', editor)\n",
        "\n",
        "crew = Crew([researcher, writer, editor], [task1, task2, task3])\n",
        "output = crew.kickoff()\n",
        "\n",
        "print(f'\\nProcess: {output[\"process\"]}\\n')\n",
        "for i, res in enumerate(output['results'], 1):\n",
        "    print(f'Task {i}: {res[\"task\"]}...')\n",
        "    print(f'  Agent: {res[\"agent\"]}')\n",
        "    print(f'  Result: {res[\"result\"]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 2: Hierarchical Process with Manager Agent\n\nHierarchical crews use a manager to delegate tasks dynamically:\n- **Manager**: Assigns tasks based on agent capabilities\n- **Specialists**: Execute assigned tasks\n- **Quality Control**: Manager reviews outputs\n- **Adaptive**: Manager can reassign based on performance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List, Dict, Any, Optional, Callable\nimport json\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass AgentCapability:\n    '''Agent's capabilities and expertise level'''\n    domain: str\n    expertise_level: float  # 0-1\n    max_concurrent_tasks: int\n    avg_completion_time_sec: float\n\nclass EnhancedCrewAgent:\n    '''Agent with capabilities and performance tracking'''\n    \n    def __init__(self, \n                 role: str, \n                 goal: str, \n                 backstory: str,\n                 capabilities: List[AgentCapability],\n                 tools: List[str] = None):\n        self.role = role\n        self.goal = goal\n        self.backstory = backstory\n        self.capabilities = {c.domain: c for c in capabilities}\n        self.tools = tools or []\n        \n        # Performance tracking\n        self.tasks_completed = 0\n        self.tasks_failed = 0\n        self.total_time_sec = 0\n        self.current_tasks = []\n    \n    def can_handle(self, task_domain: str, required_expertise: float = 0.5) -> bool:\n        '''Check if agent can handle task'''\n        if task_domain not in self.capabilities:\n            return False\n        \n        cap = self.capabilities[task_domain]\n        \n        # Check expertise level\n        if cap.expertise_level < required_expertise:\n            return False\n        \n        # Check capacity\n        if len(self.current_tasks) >= cap.max_concurrent_tasks:\n            return False\n        \n        return True\n    \n    def execute_task(self, task: 'EnhancedTask') -> dict:\n        '''Execute task with performance tracking'''\n        import time\n        \n        start = time.time()\n        \n        try:\n            # Simulate work\n            time.sleep(0.1)  # Mock execution\n            \n            result = f'[{self.role}] completed: {task.description[:50]}...'\n            \n            elapsed = time.time() - start\n            self.tasks_completed += 1\n            self.total_time_sec += elapsed\n            \n            return {\n                'status': 'success',\n                'result': result,\n                'time_sec': elapsed,\n                'agent': self.role\n            }\n            \n        except Exception as e:\n            self.tasks_failed += 1\n            return {\n                'status': 'error',\n                'error': str(e),\n                'agent': self.role\n            }\n    \n    def get_performance_metrics(self) -> dict:\n        '''Get agent performance metrics'''\n        total = self.tasks_completed + self.tasks_failed\n        return {\n            'role': self.role,\n            'tasks_completed': self.tasks_completed,\n            'tasks_failed': self.tasks_failed,\n            'success_rate': self.tasks_completed / total if total > 0 else 0,\n            'avg_time_sec': self.total_time_sec / self.tasks_completed if self.tasks_completed > 0 else 0\n        }\n\nclass EnhancedTask:\n    '''Task with domain, priority, and dependencies'''\n    \n    def __init__(self,\n                 description: str,\n                 expected_output: str,\n                 domain: str,\n                 priority: int = 5,  # 1-10\n                 required_expertise: float = 0.5,\n                 dependencies: List[str] = None):\n        self.task_id = f'task_{datetime.utcnow().timestamp()}'\n        self.description = description\n        self.expected_output = expected_output\n        self.domain = domain\n        self.priority = priority\n        self.required_expertise = required_expertise\n        self.dependencies = dependencies or []\n        self.assigned_agent = None\n        self.result = None\n        self.status = 'pending'\n\nclass ManagerAgent:\n    '''Manager that delegates tasks to specialists'''\n    \n    def __init__(self, agents: List[EnhancedCrewAgent]):\n        self.agents = agents\n        self.task_queue = []\n        self.completed_tasks = {}\n    \n    def assign_task(self, task: EnhancedTask) -> Optional[EnhancedCrewAgent]:\n        '''Intelligently assign task to best available agent'''\n        \n        # Find capable agents\n        capable_agents = [\n            agent for agent in self.agents\n            if agent.can_handle(task.domain, task.required_expertise)\n        ]\n        \n        if not capable_agents:\n            print(f'No agent capable of: {task.description[:50]}...')\n            return None\n        \n        # Select best agent based on:\n        # 1. Expertise level\n        # 2. Current workload\n        # 3. Past performance\n        def score_agent(agent: EnhancedCrewAgent) -> float:\n            cap = agent.capabilities[task.domain]\n            metrics = agent.get_performance_metrics()\n            \n            score = (\n                cap.expertise_level * 0.4 +  # Expertise\n                metrics['success_rate'] * 0.3 +  # Past performance\n                (1 - len(agent.current_tasks) / cap.max_concurrent_tasks) * 0.3  # Availability\n            )\n            \n            return score\n        \n        best_agent = max(capable_agents, key=score_agent)\n        task.assigned_agent = best_agent\n        \n        return best_agent\n    \n    def execute_crew(self, tasks: List[EnhancedTask]) -> dict:\n        '''Execute tasks with hierarchical delegation'''\n        \n        # Sort by priority and dependencies\n        sorted_tasks = self._sort_tasks(tasks)\n        \n        results = []\n        \n        for task in sorted_tasks:\n            # Check dependencies\n            if not self._dependencies_met(task):\n                print(f'Waiting for dependencies: {task.description[:50]}...')\n                continue\n            \n            # Assign to agent\n            agent = self.assign_task(task)\n            \n            if agent:\n                print(f'Assigning to {agent.role}: {task.description[:50]}...')\n                \n                # Execute\n                result = agent.execute_task(task)\n                \n                task.result = result\n                task.status = result['status']\n                self.completed_tasks[task.task_id] = task\n                \n                results.append({\n                    'task_id': task.task_id,\n                    'description': task.description[:50],\n                    'agent': agent.role,\n                    'status': result['status'],\n                    'time_sec': result['time_sec']\n                })\n            else:\n                results.append({\n                    'task_id': task.task_id,\n                    'description': task.description[:50],\n                    'status': 'no_agent_available'\n                })\n        \n        return {\n            'results': results,\n            'total_tasks': len(tasks),\n            'successful': sum(1 for r in results if r['status'] == 'success'),\n            'failed': sum(1 for r in results if r['status'] in ['error', 'no_agent_available'])\n        }\n    \n    def _sort_tasks(self, tasks: List[EnhancedTask]) -> List[EnhancedTask]:\n        '''Sort tasks by priority and dependencies'''\n        # Simple sort by priority (higher first)\n        return sorted(tasks, key=lambda t: t.priority, reverse=True)\n    \n    def _dependencies_met(self, task: EnhancedTask) -> bool:\n        '''Check if task dependencies are completed'''\n        for dep_id in task.dependencies:\n            if dep_id not in self.completed_tasks:\n                return False\n            if self.completed_tasks[dep_id].status != 'success':\n                return False\n        return True\n\n# Demo: Hierarchical crew\nprint('HIERARCHICAL CREW WITH MANAGER')\nprint('=' * 90)\n\n# Create specialist agents\nresearcher = EnhancedCrewAgent(\n    role='Senior Researcher',\n    goal='Conduct thorough research',\n    backstory='PhD in CS with 10 years research experience',\n    capabilities=[\n        AgentCapability('research', expertise_level=0.9, max_concurrent_tasks=3, avg_completion_time_sec=60),\n        AgentCapability('analysis', expertise_level=0.7, max_concurrent_tasks=2, avg_completion_time_sec=45),\n    ],\n    tools=['web_search', 'arxiv_search', 'scholar_search']\n)\n\ndata_analyst = EnhancedCrewAgent(\n    role='Data Analyst',\n    goal='Analyze data and extract insights',\n    backstory='Expert in statistical analysis and visualization',\n    capabilities=[\n        AgentCapability('analysis', expertise_level=0.95, max_concurrent_tasks=5, avg_completion_time_sec=30),\n        AgentCapability('visualization', expertise_level=0.8, max_concurrent_tasks=3, avg_completion_time_sec=20),\n    ],\n    tools=['pandas', 'matplotlib', 'statsmodels']\n)\n\nwriter = EnhancedCrewAgent(\n    role='Technical Writer',\n    goal='Create clear documentation',\n    backstory='10 years writing technical content',\n    capabilities=[\n        AgentCapability('writing', expertise_level=0.9, max_concurrent_tasks=4, avg_completion_time_sec=40),\n        AgentCapability('editing', expertise_level=0.85, max_concurrent_tasks=5, avg_completion_time_sec=20),\n    ],\n    tools=['markdown', 'grammar_check']\n)\n\n# Create manager\nmanager = ManagerAgent([researcher, data_analyst, writer])\n\n# Define tasks\ntasks = [\n    EnhancedTask(\n        'Research latest LLM architectures',\n        'Summary of 5 recent architectures',\n        domain='research',\n        priority=8,\n        required_expertise=0.7\n    ),\n    EnhancedTask(\n        'Analyze performance benchmarks',\n        'Statistical analysis with visualizations',\n        domain='analysis',\n        priority=7,\n        required_expertise=0.8\n    ),\n    EnhancedTask(\n        'Write technical blog post',\n        '2000-word article',\n        domain='writing',\n        priority=6,\n        required_expertise=0.6\n    ),\n]\n\n# Execute\nresult = manager.execute_crew(tasks)\n\nprint(f'\\nResults:')\nprint(f'  Total tasks: {result[\"total_tasks\"]}')\nprint(f'  Successful: {result[\"successful\"]}')\nprint(f'  Failed: {result[\"failed\"]}')\n\nprint(f'\\nTask breakdown:')\nfor r in result['results']:\n    print(f'  {r[\"description\"]}...')\n    print(f'    Agent: {r[\"agent\"]}')\n    print(f'    Status: {r[\"status\"]}')\n    print(f'    Time: {r.get(\"time_sec\", 0):.2f}s')\n\nprint('\\n' + '=' * 90)\nprint('Agent Performance:')\nfor agent in manager.agents:\n    metrics = agent.get_performance_metrics()\n    print(f'  {metrics[\"role\"]}: {metrics[\"tasks_completed\"]} tasks, {metrics[\"success_rate\"]:.0%} success, {metrics[\"avg_time_sec\"]:.1f}s avg')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 3: Memory Management in CrewAI\n\nCrewAI supports different memory types:\n- **Short-term**: Current task context\n- **Long-term**: Persistent facts and learnings\n- **Entity memory**: Track entities across tasks\n- **Shared memory**: All agents can access/update"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from collections import defaultdict\nimport hashlib\n\nclass CrewMemory:\n    '''Memory system for CrewAI workflows'''\n    \n    def __init__(self):\n        self.short_term = []  # Recent messages\n        self.long_term = {}   # Key facts\n        self.entity_memory = defaultdict(dict)  # Entity tracking\n        self.shared_context = {}  # Shared across agents\n        self.max_short_term_items = 20\n    \n    def add_short_term(self, item: dict):\n        '''Add to short-term memory (sliding window)'''\n        self.short_term.append(item)\n        \n        # Prune if too long\n        if len(self.short_term) > self.max_short_term_items:\n            self.short_term.pop(0)\n    \n    def add_long_term(self, key: str, value: Any):\n        '''Store important fact in long-term memory'''\n        self.long_term[key] = {\n            'value': value,\n            'stored_at': datetime.utcnow().isoformat(),\n            'access_count': 0\n        }\n    \n    def get_long_term(self, key: str) -> Optional[Any]:\n        '''Retrieve from long-term memory'''\n        if key in self.long_term:\n            self.long_term[key]['access_count'] += 1\n            return self.long_term[key]['value']\n        return None\n    \n    def track_entity(self, entity_name: str, attributes: dict):\n        '''Track entity across conversation'''\n        if entity_name not in self.entity_memory:\n            self.entity_memory[entity_name] = {\n                'first_mentioned': datetime.utcnow().isoformat(),\n                'attributes': {}\n            }\n        \n        # Update attributes\n        self.entity_memory[entity_name]['attributes'].update(attributes)\n        self.entity_memory[entity_name]['last_updated'] = datetime.utcnow().isoformat()\n    \n    def get_context_for_agent(self, agent_role: str) -> dict:\n        '''Get relevant context for agent'''\n        return {\n            'recent_messages': self.short_term[-5:],\n            'relevant_facts': self._get_relevant_facts(agent_role),\n            'shared_context': self.shared_context,\n        }\n    \n    def _get_relevant_facts(self, agent_role: str) -> dict:\n        '''Filter long-term memory for relevance'''\n        # In production, use embedding similarity\n        # For demo, return all\n        return {k: v['value'] for k, v in self.long_term.items()}\n    \n    def summarize(self) -> dict:\n        '''Get memory statistics'''\n        return {\n            'short_term_items': len(self.short_term),\n            'long_term_facts': len(self.long_term),\n            'tracked_entities': len(self.entity_memory),\n            'shared_keys': len(self.shared_context),\n        }\n\nclass MemoryAwareCrew:\n    '''Crew with persistent memory across tasks'''\n    \n    def __init__(self, agents: List[EnhancedCrewAgent], memory: CrewMemory = None):\n        self.agents = agents\n        self.memory = memory or CrewMemory()\n        self.execution_history = []\n    \n    def execute_task_sequence(self, tasks: List[EnhancedTask]) -> dict:\n        '''Execute tasks with memory'''\n        results = []\n        \n        for task in tasks:\n            # Provide memory context to agent\n            agent = self._select_agent(task)\n            \n            if agent:\n                context = self.memory.get_context_for_agent(agent.role)\n                \n                # Add context to task\n                task.context = context\n                \n                # Execute\n                result = agent.execute_task(task)\n                \n                # Store in memory\n                self.memory.add_short_term({\n                    'task': task.description,\n                    'agent': agent.role,\n                    'result': result['result'] if result['status'] == 'success' else None\n                })\n                \n                # Extract entities and facts (simplified)\n                if result['status'] == 'success':\n                    self._extract_and_store(result['result'])\n                \n                results.append(result)\n        \n        return {\n            'results': results,\n            'memory_summary': self.memory.summarize()\n        }\n    \n    def _select_agent(self, task: EnhancedTask) -> Optional[EnhancedCrewAgent]:\n        '''Select agent for task'''\n        for agent in self.agents:\n            if agent.can_handle(task.domain, task.required_expertise):\n                return agent\n        return None\n    \n    def _extract_and_store(self, result: str):\n        '''Extract facts and entities from result'''\n        # Simplified extraction (in production, use NER and fact extraction)\n        \n        # Store key facts\n        if 'important' in result.lower():\n            fact_key = hashlib.md5(result.encode()).hexdigest()[:8]\n            self.memory.add_long_term(fact_key, result[:100])\n\n# Demo memory-aware crew\nprint('\\nMEMORY-AWARE CREW DEMONSTRATION')\nprint('=' * 90)\n\nmemory = CrewMemory()\ncrew = MemoryAwareCrew([researcher, data_analyst, writer], memory)\n\ntasks = [\n    EnhancedTask('Research transformer architecture', 'Technical summary', 'research', priority=9),\n    EnhancedTask('Analyze attention mechanisms', 'Statistical analysis', 'analysis', priority=8),\n    EnhancedTask('Write article on findings', 'Blog post', 'writing', priority=7),\n]\n\nresult = crew.execute_task_sequence(tasks)\n\nprint(f'\\nMemory Summary: {result[\"memory_summary\"]}')\nprint(f'  Short-term: {result[\"memory_summary\"][\"short_term_items\"]} items')\nprint(f'  Long-term: {result[\"memory_summary\"][\"long_term_facts\"]} facts')\n\nprint('\\n' + '=' * 90)\nprint('KEY MEMORY BENEFITS:')\nprint('  - Agents share context and learnings')\nprint('  - Avoid repeating same research')\nprint('  - Build knowledge base over time')\nprint('  - Better coherence across tasks')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 4: Tool Integration\n\nCrewAI agents can use tools:\n- **Web search**: Find information online\n- **File operations**: Read/write files\n- **APIs**: Call external services\n- **Database**: Query data\n- **Custom tools**: Domain-specific operations"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import Callable, Any\nimport json\n\nclass Tool:\n    '''Tool that agents can use'''\n    \n    def __init__(self, name: str, description: str, func: Callable, requires_approval: bool = False):\n        self.name = name\n        self.description = description\n        self.func = func\n        self.requires_approval = requires_approval\n        self.usage_count = 0\n        self.error_count = 0\n    \n    def execute(self, *args, **kwargs) -> dict:\n        '''Execute tool with tracking'''\n        self.usage_count += 1\n        \n        try:\n            result = self.func(*args, **kwargs)\n            return {\n                'status': 'success',\n                'result': result,\n                'tool': self.name\n            }\n        except Exception as e:\n            self.error_count += 1\n            return {\n                'status': 'error',\n                'error': str(e),\n                'tool': self.name\n            }\n    \n    def get_metrics(self) -> dict:\n        '''Get tool usage metrics'''\n        return {\n            'tool': self.name,\n            'usage_count': self.usage_count,\n            'error_count': self.error_count,\n            'success_rate': (self.usage_count - self.error_count) / self.usage_count if self.usage_count > 0 else 0\n        }\n\n# Example tools\ndef web_search_tool(query: str) -> List[str]:\n    '''Mock web search'''\n    # In production: call Google, Bing, or specialized search API\n    return [\n        f'Result 1 for {query}',\n        f'Result 2 for {query}',\n        f'Result 3 for {query}',\n    ]\n\ndef file_write_tool(filename: str, content: str) -> str:\n    '''Mock file write'''\n    # In production: actual file I/O with safety checks\n    return f'Wrote {len(content)} chars to {filename}'\n\ndef api_call_tool(endpoint: str, params: dict) -> dict:\n    '''Mock API call'''\n    # In production: actual HTTP request\n    return {'status': 'success', 'data': f'Response from {endpoint}'}\n\nclass ToolEnabledAgent(EnhancedCrewAgent):\n    '''Agent that can use tools'''\n    \n    def __init__(self, *args, tool_objects: List[Tool] = None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tool_objects = {t.name: t for t in (tool_objects or [])}\n        self.tool_usage_log = []\n    \n    def use_tool(self, tool_name: str, *args, **kwargs) -> dict:\n        '''Use a tool'''\n        if tool_name not in self.tool_objects:\n            return {'status': 'error', 'error': f'Tool {tool_name} not available'}\n        \n        tool = self.tool_objects[tool_name]\n        \n        # Check if approval required\n        if tool.requires_approval:\n            print(f'  Tool {tool_name} requires approval...')\n            # In production: request approval\n        \n        # Execute tool\n        result = tool.execute(*args, **kwargs)\n        \n        # Log usage\n        self.tool_usage_log.append({\n            'tool': tool_name,\n            'timestamp': datetime.utcnow().isoformat(),\n            'status': result['status']\n        })\n        \n        return result\n\n# Create tools\ntools = [\n    Tool('web_search', 'Search the web for information', web_search_tool),\n    Tool('file_write', 'Write content to file', file_write_tool, requires_approval=True),\n    Tool('api_call', 'Call external API', api_call_tool),\n]\n\n# Create tool-enabled agent\nresearch_agent = ToolEnabledAgent(\n    role='Research Agent',\n    goal='Gather information',\n    backstory='Expert researcher',\n    capabilities=[AgentCapability('research', 0.9, 3, 30)],\n    tool_objects=tools\n)\n\nprint('\\nTOOL INTEGRATION DEMONSTRATION')\nprint('=' * 90)\n\n# Use tools\nprint('\\nAgent using tools:')\nresult1 = research_agent.use_tool('web_search', 'LLM agent frameworks')\nprint(f'  web_search: {result1[\"status\"]}')\n\nresult2 = research_agent.use_tool('file_write', 'report.md', 'Research findings...')\nprint(f'  file_write: {result2[\"status\"]}')\n\n# Check tool metrics\nprint('\\nTool Usage Metrics:')\nfor tool in tools:\n    metrics = tool.get_metrics()\n    print(f'  {metrics[\"tool\"]}: {metrics[\"usage_count\"]} calls, {metrics[\"success_rate\"]:.0%} success')\n\nprint('\\n' + '=' * 90)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 5: Quality Control and Validation\n\nProduction crews need output validation:\n- **Schema validation**: Ensure output matches expected format\n- **Quality scoring**: Automatic quality assessment\n- **Human review**: For critical outputs\n- **Revision loops**: Re-do if quality too low"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pydantic import BaseModel, Field, validator\nfrom typing import Literal\n\nclass TaskOutput(BaseModel):\n    '''Validated task output'''\n    content: str = Field(min_length=100, description='Main content')\n    quality_score: float = Field(ge=0.0, le=1.0, description='Quality score')\n    citations: List[str] = Field(default_factory=list, description='Sources cited')\n    confidence: float = Field(ge=0.0, le=1.0, description='Agent confidence')\n    requires_review: bool = Field(default=False, description='Needs human review')\n    \n    @validator('quality_score')\n    def check_quality(cls, v, values):\n        '''Validate quality meets minimum threshold'''\n        if v < 0.7:\n            raise ValueError(f'Quality score {v} below threshold 0.7')\n        return v\n\nclass QualityController:\n    '''Quality control for crew outputs'''\n    \n    def __init__(self, min_quality_score=0.7, enable_revision=True):\n        self.min_quality_score = min_quality_score\n        self.enable_revision = enable_revision\n        self.revision_history = []\n    \n    def assess_quality(self, output: str, expected_criteria: dict) -> float:\n        '''Assess output quality (simplified)'''\n        score = 0.5  # Baseline\n        \n        # Length check\n        if len(output) >= expected_criteria.get('min_length', 100):\n            score += 0.15\n        \n        # Keyword presence\n        required_keywords = expected_criteria.get('keywords', [])\n        keyword_coverage = sum(1 for kw in required_keywords if kw.lower() in output.lower())\n        score += (keyword_coverage / len(required_keywords)) * 0.2 if required_keywords else 0.1\n        \n        # Structure check (has sections)\n        if '\\n\\n' in output:  # Has paragraphs\n            score += 0.1\n        \n        # Citations present\n        if '[' in output and ']' in output:  # Has citations\n            score += 0.05\n        \n        return min(score, 1.0)\n    \n    def validate_output(self, output: str, expected_criteria: dict, max_revisions=2) -> dict:\n        '''Validate output with optional revision loop'''\n        \n        revision_count = 0\n        current_output = output\n        \n        while revision_count <= max_revisions:\n            # Assess quality\n            quality_score = self.assess_quality(current_output, expected_criteria)\n            \n            # Check if meets threshold\n            if quality_score >= self.min_quality_score:\n                return {\n                    'status': 'approved',\n                    'output': current_output,\n                    'quality_score': quality_score,\n                    'revisions': revision_count\n                }\n            \n            # Try revision\n            if self.enable_revision and revision_count < max_revisions:\n                print(f'Quality score {quality_score:.2f} below threshold, requesting revision...')\n                \n                # Request revision (in production: agent revises)\n                feedback = self._generate_feedback(current_output, quality_score, expected_criteria)\n                current_output = self._revise_output(current_output, feedback)\n                \n                revision_count += 1\n                \n                self.revision_history.append({\n                    'iteration': revision_count,\n                    'quality_score': quality_score,\n                    'feedback': feedback\n                })\n            else:\n                break\n        \n        # Failed quality check\n        return {\n            'status': 'failed',\n            'output': current_output,\n            'quality_score': quality_score,\n            'revisions': revision_count,\n            'requires_human_review': True\n        }\n    \n    def _generate_feedback(self, output: str, score: float, criteria: dict) -> str:\n        '''Generate improvement feedback'''\n        feedback = []\n        \n        if len(output) < criteria.get('min_length', 100):\n            feedback.append('Output too short, needs more detail')\n        \n        required_kw = criteria.get('keywords', [])\n        missing_kw = [kw for kw in required_kw if kw.lower() not in output.lower()]\n        if missing_kw:\n            feedback.append(f'Missing keywords: {missing_kw}')\n        \n        if '[' not in output:\n            feedback.append('Add citations to support claims')\n        \n        return '; '.join(feedback)\n    \n    def _revise_output(self, output: str, feedback: str) -> str:\n        '''Revise output based on feedback'''\n        # In production: agent revises\n        # For demo: simple improvement\n        revised = output + '\\n\\nAdditional details and [citation needed].'\n        return revised\n\nclass QualityAwareCrew:\n    '''Crew with automatic quality control'''\n    \n    def __init__(self, agents: List[EnhancedCrewAgent]):\n        self.agents = agents\n        self.quality_controller = QualityController(min_quality_score=0.7)\n    \n    def execute_with_qc(self, task: EnhancedTask, expected_criteria: dict) -> dict:\n        '''Execute task with quality control'''\n        \n        # Select agent\n        agent = None\n        for a in self.agents:\n            if a.can_handle(task.domain, task.required_expertise):\n                agent = a\n                break\n        \n        if not agent:\n            return {'status': 'no_agent'}\n        \n        # Execute task\n        result = agent.execute_task(task)\n        \n        if result['status'] != 'success':\n            return result\n        \n        # Quality control\n        qc_result = self.quality_controller.validate_output(\n            result['result'],\n            expected_criteria,\n            max_revisions=2\n        )\n        \n        return {\n            **result,\n            'qc_status': qc_result['status'],\n            'quality_score': qc_result['quality_score'],\n            'revisions': qc_result['revisions']\n        }\n\n# Demo quality control\nprint('\\nQUALITY CONTROL DEMONSTRATION')\nprint('=' * 90)\n\nqc_crew = QualityAwareCrew([writer])\n\ntask = EnhancedTask(\n    'Write article about RAG systems',\n    'Technical article',\n    domain='writing',\n    required_expertise=0.6\n)\n\ncriteria = {\n    'min_length': 200,\n    'keywords': ['retrieval', 'generation', 'embeddings'],\n}\n\nresult = qc_crew.execute_with_qc(task, criteria)\n\nprint(f'\\nExecution Result:')\nprint(f'  Status: {result[\"status\"]}')\nprint(f'  QC Status: {result[\"qc_status\"]}')\nprint(f'  Quality Score: {result[\"quality_score\"]:.2f}')\nprint(f'  Revisions: {result[\"revisions\"]}')\n\nprint('\\n' + '=' * 90)\nprint('KEY QC PATTERNS:')\nprint('  - Automatic quality assessment')\nprint('  - Revision loops for improvement')\nprint('  - Schema validation (Pydantic)')\nprint('  - Human escalation for low quality')\nprint('  - Track quality metrics over time')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Interview Questions: CrewAI Production Systems\n\n### For Senior/Staff Engineers"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "crewai_interview_questions = [\n    {\n        'level': 'Senior',\n        'question': 'You have a research crew (Researcher \u2192 Analyst \u2192 Writer \u2192 Editor). The Researcher takes 5 minutes, bottlenecking the entire pipeline. How do you optimize while maintaining output quality?',\n        'answer': '''\n**Problem Analysis:**\n- Current: Sequential execution\n- Bottleneck: Researcher (5 min)\n- Total time: 5 + 2 + 3 + 1 = 11 minutes\n- Throughput: ~5.5 reports/hour\n- Writer and Editor idle for first 7 minutes\n\n**Optimization Strategies:**\n\n**1. Parallel Research (Best: 60% reduction)**\n```python\nclass ParallelResearchCrew:\n    '''Split research among multiple specialists'''\n    \n    def __init__(self):\n        # Multiple researchers for different aspects\n        self.researchers = [\n            Agent('Researcher_Academic', expertise=['papers', 'research']),\n            Agent('Researcher_Industry', expertise=['products', 'companies']),\n            Agent('Researcher_News', expertise=['trends', 'events']),\n        ]\n        self.analyst = Agent('Analyst')\n        self.writer = Agent('Writer')\n        self.editor = Agent('Editor')\n    \n    async def execute(self, topic: str):\n        # Parallel research (5 min \u2192 2 min with 3 researchers)\n        research_tasks = [\n            researcher.research(f'{topic} - {researcher.expertise}')\n            for researcher in self.researchers\n        ]\n        research_results = await asyncio.gather(*research_tasks)\n        \n        # Consolidate research (2 min)\n        analysis = await self.analyst.analyze(research_results)\n        \n        # Sequential writing and editing (3 + 1 = 4 min)\n        draft = await self.writer.write(analysis)\n        final = await self.editor.edit(draft)\n        \n        return final\n\n# Time: max(2,2,2) + 2 + 3 + 1 = 8 min (27% faster)\n```\n\n**2. Pipeline with Batching (45% throughput increase)**\n```python\nclass PipelinedCrew:\n    '''Process multiple reports in pipeline'''\n    \n    def __init__(self):\n        self.researcher = Agent('Researcher')\n        self.analyst = Agent('Analyst')\n        self.writer = Agent('Writer')\n        self.editor = Agent('Editor')\n        \n        # Queues between stages\n        self.research_queue = Queue()\n        self.analysis_queue = Queue()\n        self.writing_queue = Queue()\n    \n    async def run_pipeline(self, topics: List[str]):\n        '''Run pipeline for multiple topics'''\n        \n        # Start all stages concurrently\n        await asyncio.gather(\n            self._research_stage(topics),\n            self._analysis_stage(),\n            self._writing_stage(),\n            self._editing_stage()\n        )\n    \n    async def _research_stage(self, topics):\n        for topic in topics:\n            result = await self.researcher.research(topic)  # 5 min\n            await self.research_queue.put(result)\n    \n    async def _analysis_stage(self):\n        while True:\n            research = await self.research_queue.get()\n            analysis = await self.analyst.analyze(research)  # 2 min\n            await self.analysis_queue.put(analysis)\n    \n    # ... similar for other stages\n\n# Throughput with pipeline:\n# Report 1: 11 min\n# Report 2: 11 min (starts while Report 1 in progress)\n# Steady state: 5 reports/5 min = 60 reports/hour vs 33/hour (82% improvement)\n```\n\n**3. Caching Research (50% improvement for repeated topics)**\n```python\nimport hashlib\n\nclass CachedResearcher:\n    '''Cache research results'''\n    \n    def __init__(self, cache_ttl_hours=24):\n        self.cache = {}\n        self.cache_ttl = cache_ttl_hours * 3600\n    \n    async def research(self, topic: str) -> dict:\n        # Generate cache key\n        cache_key = hashlib.md5(topic.lower().encode()).hexdigest()\n        \n        # Check cache\n        if cache_key in self.cache:\n            cached = self.cache[cache_key]\n            if time.time() - cached['timestamp'] < self.cache_ttl:\n                print(f'Cache hit for: {topic}')\n                return cached['result']\n        \n        # Perform research\n        result = await self._do_research(topic)  # 5 min\n        \n        # Cache result\n        self.cache[cache_key] = {\n            'result': result,\n            'timestamp': time.time()\n        }\n        \n        return result\n\n# With 40% cache hit rate: 5 min \u2192 3 min average (40% improvement)\n```\n\n**4. Adaptive Depth (30% improvement)**\n```python\nclass AdaptiveResearcher:\n    '''Adjust research depth based on requirements'''\n    \n    def research(self, topic: str, depth: str = 'standard'):\n        if depth == 'quick':\n            # Surface-level: 1 min\n            return self.quick_research(topic)\n        elif depth == 'standard':\n            # Normal: 5 min\n            return self.standard_research(topic)\n        elif depth == 'deep':\n            # Comprehensive: 15 min\n            return self.deep_research(topic)\n    \n    def determine_required_depth(self, task_metadata: dict) -> str:\n        '''Decide research depth needed'''\n        if task_metadata.get('audience') == 'executive':\n            return 'quick'  # High-level summary\n        elif task_metadata.get('technical_depth') == 'high':\n            return 'deep'  # Detailed technical analysis\n        else:\n            return 'standard'\n```\n\n**5. Combined Solution:**\n```python\nclass OptimizedCrew:\n    def __init__(self):\n        # Multiple parallel researchers with caching\n        self.researchers = [\n            CachedResearcher('Academic'),\n            CachedResearcher('Industry'),\n            CachedResearcher('News'),\n        ]\n        \n        # Adaptive depth selection\n        self.depth_controller = AdaptiveResearcher()\n        \n        # Pipeline for batch processing\n        self.pipeline = PipelinedCrew()\n    \n    async def execute_optimized(self, topic: str, metadata: dict):\n        # Determine depth\n        depth = self.depth_controller.determine_required_depth(metadata)\n        \n        # Parallel research with caching\n        research_results = await asyncio.gather(*[\n            researcher.research(topic, depth=depth)\n            for researcher in self.researchers\n        ])\n        \n        # Rest of pipeline...\n        return self._complete_pipeline(research_results)\n```\n\n**Results:**\n\n| Approach | Time | Throughput | Improvement |\n|----------|------|------------|-------------|\n| Baseline | 11 min | 5.5/hour | - |\n| Parallel Research | 8 min | 7.5/hour | +36% |\n| Pipeline (batch) | 11 min first, 5 min steady | 12/hour | +118% |\n| Caching (40% hit) | 8.2 min avg | 7.3/hour | +33% |\n| **Combined** | **5 min steady state** | **12/hour** | **+118%** |\n\n**Trade-offs:**\n- Parallel research: Slightly less deep but faster\n- Pipeline: Higher throughput but more complex\n- Caching: Staleness risk (set appropriate TTL)\n- Adaptive depth: May miss details on 'quick' mode\n\n**Recommendation:**\n- Use combined approach for production\n- Parallel research for speed\n- Caching with 24-hour TTL\n- Pipeline for batch processing\n- Adaptive depth based on use case\n- Monitor cache hit rate (target: 40%+)\n- A/B test quality: ensure optimizations don't degrade output\n        ''',\n    },\n    {\n        'level': 'Staff',\n        'question': 'Design a CrewAI system that produces a daily market intelligence report by coordinating 10+ specialist agents. Include task dependencies, parallel execution, quality control, and human review integration. System must complete in under 30 minutes.',\n        'answer': '''\n**Market Intelligence System Architecture:**\n\n**1. Agent Team Structure (10 specialists)**\n```python\nagent_team = {\n    # Research tier (parallel execution)\n    'news_researcher': {\n        'focus': 'Breaking news, press releases',\n        'sources': ['news_api', 'rss_feeds'],\n        'time': '5 min',\n    },\n    'social_researcher': {\n        'focus': 'Social media trends, sentiment',\n        'sources': ['twitter_api', 'reddit_api'],\n        'time': '5 min',\n    },\n    'market_researcher': {\n        'focus': 'Stock data, financial reports',\n        'sources': ['yahoo_finance', 'sec_filings'],\n        'time': '5 min',\n    },\n    'competitor_researcher': {\n        'focus': 'Competitor analysis',\n        'sources': ['crunchbase', 'product_hunt'],\n        'time': '5 min',\n    },\n    \n    # Analysis tier (depends on research)\n    'data_analyst': {\n        'focus': 'Quantitative analysis',\n        'time': '3 min',\n    },\n    'sentiment_analyst': {\n        'focus': 'Sentiment and narrative analysis',\n        'time': '3 min',\n    },\n    \n    # Synthesis tier\n    'synthesizer': {\n        'focus': 'Combine all inputs into coherent narrative',\n        'time': '4 min',\n    },\n    \n    # Writing tier\n    'writer': {\n        'focus': 'Draft report sections',\n        'time': '5 min',\n    },\n    \n    # Review tier\n    'fact_checker': {\n        'focus': 'Verify claims and citations',\n        'time': '3 min',\n    },\n    'editor': {\n        'focus': 'Polish and format final report',\n        'time': '2 min',\n    },\n}\n\n# Total sequential: 40 min (exceeds 30 min limit)\n# Need parallelization!\n```\n\n**2. Dependency Graph with Parallel Execution:**\n```python\nimport networkx as nx\n\nclass TaskDAG:\n    '''Directed Acyclic Graph for task dependencies'''\n    \n    def __init__(self):\n        self.graph = nx.DiGraph()\n        self.task_metadata = {}\n    \n    def add_task(self, task_id: str, agent: str, duration_min: float, dependencies: List[str] = None):\n        '''Add task to DAG'''\n        self.graph.add_node(task_id)\n        self.task_metadata[task_id] = {\n            'agent': agent,\n            'duration_min': duration_min,\n            'status': 'pending'\n        }\n        \n        # Add dependency edges\n        for dep in (dependencies or []):\n            self.graph.add_edge(dep, task_id)\n    \n    def get_execution_levels(self) -> List[List[str]]:\n        '''Get tasks grouped by dependency level (can execute in parallel)'''\n        return list(nx.topological_generations(self.graph))\n    \n    def get_critical_path(self) -> Tuple[List[str], float]:\n        '''Find critical path (longest path through DAG)'''\n        # Calculate longest path\n        levels = self.get_execution_levels()\n        \n        critical_path = []\n        total_time = 0\n        \n        for level in levels:\n            # Within each level, find longest task\n            longest_task = max(level, key=lambda t: self.task_metadata[t]['duration_min'])\n            critical_path.append(longest_task)\n            total_time += self.task_metadata[longest_task]['duration_min']\n        \n        return critical_path, total_time\n\n# Build task DAG\ndag = TaskDAG()\n\n# Research tier (can run in parallel)\ndag.add_task('research_news', 'news_researcher', 5.0)\ndag.add_task('research_social', 'social_researcher', 5.0)\ndag.add_task('research_market', 'market_researcher', 5.0)\ndag.add_task('research_competitor', 'competitor_researcher', 5.0)\n\n# Analysis tier (depends on research)\ndag.add_task('analyze_data', 'data_analyst', 3.0, \n            dependencies=['research_market', 'research_competitor'])\ndag.add_task('analyze_sentiment', 'sentiment_analyst', 3.0,\n            dependencies=['research_news', 'research_social'])\n\n# Synthesis (depends on all analysis)\ndag.add_task('synthesize', 'synthesizer', 4.0,\n            dependencies=['analyze_data', 'analyze_sentiment'])\n\n# Writing (depends on synthesis)\ndag.add_task('write_report', 'writer', 5.0,\n            dependencies=['synthesize'])\n\n# Review (depends on writing)\ndag.add_task('fact_check', 'fact_checker', 3.0,\n            dependencies=['write_report'])\ndag.add_task('edit', 'editor', 2.0,\n            dependencies=['fact_check'])\n\n# Analyze execution\nlevels = dag.get_execution_levels()\ncritical_path, critical_time = dag.get_critical_path()\n\nprint('\\nTASK EXECUTION PLAN')\nprint('=' * 80)\nfor i, level in enumerate(levels, 1):\n    max_time = max(dag.task_metadata[t]['duration_min'] for t in level)\n    print(f'\\nLevel {i} (parallel, {max_time} min):')\n    for task_id in level:\n        meta = dag.task_metadata[task_id]\n        print(f'  - {task_id}: {meta[\"agent\"]} ({meta[\"duration_min\"]} min)')\n\nprint(f'\\n{'=' * 80}')\nprint(f'Critical Path: {\" \u2192 \".join(critical_path)}')\nprint(f'Total Time: {critical_time} min')\nprint(f'Speedup: {40 / critical_time:.1f}x (40 min sequential \u2192 {critical_time} min parallel)')\n```\n\n**3. Execution Engine with Parallel Processing:**\n```python\nclass ParallelCrewExecutor:\n    '''Execute DAG with maximum parallelism'''\n    \n    def __init__(self, dag: TaskDAG, agents: Dict[str, Agent]):\n        self.dag = dag\n        self.agents = agents\n        self.results = {}\n    \n    async def execute_dag(self) -> dict:\n        '''Execute all tasks respecting dependencies'''\n        levels = self.dag.get_execution_levels()\n        \n        total_start = time.time()\n        \n        for level_num, level_tasks in enumerate(levels, 1):\n            print(f'\\nExecuting level {level_num} ({len(level_tasks)} tasks in parallel)...')\n            \n            level_start = time.time()\n            \n            # Execute all tasks in level concurrently\n            tasks = []\n            for task_id in level_tasks:\n                meta = self.dag.task_metadata[task_id]\n                agent = self.agents[meta['agent']]\n                \n                task = self._execute_task(task_id, agent)\n                tasks.append(task)\n            \n            # Wait for all tasks in level\n            level_results = await asyncio.gather(*tasks)\n            \n            # Store results\n            for task_id, result in zip(level_tasks, level_results):\n                self.results[task_id] = result\n            \n            level_time = time.time() - level_start\n            print(f'  Level {level_num} completed in {level_time:.1f}s')\n        \n        total_time = time.time() - total_start\n        \n        return {\n            'results': self.results,\n            'total_time_sec': total_time,\n            'levels_executed': len(levels)\n        }\n    \n    async def _execute_task(self, task_id: str, agent: Agent) -> dict:\n        '''Execute single task'''\n        # Simulate async work\n        meta = self.dag.task_metadata[task_id]\n        await asyncio.sleep(meta['duration_min'] * 0.01)  # Scaled for demo\n        \n        return {\n            'task_id': task_id,\n            'status': 'success',\n            'agent': meta['agent']\n        }\n```\n\n**4. Quality Control Gates:**\n```python\nclass QualityGate:\n    '''Quality control between stages'''\n    \n    def __init__(self, min_quality=0.75):\n        self.min_quality = min_quality\n    \n    async def validate_stage_output(self, output: str, stage: str) -> dict:\n        '''Validate output quality before next stage'''\n        \n        # Calculate quality score\n        quality_score = self._assess_quality(output, stage)\n        \n        if quality_score < self.min_quality:\n            return {\n                'passed': False,\n                'score': quality_score,\n                'action': 'revise',\n                'feedback': self._generate_feedback(output, stage)\n            }\n        \n        return {\n            'passed': True,\n            'score': quality_score\n        }\n    \n    def _assess_quality(self, output: str, stage: str) -> float:\n        '''Stage-specific quality assessment'''\n        score = 0.5\n        \n        if stage == 'research':\n            # Check for citations\n            if '[' in output and ']' in output:\n                score += 0.2\n            # Check for data\n            if any(char.isdigit() for char in output):\n                score += 0.15\n        \n        elif stage == 'analysis':\n            # Check for insights\n            if 'trend' in output.lower() or 'insight' in output.lower():\n                score += 0.2\n        \n        # Length check\n        if len(output) > 500:\n            score += 0.15\n        \n        return min(score, 1.0)\n```\n\n**5. Human Review Integration:**\n```python\nclass HumanReviewQueue:\n    '''Queue high-priority items for human review'''\n    \n    def __init__(self):\n        self.queue = []\n        self.reviewed = {}\n    \n    def submit_for_review(self, report_id: str, report: str, priority: str = 'normal'):\n        '''Submit report for human review'''\n        self.queue.append({\n            'report_id': report_id,\n            'report': report,\n            'priority': priority,\n            'submitted_at': datetime.utcnow().isoformat(),\n            'status': 'pending'\n        })\n        \n        # Send notification (Slack, email, etc.)\n        if priority == 'high':\n            self._send_urgent_notification(report_id)\n    \n    def get_pending_reviews(self, priority: str = None) -> List[dict]:\n        '''Get reports awaiting review'''\n        pending = [r for r in self.queue if r['status'] == 'pending']\n        \n        if priority:\n            pending = [r for r in pending if r['priority'] == priority]\n        \n        return pending\n```\n\n**6. Complete System:**\n```python\nclass MarketIntelligenceCrew:\n    '''Complete market intelligence system'''\n    \n    def __init__(self):\n        self.dag = self._build_dag()\n        self.agents = self._initialize_agents()\n        self.executor = ParallelCrewExecutor(self.dag, self.agents)\n        self.quality_gates = QualityGate(min_quality=0.75)\n        self.human_review = HumanReviewQueue()\n        self.cache = CachedResearcher(cache_ttl_hours=24)\n    \n    async def generate_daily_report(self, date: str) -> dict:\n        '''Generate complete market intelligence report'''\n        \n        print(f'Generating report for {date}...')\n        start = time.time()\n        \n        # Execute DAG\n        result = await self.executor.execute_dag()\n        \n        # Quality check\n        final_report = result['results']['edit']\n        qc_result = await self.quality_gates.validate_stage_output(final_report, 'final')\n        \n        elapsed = time.time() - start\n        \n        if qc_result['passed']:\n            print(f'\u2713 Report generated in {elapsed:.1f}s (quality: {qc_result[\"score\"]:.2f})')\n            \n            # Auto-publish if quality high enough\n            if qc_result['score'] >= 0.9:\n                return {'status': 'published', 'report': final_report}\n            else:\n                # Submit for human review\n                self.human_review.submit_for_review(date, final_report, priority='normal')\n                return {'status': 'pending_review', 'report': final_report}\n        else:\n            # Quality too low, escalate\n            self.human_review.submit_for_review(date, final_report, priority='high')\n            return {'status': 'needs_revision', 'feedback': qc_result['feedback']}\n```\n\n**Execution Timeline (Target: < 30 min):**\n```\nLevel 1 (0-5 min): Research [4 agents in parallel]\n  \u2514\u2500 news, social, market, competitor research\n\nLevel 2 (5-8 min): Analysis [2 agents in parallel]\n  \u2514\u2500 data analysis, sentiment analysis\n\nLevel 3 (8-12 min): Synthesis [1 agent]\n  \u2514\u2500 combine insights\n\nLevel 4 (12-17 min): Writing [1 agent]\n  \u2514\u2500 draft sections\n\nLevel 5 (17-22 min): Review [2 agents in parallel]\n  \u2514\u2500 fact check + edit\n\nTotal: 22 minutes \u2713 (under 30 min target)\n```\n\n**7. Monitoring Dashboard:**\n```python\nclass CrewMonitor:\n    '''Real-time monitoring of crew execution'''\n    \n    def __init__(self):\n        self.metrics = defaultdict(list)\n    \n    def track_execution(self, report_date: str, execution_result: dict):\n        self.metrics['execution_time'].append(execution_result['total_time_sec'])\n        self.metrics['report_date'].append(report_date)\n    \n    def get_dashboard_data(self) -> dict:\n        return {\n            'avg_execution_time_min': np.mean(self.metrics['execution_time']) / 60,\n            'p95_execution_time_min': np.percentile(self.metrics['execution_time'], 95) / 60,\n            'reports_generated': len(self.metrics['report_date']),\n            'success_rate': 0.95,  # Track separately\n        }\n```\n\n**Key Features:**\n- \u2713 10 specialist agents\n- \u2713 5-level DAG with parallelism\n- \u2713 Completes in 22 minutes (< 30 min target)\n- \u2713 Quality gates between stages\n- \u2713 Auto-publish or human review based on quality\n- \u2713 Caching for efficiency\n- \u2713 Monitoring and metrics\n\n**Cost Analysis:**\n- 10 agents \u00d7 3 min avg \u00d7 $0.02/min = $0.60 per report\n- 365 reports/year \u00d7 $0.60 = $219/year\n- vs. Human analyst: 4 hours \u00d7 $50/hour = $200/day = $73K/year\n- **Savings: 99.7%**\n        ''',\n    },\n]\n\nfor i, qa in enumerate(crewai_interview_questions, 1):\n    print(f'\\n{'=' * 100}')\n    print(f'Q{i} [{qa[\"level\"]} Level]')\n    print('=' * 100)\n    print(f'\\n{qa[\"question\"]}\\n')\n    print('ANSWER:')\n    print(qa['answer'])\n    print()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## CrewAI Framework - Complete Core Components\n\n### Based on 8-Hour Curriculum\n\nThis section covers all CrewAI core components with production implementations:\n- Agent (with role, goal, backstory)\n- Task (with expected output)\n- Crew (orchestration)\n- Tool (function integration)\n- Process (sequential vs hierarchical)\n- Memory (short-term, long-term, entity)\n- Output Parser (structured validation)\n- Flow (complex workflows)\n- Config (YAML-based setup)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 1: Agent - Complete Implementation\n\nCrewAI Agent with all features: role, goal, backstory, tools, delegation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List, Dict, Any, Optional, Callable\nfrom pydantic import BaseModel, Field\nimport json\n\nclass CrewAIAgent:\n    '''\n    Complete CrewAI Agent implementation.\n    \n    Core attributes:\n    - role: Job title and function\n    - goal: What the agent aims to achieve\n    - backstory: Context that shapes behavior\n    - tools: Functions the agent can use\n    - verbose: Logging level\n    - allow_delegation: Can delegate to other agents\n    - max_iter: Maximum iterations for task\n    '''\n    \n    def __init__(self,\n                 role: str,\n                 goal: str,\n                 backstory: str,\n                 tools: List = None,\n                 verbose: bool = True,\n                 allow_delegation: bool = False,\n                 max_iter: int = 15,\n                 memory: bool = True):\n        \n        self.role = role\n        self.goal = goal\n        self.backstory = backstory\n        self.tools = tools or []\n        self.verbose = verbose\n        self.allow_delegation = allow_delegation\n        self.max_iter = max_iter\n        self.memory_enabled = memory\n        \n        # Internal state\n        self.task_history = []\n        self.tool_usage = defaultdict(int)\n        self.delegation_count = 0\n    \n    def execute_task(self, task: 'CrewAITask', context: dict = None) -> dict:\n        '''\n        Execute a task with full CrewAI capabilities.\n        '''\n        \n        iterations = 0\n        result = None\n        \n        while iterations < self.max_iter:\n            iterations += 1\n            \n            if self.verbose:\n                print(f'  [{self.role}] Iteration {iterations}/{self.max_iter}')\n            \n            # Attempt to complete task\n            try:\n                # In production: LLM generates plan and executes\n                result = self._work_on_task(task, context)\n                \n                # Check if task complete\n                if self._is_task_complete(result, task.expected_output):\n                    break\n                \n            except Exception as e:\n                if self.verbose:\n                    print(f'  Error: {e}')\n                \n                # Try delegation if allowed\n                if self.allow_delegation and self.delegation_count < 2:\n                    result = self._delegate_task(task)\n                    self.delegation_count += 1\n                    break\n                else:\n                    result = {'error': str(e)}\n                    break\n        \n        # Store in history\n        self.task_history.append({\n            'task': task.description,\n            'result': result,\n            'iterations': iterations\n        })\n        \n        return {\n            'result': result,\n            'iterations': iterations,\n            'agent': self.role,\n            'tools_used': dict(self.tool_usage)\n        }\n    \n    def _work_on_task(self, task, context) -> str:\n        '''Work on task (calls LLM in production)'''\n        \n        # Build prompt from role, goal, backstory, and task\n        prompt = f'''Role: {self.role}\nGoal: {self.goal}\nBackstory: {self.backstory}\n\nTask: {task.description}\nExpected Output: {task.expected_output}\n\nProvide your response:'''\n        \n        # In production: call LLM\n        # response = llm.generate(prompt)\n        \n        # Mock response\n        response = f'[{self.role}] completed task: {task.description[:50]}...'\n        \n        # Use tools if needed\n        if self.tools and 'search' in task.description.lower():\n            tool_result = self._use_tool('search', task.description)\n            response += f'\\nTool result: {tool_result}'\n        \n        return response\n    \n    def _use_tool(self, tool_name: str, input_data: str) -> str:\n        '''Use a tool'''\n        self.tool_usage[tool_name] += 1\n        \n        # Find tool in list\n        for tool in self.tools:\n            if tool.name == tool_name:\n                return tool.run(input_data)\n        \n        return f'Tool {tool_name} not found'\n    \n    def _is_task_complete(self, result: Any, expected_output: str) -> bool:\n        '''Check if task meets expected output'''\n        # In production: Use semantic similarity or LLM evaluation\n        return result is not None and len(str(result)) > 50\n    \n    def _delegate_task(self, task) -> str:\n        '''Delegate task to another agent'''\n        if self.verbose:\n            print(f'  [{self.role}] Delegating task...')\n        return f'Task delegated by {self.role}'\n    \n    def get_performance_summary(self) -> dict:\n        '''Get agent performance metrics'''\n        return {\n            'role': self.role,\n            'tasks_completed': len(self.task_history),\n            'avg_iterations': np.mean([t['iterations'] for t in self.task_history]) if self.task_history else 0,\n            'tools_used': dict(self.tool_usage),\n            'delegations': self.delegation_count\n        }\n\nprint('CREWAI AGENT - COMPLETE IMPLEMENTATION')\nprint('=' * 90)\n\n# Create production-ready agent\nresearch_agent = CrewAIAgent(\n    role='Senior Research Analyst',\n    goal='Conduct thorough research and provide data-driven insights',\n    backstory='''You are a senior research analyst with 10 years of experience \nin market research and competitive analysis. You excel at finding reliable sources, \nvalidating information, and synthesizing complex data into actionable insights.''',\n    tools=[],  # Will add tools later\n    verbose=True,\n    allow_delegation=True,\n    max_iter=10,\n    memory=True\n)\n\nprint(f'\\nAgent Created:')\nprint(f'  Role: {research_agent.role}')\nprint(f'  Goal: {research_agent.goal}')\nprint(f'  Backstory: {research_agent.backstory[:80]}...')\nprint(f'  Max iterations: {research_agent.max_iter}')\nprint(f'  Can delegate: {research_agent.allow_delegation}')\n\nprint('\\n' + '=' * 90)\nprint('AGENT DESIGN PRINCIPLES:')\nprint('  - Role: Clear job title and function')\nprint('  - Goal: Specific, measurable objective')\nprint('  - Backstory: Adds context and personality')\nprint('  - Tools: Extend agent capabilities')\nprint('  - Delegation: Enable collaboration')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 2: Task - With Expected Output and Context"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CrewAITask:\n    '''\n    CrewAI Task with complete configuration.\n    \n    Attributes:\n    - description: What needs to be done\n    - expected_output: Criteria for completion\n    - agent: Assigned agent\n    - context: Input from previous tasks\n    - tools: Task-specific tools\n    - async_execution: Run asynchronously\n    - output_format: Structured output specification\n    '''\n    \n    def __init__(self,\n                 description: str,\n                 expected_output: str,\n                 agent: Optional[CrewAIAgent] = None,\n                 context: List['CrewAITask'] = None,\n                 tools: List = None,\n                 async_execution: bool = False,\n                 output_format: Optional[BaseModel] = None):\n        \n        self.task_id = hashlib.md5(description.encode()).hexdigest()[:8]\n        self.description = description\n        self.expected_output = expected_output\n        self.agent = agent\n        self.context = context or []  # Depends on these tasks\n        self.tools = tools or []\n        self.async_execution = async_execution\n        self.output_format = output_format\n        \n        self.result = None\n        self.status = 'pending'\n        self.execution_time_sec = 0\n    \n    def execute(self) -> dict:\n        '''Execute task'''\n        import time\n        start = time.time()\n        \n        # Get context from dependent tasks\n        context_data = self._gather_context()\n        \n        # Execute with agent\n        if self.agent:\n            result = self.agent.execute_task(self, context_data)\n            self.result = result['result']\n            self.status = 'completed' if result['result'] else 'failed'\n        else:\n            self.result = None\n            self.status = 'no_agent'\n        \n        self.execution_time_sec = time.time() - start\n        \n        # Validate output format if specified\n        if self.output_format and self.result:\n            try:\n                validated = self.output_format.parse_obj(json.loads(str(self.result)))\n                self.result = validated\n            except Exception as e:\n                self.status = 'validation_failed'\n        \n        return {\n            'task_id': self.task_id,\n            'status': self.status,\n            'result': self.result,\n            'execution_time': self.execution_time_sec\n        }\n    \n    def _gather_context(self) -> dict:\n        '''Gather results from context tasks'''\n        context_data = {}\n        for ctx_task in self.context:\n            if ctx_task.result:\n                context_data[ctx_task.task_id] = ctx_task.result\n        return context_data\n    \n    def __repr__(self):\n        return f'Task({self.task_id}, {self.description[:30]}...)'\n\n# Example task with output format\nclass ResearchOutput(BaseModel):\n    '''Structured research output'''\n    summary: str = Field(description='Brief summary')\n    key_findings: List[str] = Field(description='3-5 key findings')\n    sources: List[str] = Field(description='Cited sources')\n    confidence: float = Field(ge=0.0, le=1.0)\n\nprint('\\nCREWAI TASK - COMPLETE IMPLEMENTATION')\nprint('=' * 90)\n\n# Create tasks with dependencies\ntask1 = CrewAITask(\n    description='Research current trends in LLM agents',\n    expected_output='List of 5 trends with sources',\n    agent=research_agent,\n    output_format=ResearchOutput\n)\n\ntask2 = CrewAITask(\n    description='Analyze the research findings',\n    expected_output='Detailed analysis with insights',\n    context=[task1],  # Depends on task1\n    async_execution=False\n)\n\ntask3 = CrewAITask(\n    description='Write executive summary',\n    expected_output='2-paragraph summary',\n    context=[task1, task2],  # Depends on both\n    async_execution=False\n)\n\nprint(f'\\nTask chain created:')\nfor i, task in enumerate([task1, task2, task3], 1):\n    print(f'  {i}. {task.description}')\n    print(f'     Expected: {task.expected_output}')\n    if task.context:\n        print(f'     Depends on: {[t.task_id for t in task.context]}')\n\nprint('\\n' + '=' * 90)\nprint('TASK DESIGN PRINCIPLES:')\nprint('  - Clear description (what to do)')\nprint('  - Specific expected output (completion criteria)')\nprint('  - Dependencies via context parameter')\nprint('  - Output format for validation')\nprint('  - Async execution for parallelism')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 3: Crew - Complete Orchestration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from enum import Enum\n\nclass ProcessType(Enum):\n    SEQUENTIAL = 'sequential'\n    HIERARCHICAL = 'hierarchical'\n\nclass CrewAICrew:\n    '''\n    Complete CrewAI Crew implementation.\n    \n    Features:\n    - Sequential or hierarchical process\n    - Task dependency management\n    - Agent coordination\n    - Memory sharing\n    - Output aggregation\n    - Progress tracking\n    '''\n    \n    def __init__(self,\n                 agents: List[CrewAIAgent],\n                 tasks: List[CrewAITask],\n                 process: ProcessType = ProcessType.SEQUENTIAL,\n                 verbose: bool = True,\n                 memory: bool = True,\n                 cache: bool = True):\n        \n        self.agents = agents\n        self.tasks = tasks\n        self.process = process\n        self.verbose = verbose\n        self.memory_enabled = memory\n        self.cache_enabled = cache\n        \n        # Initialize crew memory\n        if memory:\n            self.crew_memory = CrewMemory()\n        \n        self.execution_results = []\n    \n    def kickoff(self) -> dict:\n        '''\n        Start crew execution.\n        '''\n        if self.verbose:\n            print(f'\\nCrew starting ({self.process.value} mode)...')\n            print(f'Agents: {[a.role for a in self.agents]}')\n            print(f'Tasks: {len(self.tasks)}')\n            print('-' * 80)\n        \n        if self.process == ProcessType.SEQUENTIAL:\n            return self._execute_sequential()\n        elif self.process == ProcessType.HIERARCHICAL:\n            return self._execute_hierarchical()\n    \n    def _execute_sequential(self) -> dict:\n        '''Execute tasks sequentially'''\n        \n        for i, task in enumerate(self.tasks, 1):\n            if self.verbose:\n                print(f'\\nTask {i}/{len(self.tasks)}: {task.description[:50]}...')\n            \n            # Execute task\n            result = task.execute()\n            \n            # Store result\n            self.execution_results.append(result)\n            \n            # Update crew memory\n            if self.memory_enabled:\n                self.crew_memory.add_short_term({\n                    'task_id': task.task_id,\n                    'result': result['result']\n                })\n            \n            if self.verbose:\n                print(f'  Status: {result[\"status\"]}')\n                print(f'  Time: {result[\"execution_time\"]:.2f}s')\n        \n        return {\n            'results': self.execution_results,\n            'total_tasks': len(self.tasks),\n            'successful': sum(1 for r in self.execution_results if r['status'] == 'completed'),\n            'process': self.process.value\n        }\n    \n    def _execute_hierarchical(self) -> dict:\n        '''Execute with manager delegation'''\n        \n        # In hierarchical mode, manager agent coordinates\n        manager = self._get_or_create_manager()\n        \n        # Manager delegates tasks to specialists\n        for task in self.tasks:\n            # Manager selects best agent\n            selected_agent = manager.select_agent_for_task(task, self.agents)\n            task.agent = selected_agent\n            \n            # Execute\n            result = task.execute()\n            self.execution_results.append(result)\n        \n        return {\n            'results': self.execution_results,\n            'total_tasks': len(self.tasks),\n            'successful': sum(1 for r in self.execution_results if r['status'] == 'completed'),\n            'process': self.process.value,\n            'manager': manager.role\n        }\n    \n    def _get_or_create_manager(self) -> CrewAIAgent:\n        '''Get manager agent for hierarchical process'''\n        # Check if manager exists\n        for agent in self.agents:\n            if 'manager' in agent.role.lower():\n                return agent\n        \n        # Create default manager\n        return CrewAIAgent(\n            role='Project Manager',\n            goal='Coordinate team and ensure task completion',\n            backstory='Experienced project manager',\n            allow_delegation=True\n        )\n\nprint('CREWAI CREW - COMPLETE ORCHESTRATION')\nprint('=' * 90)\n\n# Create complete crew\ncrew = CrewAICrew(\n    agents=[research_agent],  # Add more agents as needed\n    tasks=[task1, task2, task3],\n    process=ProcessType.SEQUENTIAL,\n    verbose=True,\n    memory=True,\n    cache=True\n)\n\n# Execute crew\nresult = crew.kickoff()\n\nprint(f'\\n{'=' * 90}')\nprint(f'Crew execution complete!')\nprint(f'  Total tasks: {result[\"total_tasks\"]}')\nprint(f'  Successful: {result[\"successful\"]}')\nprint(f'  Process: {result[\"process\"]}')\n\nprint('\\n' + '=' * 90)\nprint('CREW CONFIGURATION OPTIONS:')\nprint('  - process: sequential (ordered) vs hierarchical (manager delegates)')\nprint('  - verbose: Enable detailed logging')\nprint('  - memory: Share context across tasks')\nprint('  - cache: Reuse results from similar tasks')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 4: Tool - Function Integration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pydantic import BaseModel, Field\n\nclass CrewAITool:\n    '''\n    CrewAI Tool for extending agent capabilities.\n    \n    Features:\n    - Name and description\n    - Input schema validation\n    - Output schema validation\n    - Error handling\n    - Usage tracking\n    - Caching\n    '''\n    \n    def __init__(self,\n                 name: str,\n                 description: str,\n                 func: Callable,\n                 args_schema: Optional[BaseModel] = None,\n                 cache_results: bool = False):\n        \n        self.name = name\n        self.description = description\n        self.func = func\n        self.args_schema = args_schema\n        self.cache_enabled = cache_results\n        \n        self.call_count = 0\n        self.error_count = 0\n        self.cache = {}\n    \n    def run(self, *args, **kwargs) -> Any:\n        '''Execute tool with validation and caching'''\n        \n        self.call_count += 1\n        \n        # Check cache\n        if self.cache_enabled:\n            cache_key = self._get_cache_key(args, kwargs)\n            if cache_key in self.cache:\n                return self.cache[cache_key]\n        \n        # Validate args if schema provided\n        if self.args_schema:\n            try:\n                validated_args = self.args_schema(**kwargs)\n                kwargs = validated_args.dict()\n            except Exception as e:\n                self.error_count += 1\n                return {'error': f'Invalid arguments: {e}'}\n        \n        # Execute function\n        try:\n            result = self.func(*args, **kwargs)\n            \n            # Cache result\n            if self.cache_enabled:\n                self.cache[cache_key] = result\n            \n            return result\n            \n        except Exception as e:\n            self.error_count += 1\n            return {'error': str(e)}\n    \n    def _get_cache_key(self, args, kwargs) -> str:\n        '''Generate cache key'''\n        key_str = f'{args}:{kwargs}'\n        return hashlib.md5(key_str.encode()).hexdigest()[:8]\n    \n    def get_metrics(self) -> dict:\n        '''Get tool usage metrics'''\n        return {\n            'name': self.name,\n            'call_count': self.call_count,\n            'error_count': self.error_count,\n            'success_rate': (self.call_count - self.error_count) / self.call_count if self.call_count > 0 else 0,\n            'cache_size': len(self.cache) if self.cache_enabled else 0\n        }\n\n# Define tools\nclass WebSearchArgs(BaseModel):\n    query: str = Field(description='Search query')\n    num_results: int = Field(default=5, description='Number of results')\n\ndef web_search_func(query: str, num_results: int = 5) -> List[str]:\n    '''Mock web search'''\n    return [f'Result {i+1} for \"{query}\"' for i in range(num_results)]\n\nclass FileWriteArgs(BaseModel):\n    filename: str = Field(description='Output filename')\n    content: str = Field(description='Content to write')\n\ndef file_write_func(filename: str, content: str) -> str:\n    '''Mock file write'''\n    return f'Wrote {len(content)} characters to {filename}'\n\nprint('CREWAI TOOLS - FUNCTION INTEGRATION')\nprint('=' * 90)\n\n# Create tools\nweb_search = CrewAITool(\n    name='web_search',\n    description='Search the web for information',\n    func=web_search_func,\n    args_schema=WebSearchArgs,\n    cache_results=True\n)\n\nfile_write = CrewAITool(\n    name='file_write',\n    description='Write content to a file',\n    func=file_write_func,\n    args_schema=FileWriteArgs,\n    cache_results=False\n)\n\n# Use tools\nprint('\\nUsing tools:')\nresult1 = web_search.run(query='LLM agent frameworks', num_results=3)\nprint(f'  web_search: {len(result1)} results')\n\nresult2 = web_search.run(query='LLM agent frameworks', num_results=3)  # Cache hit\nprint(f'  web_search (cached): {len(result2)} results')\n\nresult3 = file_write.run(filename='report.md', content='Research findings...')\nprint(f'  file_write: {result3}')\n\nprint('\\nTool Metrics:')\nfor tool in [web_search, file_write]:\n    metrics = tool.get_metrics()\n    print(f'  {metrics[\"name\"]}: {metrics[\"call_count\"]} calls, {metrics[\"success_rate\"]:.0%} success, cache: {metrics[\"cache_size\"]}')\n\nprint('\\n' + '=' * 90)\nprint('TOOL DESIGN PATTERNS:')\nprint('  - Clear name and description')\nprint('  - Input validation with Pydantic schemas')\nprint('  - Error handling and reporting')\nprint('  - Optional caching for expensive operations')\nprint('  - Usage metrics for monitoring')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 5: Process Types - Sequential vs Hierarchical"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('PROCESS TYPES - WHEN TO USE EACH')\nprint('=' * 90)\n\nprocess_comparison = {\n    'Sequential': {\n        'Description': 'Tasks execute in order, one after another',\n        'Best for': [\n            'Pipeline workflows (research \u2192 analyze \u2192 write)',\n            'When task B depends on task A output',\n            'Simple, predictable flows',\n            'Clear task ordering'\n        ],\n        'Pros': [\n            'Simple to understand and debug',\n            'Predictable execution order',\n            'Easy to implement',\n            'Clear data flow'\n        ],\n        'Cons': [\n            'No parallelism (slower)',\n            'One failure blocks entire pipeline',\n            'Underutilizes resources'\n        ],\n        'Example': '''crew = CrewAICrew(\n    agents=[researcher, analyst, writer],\n    tasks=[research_task, analysis_task, writing_task],\n    process=ProcessType.SEQUENTIAL\n)'''\n    },\n    \n    'Hierarchical': {\n        'Description': 'Manager delegates tasks to specialist agents',\n        'Best for': [\n            'Large teams (5+ agents)',\n            'Complex task assignment logic',\n            'Dynamic workload distribution',\n            'When expertise matching matters'\n        ],\n        'Pros': [\n            'Intelligent task distribution',\n            'Load balancing across agents',\n            'Specialist expertise utilized',\n            'Scalable to large teams'\n        ],\n        'Cons': [\n            'More complex to set up',\n            'Manager can be bottleneck',\n            'Requires good manager prompting',\n            'Harder to debug'\n        ],\n        'Example': '''crew = CrewAICrew(\n    agents=[manager, specialist1, specialist2, specialist3],\n    tasks=tasks,\n    process=ProcessType.HIERARCHICAL\n)'''\n    },\n}\n\nfor process_name, details in process_comparison.items():\n    print(f'\\n{process_name} Process')\n    print('-' * 80)\n    print(f\"Description: {details['Description']}\\n\")\n    \n    print('Best for:')\n    for item in details['Best for']:\n        print(f'  \u2022 {item}')\n    \n    print('\\nPros:')\n    for pro in details['Pros']:\n        print(f'  + {pro}')\n    \n    print('\\nCons:')\n    for con in details['Cons']:\n        print(f'  - {con}')\n    \n    print(f'\\nExample:\\n{details[\"Example\"]}')\n\nprint('\\n' + '=' * 90)\nprint('DECISION GUIDE:')\nprint('  - Use SEQUENTIAL for: 3-5 agents, clear dependencies, simple flows')\nprint('  - Use HIERARCHICAL for: 5+ agents, complex routing, large teams')\nprint('  - Start with SEQUENTIAL, upgrade to HIERARCHICAL as team grows')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 6: Memory System - Complete Implementation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CrewMemorySystem:\n    '''\n    Complete memory system for CrewAI.\n    \n    Memory types:\n    - Short-term: Recent conversation\n    - Long-term: Persistent facts\n    - Entity: Track entities across tasks\n    - Procedural: Learn from past actions\n    '''\n    \n    def __init__(self):\n        self.short_term = deque(maxlen=50)  # Last 50 items\n        self.long_term = {}  # Key-value store\n        self.entity_memory = defaultdict(dict)  # Entity attributes\n        self.procedural = []  # Action patterns\n    \n    def add_short_term(self, item: dict):\n        '''Add to short-term memory'''\n        self.short_term.append({\n            **item,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n    \n    def add_long_term(self, key: str, value: Any, importance: float = 0.5):\n        '''Store in long-term memory'''\n        self.long_term[key] = {\n            'value': value,\n            'importance': importance,\n            'stored_at': datetime.utcnow().isoformat(),\n            'access_count': 0\n        }\n    \n    def get_long_term(self, key: str) -> Optional[Any]:\n        '''Retrieve from long-term memory'''\n        if key in self.long_term:\n            self.long_term[key]['access_count'] += 1\n            return self.long_term[key]['value']\n        return None\n    \n    def track_entity(self, entity_name: str, attributes: dict):\n        '''Track entity across tasks'''\n        self.entity_memory[entity_name].update(attributes)\n        self.entity_memory[entity_name]['last_updated'] = datetime.utcnow().isoformat()\n    \n    def get_entity(self, entity_name: str) -> dict:\n        '''Get entity information'''\n        return dict(self.entity_memory.get(entity_name, {}))\n    \n    def learn_procedure(self, action: str, outcome: str, success: bool):\n        '''Learn from action outcomes'''\n        self.procedural.append({\n            'action': action,\n            'outcome': outcome,\n            'success': success,\n            'timestamp': datetime.utcnow().isoformat()\n        })\n    \n    def get_relevant_memories(self, query: str, top_k: int = 5) -> dict:\n        '''Get relevant memories for current task'''\n        # In production: use embedding similarity\n        # For demo: return recent items\n        return {\n            'short_term': list(self.short_term)[-top_k:],\n            'long_term_count': len(self.long_term),\n            'entities_tracked': len(self.entity_memory),\n            'procedures_learned': len(self.procedural)\n        }\n    \n    def summarize(self) -> dict:\n        '''Get memory statistics'''\n        return {\n            'short_term_items': len(self.short_term),\n            'long_term_facts': len(self.long_term),\n            'entities_tracked': len(self.entity_memory),\n            'procedures_learned': len(self.procedural),\n        }\n\nprint('CREWAI MEMORY SYSTEM')\nprint('=' * 90)\n\nmemory = CrewMemorySystem()\n\n# Demonstrate memory usage\nprint('\\nAdding memories...')\n\n# Short-term\nmemory.add_short_term({'task': 'Research LLMs', 'result': 'Found 5 papers'})\nmemory.add_short_term({'task': 'Analyze results', 'result': 'Key insight: Agents improve quality'})\n\n# Long-term\nmemory.add_long_term('company_name', 'TechCorp', importance=0.9)\nmemory.add_long_term('project_deadline', '2025-12-31', importance=0.8)\n\n# Entity tracking\nmemory.track_entity('GPT-4', {'type': 'LLM', 'vendor': 'OpenAI', 'cost_per_1k': 0.03})\nmemory.track_entity('Claude', {'type': 'LLM', 'vendor': 'Anthropic', 'cost_per_1k': 0.024})\n\n# Procedural learning\nmemory.learn_procedure('web_search', 'Found 10 results', success=True)\nmemory.learn_procedure('api_call', 'Timeout error', success=False)\n\nprint('\\nMemory summary:')\nsummary = memory.summarize()\nfor key, value in summary.items():\n    print(f'  {key}: {value}')\n\nprint('\\nEntity example:')\nprint(f'  GPT-4: {memory.get_entity(\"GPT-4\")}')\n\nprint('\\n' + '=' * 90)\nprint('MEMORY BENEFITS:')\nprint('  - Short-term: Conversation context')\nprint('  - Long-term: Important facts preserved')\nprint('  - Entity: Track objects across tasks')\nprint('  - Procedural: Learn from experience')\nprint('  - Shared across all crew agents')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 7: Output Parser - Structured Output Validation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CrewAIOutputParser:\n    '''\n    Parse and validate crew outputs.\n    \n    Features:\n    - Schema validation\n    - Multiple format support (JSON, Markdown, custom)\n    - Extraction from LLM responses\n    - Retry on parse failure\n    '''\n    \n    def __init__(self, output_schema: Optional[BaseModel] = None, format_type: str = 'json'):\n        self.output_schema = output_schema\n        self.format_type = format_type\n    \n    def parse(self, raw_output: str, max_retries: int = 2) -> Any:\n        '''Parse and validate output'''\n        \n        for attempt in range(max_retries):\n            try:\n                if self.format_type == 'json':\n                    return self._parse_json(raw_output)\n                elif self.format_type == 'markdown':\n                    return self._parse_markdown(raw_output)\n                else:\n                    return raw_output\n                    \n            except Exception as e:\n                if attempt == max_retries - 1:\n                    raise ValueError(f'Failed to parse output: {e}')\n    \n    def _parse_json(self, raw_output: str) -> Any:\n        '''Extract and validate JSON'''\n        # Extract JSON from markdown code blocks\n        if '```json' in raw_output:\n            json_str = raw_output.split('```json')[1].split('```')[0].strip()\n        elif '```' in raw_output:\n            json_str = raw_output.split('```')[1].split('```')[0].strip()\n        else:\n            json_str = raw_output.strip()\n        \n        # Parse JSON\n        data = json.loads(json_str)\n        \n        # Validate against schema\n        if self.output_schema:\n            validated = self.output_schema(**data)\n            return validated\n        \n        return data\n    \n    def _parse_markdown(self, raw_output: str) -> dict:\n        '''Parse markdown output'''\n        # Extract sections\n        sections = {}\n        current_section = 'content'\n        current_content = []\n        \n        for line in raw_output.split('\\n'):\n            if line.startswith('##'):\n                # Save previous section\n                if current_content:\n                    sections[current_section] = '\\n'.join(current_content)\n                \n                # Start new section\n                current_section = line.replace('#', '').strip()\n                current_content = []\n            else:\n                current_content.append(line)\n        \n        # Save last section\n        if current_content:\n            sections[current_section] = '\\n'.join(current_content)\n        \n        return sections\n\nclass ReportOutput(BaseModel):\n    '''Example output schema for report'''\n    title: str\n    executive_summary: str\n    key_findings: List[str]\n    recommendations: List[str]\n    confidence_score: float = Field(ge=0.0, le=1.0)\n\nprint('OUTPUT PARSER DEMONSTRATION')\nprint('=' * 90)\n\n# Create parser with schema\nparser = CrewAIOutputParser(\n    output_schema=ReportOutput,\n    format_type='json'\n)\n\n# Mock LLM output\nllm_output = '''```json\n{\n  \"title\": \"LLM Agent Analysis\",\n  \"executive_summary\": \"Analysis of current LLM agent frameworks.\",\n  \"key_findings\": [\n    \"AutoGen excels at code generation\",\n    \"CrewAI best for role-based delegation\",\n    \"LangGraph ideal for complex workflows\"\n  ],\n  \"recommendations\": [\n    \"Use AutoGen for coding tasks\",\n    \"Use CrewAI for content creation\"\n  ],\n  \"confidence_score\": 0.85\n}\n```'''\n\nprint('\\nParsing LLM output...')\ntry:\n    parsed = parser.parse(llm_output)\n    print(f'\u2713 Successfully parsed and validated')\n    print(f'  Title: {parsed.title}')\n    print(f'  Key findings: {len(parsed.key_findings)}')\n    print(f'  Confidence: {parsed.confidence_score}')\nexcept Exception as e:\n    print(f'\u2717 Parse failed: {e}')\n\nprint('\\n' + '=' * 90)\nprint('OUTPUT PARSER BENEFITS:')\nprint('  - Ensures output matches expected format')\nprint('  - Automatic extraction from markdown blocks')\nprint('  - Schema validation with Pydantic')\nprint('  - Retry on parse failure')\nprint('  - Multiple format support')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 8: Flow - Complex Multi-Step Workflows"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CrewAIFlow:\n    '''\n    CrewAI Flow for complex multi-crew workflows.\n    \n    Features:\n    - Orchestrate multiple crews\n    - Conditional routing between crews\n    - State management across crews\n    - Error handling and recovery\n    '''\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.crews = {}\n        self.flow_state = {}\n        self.flow_history = []\n    \n    def add_crew(self, crew_name: str, crew: CrewAICrew):\n        '''Add crew to flow'''\n        self.crews[crew_name] = crew\n    \n    def execute_flow(self, initial_input: dict, flow_steps: List[dict]) -> dict:\n        '''\n        Execute complex flow.\n        \n        flow_steps format:\n        [\n            {'crew': 'research_crew', 'condition': lambda state: True},\n            {'crew': 'analysis_crew', 'condition': lambda state: state['quality'] > 0.7},\n            ...\n        ]\n        '''\n        \n        self.flow_state = initial_input\n        \n        for step_num, step in enumerate(flow_steps, 1):\n            crew_name = step['crew']\n            condition = step.get('condition', lambda s: True)\n            \n            # Check condition\n            if not condition(self.flow_state):\n                print(f'Step {step_num}: Skipping {crew_name} (condition not met)')\n                continue\n            \n            print(f'\\nStep {step_num}: Executing {crew_name}...')\n            \n            # Execute crew\n            if crew_name in self.crews:\n                crew_result = self.crews[crew_name].kickoff()\n                \n                # Update flow state\n                self.flow_state[crew_name] = crew_result\n                \n                # Store in history\n                self.flow_history.append({\n                    'step': step_num,\n                    'crew': crew_name,\n                    'result': crew_result\n                })\n            else:\n                print(f'  Warning: Crew {crew_name} not found')\n        \n        return {\n            'flow_name': self.name,\n            'steps_executed': len(self.flow_history),\n            'final_state': self.flow_state,\n            'history': self.flow_history\n        }\n\nprint('CREWAI FLOW - COMPLEX WORKFLOWS')\nprint('=' * 90)\n\n# Example: Multi-stage content creation flow\nflow = CrewAIFlow('content_creation_pipeline')\n\n# Create crews for different stages\nresearch_crew = CrewAICrew(\n    agents=[research_agent],\n    tasks=[task1],\n    process=ProcessType.SEQUENTIAL\n)\n\nflow.add_crew('research', research_crew)\n# flow.add_crew('analysis', analysis_crew)  # Would add more\n# flow.add_crew('writing', writing_crew)\n\n# Define flow steps\nflow_steps = [\n    {'crew': 'research', 'condition': lambda s: True},\n    # {'crew': 'analysis', 'condition': lambda s: s.get('research_quality', 0) > 0.7},\n    # {'crew': 'writing', 'condition': lambda s: s.get('analysis_complete', False)},\n]\n\nprint('\\nExecuting flow...')\nresult = flow.execute_flow(\n    initial_input={'topic': 'LLM Agents'},\n    flow_steps=flow_steps\n)\n\nprint(f'\\nFlow complete: {result[\"steps_executed\"]} steps executed')\n\nprint('\\n' + '=' * 90)\nprint('FLOW BENEFITS:')\nprint('  - Orchestrate multiple crews')\nprint('  - Conditional routing between stages')\nprint('  - State management across workflow')\nprint('  - Error recovery at crew level')\nprint('  - Complex multi-stage pipelines')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 9: Config - YAML-Based Setup\n\nDefine crews and agents declaratively."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import yaml\n\nclass CrewAIConfig:\n    '''\n    Load CrewAI configuration from YAML.\n    \n    Enables:\n    - Declarative crew definition\n    - Version control of configs\n    - Easy modifications without code changes\n    - Environment-specific configs\n    '''\n    \n    def __init__(self, config_file: str = None):\n        self.config = {}\n        if config_file:\n            self.load_from_file(config_file)\n    \n    def load_from_file(self, file_path: str):\n        '''Load configuration from YAML file'''\n        with open(file_path, 'r') as f:\n            self.config = yaml.safe_load(f)\n    \n    def load_from_dict(self, config_dict: dict):\n        '''Load from dictionary'''\n        self.config = config_dict\n    \n    def build_crew(self) -> CrewAICrew:\n        '''Build crew from configuration'''\n        \n        # Create agents from config\n        agents = []\n        for agent_config in self.config.get('agents', []):\n            agent = CrewAIAgent(\n                role=agent_config['role'],\n                goal=agent_config['goal'],\n                backstory=agent_config['backstory'],\n                tools=agent_config.get('tools', []),\n                verbose=agent_config.get('verbose', True),\n                allow_delegation=agent_config.get('allow_delegation', False),\n                memory=agent_config.get('memory', True)\n            )\n            agents.append(agent)\n        \n        # Create tasks from config\n        tasks = []\n        agent_map = {a.role: a for a in agents}\n        \n        for task_config in self.config.get('tasks', []):\n            # Find agent by role\n            agent = agent_map.get(task_config.get('agent_role'))\n            \n            task = CrewAITask(\n                description=task_config['description'],\n                expected_output=task_config['expected_output'],\n                agent=agent,\n                async_execution=task_config.get('async', False)\n            )\n            tasks.append(task)\n        \n        # Create crew\n        crew_config = self.config.get('crew', {})\n        process_str = crew_config.get('process', 'sequential')\n        process = ProcessType.SEQUENTIAL if process_str == 'sequential' else ProcessType.HIERARCHICAL\n        \n        crew = CrewAICrew(\n            agents=agents,\n            tasks=tasks,\n            process=process,\n            verbose=crew_config.get('verbose', True),\n            memory=crew_config.get('memory', True)\n        )\n        \n        return crew\n\nprint('CREWAI CONFIG - YAML SETUP')\nprint('=' * 90)\n\n# Example configuration\nexample_config = {\n    'agents': [\n        {\n            'role': 'Research Specialist',\n            'goal': 'Find comprehensive information',\n            'backstory': 'Expert researcher with access to multiple sources',\n            'tools': ['web_search', 'arxiv_search'],\n            'allow_delegation': False,\n            'memory': True\n        },\n        {\n            'role': 'Technical Writer',\n            'goal': 'Create clear technical documentation',\n            'backstory': 'Technical writer with 8 years experience',\n            'tools': ['markdown_formatter'],\n            'allow_delegation': False,\n            'memory': True\n        }\n    ],\n    'tasks': [\n        {\n            'description': 'Research latest RAG techniques',\n            'expected_output': 'Summary of 5 techniques with sources',\n            'agent_role': 'Research Specialist',\n            'async': False\n        },\n        {\n            'description': 'Write technical blog post about research',\n            'expected_output': '1500-word blog post',\n            'agent_role': 'Technical Writer',\n            'async': False\n        }\n    ],\n    'crew': {\n        'process': 'sequential',\n        'verbose': True,\n        'memory': True\n    }\n}\n\nprint('\\nExample Configuration:')\nprint(yaml.dump(example_config, default_flow_style=False, indent=2)[:500] + '...')\n\n# Build crew from config\nconfig_manager = CrewAIConfig()\nconfig_manager.load_from_dict(example_config)\n\ntry:\n    configured_crew = config_manager.build_crew()\n    print(f'\\n\u2713 Crew built from config:')\n    print(f'  Agents: {len(configured_crew.agents)}')\n    print(f'  Tasks: {len(configured_crew.tasks)}')\n    print(f'  Process: {configured_crew.process.value}')\nexcept Exception as e:\n    print(f'\\n\u2717 Error building crew: {e}')\n\nprint('\\n' + '=' * 90)\nprint('CONFIG BENEFITS:')\nprint('  - Version control configurations')\nprint('  - No code changes for crew modifications')\nprint('  - Environment-specific configs (dev, staging, prod)')\nprint('  - Easy experimentation with different setups')\nprint('  - Shareable configurations across team')"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}