{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 3: LangChain\n",
        "\n",
        "## Applied AI Scientist Field Notes - Expanded Edition\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3: LangChain - Production Chains, Agents, and Evaluation\n",
        "\n",
        "### Topics\n",
        "1. Chain design patterns\n",
        "2. Error handling and retries\n",
        "3. Memory management\n",
        "4. Tool use and function calling\n",
        "5. Agent architectures\n",
        "6. Evaluation frameworks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain-community langchain-openai\n",
        "%pip install -q pydantic\n",
        "\n",
        "print('LangChain installed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Robust Chain Design\n",
        "\n",
        "Production chains need:\n",
        "- Structured output parsing\n",
        "- Automatic retries\n",
        "- Validation\n",
        "- Error recovery\n",
        "- Metrics tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "from pydantic import BaseModel, Field\n",
        "import time\n",
        "\n",
        "class RobustChain:\n",
        "    '''Production chain with retry and observability'''\n",
        "    \n",
        "    def __init__(self, llm_func: Callable, parser: Callable, max_retries=3):\n",
        "        self.llm_func = llm_func\n",
        "        self.parser = parser\n",
        "        self.max_retries = max_retries\n",
        "        self.metrics = {'calls': 0, 'retries': 0, 'failures': 0, 'total_latency': 0}\n",
        "    \n",
        "    def invoke(self, prompt: str) -> Any:\n",
        "        '''Execute with retry logic'''\n",
        "        self.metrics['calls'] += 1\n",
        "        start = time.time()\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = self.llm_func(prompt)\n",
        "                result = self.parser(response)\n",
        "                \n",
        "                self.metrics['total_latency'] += (time.time() - start)\n",
        "                return result\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.metrics['retries'] += 1\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    self.metrics['failures'] += 1\n",
        "                    raise RuntimeError(f'Chain failed after {self.max_retries} attempts: {e}')\n",
        "                \n",
        "                prompt += f'\\n\\n[Error in previous attempt: {e}. Fix the output.]'\n",
        "                time.sleep(0.5)\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        return {\n",
        "            **self.metrics,\n",
        "            'success_rate': 1 - (self.metrics['failures'] / self.metrics['calls']) if self.metrics['calls'] > 0 else 0,\n",
        "            'avg_latency': self.metrics['total_latency'] / self.metrics['calls'] if self.metrics['calls'] > 0 else 0\n",
        "        }\n",
        "\n",
        "# Example\n",
        "class SentimentOutput(BaseModel):\n",
        "    sentiment: str = Field(description='positive, negative, or neutral')\n",
        "    confidence: float = Field(ge=0.0, le=1.0)\n",
        "\n",
        "def mock_llm(prompt):\n",
        "    return '{\"sentiment\": \"positive\", \"confidence\": 0.85}'\n",
        "\n",
        "def parser(response):\n",
        "    import json\n",
        "    data = json.loads(response)\n",
        "    return SentimentOutput(**data)\n",
        "\n",
        "chain = RobustChain(mock_llm, parser)\n",
        "result = chain.invoke('This product is great!')\n",
        "print(f'Result: {result}')\n",
        "print(f'Metrics: {chain.get_metrics()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Memory Management for Stateful Conversations\n",
        "\n",
        "Production conversational systems require efficient memory management to maintain context across turns while controlling costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import json\n",
        "\n",
        "class ConversationMemory:\n",
        "    '''Production conversation memory with token budgeting'''\n",
        "    \n",
        "    def __init__(self, max_tokens: int = 2000, summarization_threshold: int = 1500):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.summarization_threshold = summarization_threshold\n",
        "        self.messages = deque()\n",
        "        self.summary = None\n",
        "        self.token_count = 0\n",
        "    \n",
        "    def add_message(self, role: str, content: str):\n",
        "        '''Add message with automatic summarization'''\n",
        "        import tiktoken\n",
        "        enc = tiktoken.encoding_for_model('gpt-4')\n",
        "        \n",
        "        msg_tokens = len(enc.encode(content))\n",
        "        \n",
        "        # Check if we need to summarize\n",
        "        if self.token_count + msg_tokens > self.summarization_threshold:\n",
        "            self._summarize_old_messages()\n",
        "        \n",
        "        self.messages.append({'role': role, 'content': content, 'tokens': msg_tokens})\n",
        "        self.token_count += msg_tokens\n",
        "        \n",
        "        # Enforce hard limit\n",
        "        while self.token_count > self.max_tokens and len(self.messages) > 1:\n",
        "            removed = self.messages.popleft()\n",
        "            self.token_count -= removed['tokens']\n",
        "    \n",
        "    def _summarize_old_messages(self):\n",
        "        '''Summarize older messages to save tokens'''\n",
        "        # Keep last 3 messages, summarize the rest\n",
        "        if len(self.messages) <= 3:\n",
        "            return\n",
        "        \n",
        "        to_summarize = list(self.messages)[:-3]\n",
        "        conversation_text = '\\\\n'.join([\n",
        "            f\"{msg['role']}: {msg['content']}\" for msg in to_summarize\n",
        "        ])\n",
        "        \n",
        "        # Mock LLM call for summarization\n",
        "        summary = f\"Summary: Discussed {len(to_summarize)} topics including...\"\n",
        "        \n",
        "        # Clear old messages, keep summary\n",
        "        for _ in range(len(to_summarize)):\n",
        "            removed = self.messages.popleft()\n",
        "            self.token_count -= removed['tokens']\n",
        "        \n",
        "        self.summary = summary\n",
        "        self.token_count += 100  # Approximate summary tokens\n",
        "    \n",
        "    def get_context(self) -> List[dict]:\n",
        "        '''Get conversation context for LLM'''\n",
        "        context = []\n",
        "        \n",
        "        if self.summary:\n",
        "            context.append({'role': 'system', 'content': self.summary})\n",
        "        \n",
        "        context.extend(list(self.messages))\n",
        "        return context\n",
        "    \n",
        "    def get_metrics(self) -> dict:\n",
        "        '''Memory usage metrics'''\n",
        "        return {\n",
        "            'total_messages': len(self.messages),\n",
        "            'token_count': self.token_count,\n",
        "            'token_utilization': self.token_count / self.max_tokens,\n",
        "            'has_summary': self.summary is not None\n",
        "        }\n",
        "\n",
        "# Demo\n",
        "memory = ConversationMemory(max_tokens=500, summarization_threshold=300)\n",
        "\n",
        "conversation = [\n",
        "    ('user', 'Tell me about Python'),\n",
        "    ('assistant', 'Python is a high-level programming language known for its simplicity and readability.'),\n",
        "    ('user', 'What about its history?'),\n",
        "    ('assistant', 'Python was created by Guido van Rossum and first released in 1991.'),\n",
        "    ('user', 'What are its main uses?'),\n",
        "    ('assistant', 'Python is widely used in web development, data science, machine learning, automation, and more.'),\n",
        "]\n",
        "\n",
        "for role, content in conversation:\n",
        "    memory.add_message(role, content)\n",
        "    metrics = memory.get_metrics()\n",
        "    print(f\"Added {role} message | Tokens: {metrics['token_count']}/{memory.max_tokens} | Messages: {metrics['total_messages']}\")\n",
        "\n",
        "print(f\"\\nFinal context: {len(memory.get_context())} messages\")\n",
        "print(f\"Token utilization: {memory.get_metrics()['token_utilization']:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Questions: LangChain & Production Chains\n",
        "\n",
        "### For Experienced Professionals\n",
        "\n",
        "Production chain systems require robust error handling, efficient memory management, and comprehensive evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interview_questions_langchain = [\n",
        "    {\n",
        "        \"level\": \"Senior\",\n",
        "        \"question\": \"Your LangChain agent makes 5-10 tool calls per query, costing $0.15-$0.30. Business wants to reduce cost by 50% without sacrificing quality. Walk through your optimization approach.\",\n",
        "        \"answer\": \"\"\"\n",
        "**Current Cost Breakdown:**\n",
        "- Base query: $0.03 (input) + $0.06 (output) = $0.09\n",
        "- 5-10 tool calls: Each call adds $0.01-$0.03\n",
        "- Total: $0.15-$0.30 per query\n",
        "\n",
        "**Target: $0.075-$0.15 (50% reduction)**\n",
        "\n",
        "**Root Cause Analysis:**\n",
        "\n",
        "1. **Profile Tool Usage:**\n",
        "```python\n",
        "def profile_agent_execution(agent, queries: List[str]):\n",
        "    tool_usage = defaultdict(int)\n",
        "    redundant_calls = 0\n",
        "    \n",
        "    for query in queries:\n",
        "        trace = agent.run_with_trace(query)\n",
        "        \n",
        "        # Track which tools are called\n",
        "        for call in trace['tool_calls']:\n",
        "            tool_usage[call['tool']] += 1\n",
        "            \n",
        "            # Check for redundant calls (same tool, similar args)\n",
        "            if is_redundant(call, trace['tool_calls']):\n",
        "                redundant_calls += 1\n",
        "    \n",
        "    return {\n",
        "        'tool_usage': dict(tool_usage),\n",
        "        'avg_calls_per_query': sum(tool_usage.values()) / len(queries),\n",
        "        'redundant_call_rate': redundant_calls / sum(tool_usage.values())\n",
        "    }\n",
        "\n",
        "# Example output:\n",
        "# {\n",
        "#   'tool_usage': {'search': 120, 'calculator': 80, 'database': 50},\n",
        "#   'avg_calls_per_query': 7.5,\n",
        "#   'redundant_call_rate': 0.15  # 15% of calls are redundant\n",
        "# }\n",
        "```\n",
        "\n",
        "**Optimization Strategy:**\n",
        "\n",
        "**1. Caching Tool Results (Immediate, ~25% cost reduction):**\n",
        "```python\n",
        "import hashlib\n",
        "from functools import lru_cache\n",
        "\n",
        "class CachedToolExecutor:\n",
        "    def __init__(self):\n",
        "        self.cache = {}  # In production: Redis with TTL\n",
        "        self.cache_hits = 0\n",
        "        self.cache_misses = 0\n",
        "    \n",
        "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
        "        # Create cache key from tool + args\n",
        "        cache_key = self._get_cache_key(tool_name, args)\n",
        "        \n",
        "        if cache_key in self.cache:\n",
        "            self.cache_hits += 1\n",
        "            return self.cache[cache_key]\n",
        "        \n",
        "        # Execute tool\n",
        "        result = self._run_tool(tool_name, args)\n",
        "        \n",
        "        # Cache result\n",
        "        self.cache[cache_key] = result\n",
        "        self.cache_misses += 1\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def _get_cache_key(self, tool_name: str, args: dict) -> str:\n",
        "        # Deterministic key from tool + args\n",
        "        key_str = f\"{tool_name}:{json.dumps(args, sort_keys=True)}\"\n",
        "        return hashlib.md5(key_str.encode()).hexdigest()\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        total = self.cache_hits + self.cache_misses\n",
        "        return {\n",
        "            'cache_hit_rate': self.cache_hits / total if total > 0 else 0,\n",
        "            'cost_saved_pct': (self.cache_hits / total) * 100 if total > 0 else 0\n",
        "        }\n",
        "\n",
        "# Expected: 20-30% cache hit rate → 20-30% cost reduction on tool calls\n",
        "```\n",
        "\n",
        "**2. Plan-and-Execute Pattern (Medium-term, ~40% cost reduction):**\n",
        "```python\n",
        "class PlanAndExecuteAgent:\n",
        "    '''Generate plan first, execute efficiently'''\n",
        "    \n",
        "    def run(self, query: str) -> str:\n",
        "        # Step 1: Generate execution plan (1 LLM call, cheap with low temp)\n",
        "        plan = self.generate_plan(query)\n",
        "        # Example plan: \n",
        "        # 1. Search for \"Q4 revenue\"\n",
        "        # 2. Extract number from results\n",
        "        # 3. Calculate YoY growth\n",
        "        \n",
        "        # Step 2: Execute plan steps (fewer, more targeted tool calls)\n",
        "        results = {}\n",
        "        for step in plan['steps']:\n",
        "            tool = step['tool']\n",
        "            args = step['args']\n",
        "            results[step['id']] = self.execute_tool(tool, args)\n",
        "        \n",
        "        # Step 3: Synthesize final answer (1 LLM call)\n",
        "        answer = self.synthesize_answer(query, plan, results)\n",
        "        \n",
        "        return answer\n",
        "    \n",
        "    def generate_plan(self, query: str) -> dict:\n",
        "        '''Generate execution plan with low temperature'''\n",
        "        prompt = f'''\n",
        "        Query: {query}\n",
        "        \n",
        "        Generate a step-by-step plan to answer this query.\n",
        "        Available tools: search, calculator, database_query\n",
        "        \n",
        "        Return JSON:\n",
        "        {{\n",
        "            \"steps\": [\n",
        "                {{\"id\": 1, \"action\": \"description\", \"tool\": \"tool_name\", \"args\": {{...}} }}\n",
        "            ]\n",
        "        }}\n",
        "        '''\n",
        "        \n",
        "        # Use low temperature for deterministic planning\n",
        "        response = llm.generate(prompt, temperature=0.2)\n",
        "        return json.loads(response)\n",
        "    \n",
        "    def synthesize_answer(self, query: str, plan: dict, results: dict) -> str:\n",
        "        '''Combine results into final answer'''\n",
        "        prompt = f'''\n",
        "        Query: {query}\n",
        "        \n",
        "        Execution results:\n",
        "        {json.dumps(results, indent=2)}\n",
        "        \n",
        "        Synthesize a clear answer to the query.\n",
        "        '''\n",
        "        \n",
        "        return llm.generate(prompt, temperature=0.5)\n",
        "\n",
        "# Comparison:\n",
        "# ReAct agent (traditional):\n",
        "#   - Query → Tool call → Observation → Think → Tool call → ...\n",
        "#   - 5-10 LLM calls, 5-10 tool calls\n",
        "#   - Cost: $0.15-$0.30\n",
        "#\n",
        "# Plan-and-Execute:\n",
        "#   - Query → Plan (1 LLM call) → Execute tools (3-5 calls) → Synthesize (1 LLM call)\n",
        "#   - 2 LLM calls, 3-5 tool calls\n",
        "#   - Cost: $0.08-$0.12 (60% of original)\n",
        "```\n",
        "\n",
        "**3. Tool Fusion (Long-term, ~50% cost reduction):**\n",
        "```python\n",
        "class FusedToolExecutor:\n",
        "    '''Combine multiple tool calls into single batch operation'''\n",
        "    \n",
        "    def batch_execute(self, tool_calls: List[dict]) -> List[Any]:\n",
        "        '''Execute multiple tool calls in parallel'''\n",
        "        # Group by tool type\n",
        "        by_tool = defaultdict(list)\n",
        "        for call in tool_calls:\n",
        "            by_tool[call['tool']].append(call)\n",
        "        \n",
        "        # Execute in batches\n",
        "        results = {}\n",
        "        for tool_name, calls in by_tool.items():\n",
        "            if tool_name == 'search':\n",
        "                # Batch search: 1 API call for multiple queries\n",
        "                queries = [c['args']['query'] for c in calls]\n",
        "                batch_results = self.search_batch(queries)\n",
        "                for call, result in zip(calls, batch_results):\n",
        "                    results[call['id']] = result\n",
        "            \n",
        "            elif tool_name == 'database_query':\n",
        "                # Batch DB queries with UNION\n",
        "                batch_results = self.database_batch([c['args'] for c in calls])\n",
        "                for call, result in zip(calls, batch_results):\n",
        "                    results[call['id']] = result\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def search_batch(self, queries: List[str]) -> List[str]:\n",
        "        '''Single API call for multiple search queries'''\n",
        "        # Many search APIs support batch requests\n",
        "        return search_api.batch_search(queries)\n",
        "\n",
        "# Cost comparison:\n",
        "# Sequential: 5 search calls × $0.01 = $0.05\n",
        "# Batched: 1 batch call = $0.02 (60% savings)\n",
        "```\n",
        "\n",
        "**4. Model Routing (Immediate, ~30% cost reduction):**\n",
        "```python\n",
        "class SmartModelRouter:\n",
        "    '''Route to cheaper models when appropriate'''\n",
        "    \n",
        "    def route_query(self, query: str, task_type: str) -> str:\n",
        "        # Simple queries → cheap model (GPT-3.5, $0.002/1K tokens)\n",
        "        # Complex queries → expensive model (GPT-4, $0.03/1K tokens)\n",
        "        \n",
        "        complexity = self.estimate_complexity(query)\n",
        "        \n",
        "        if complexity == 'simple' or task_type in ['extraction', 'classification']:\n",
        "            model = 'gpt-3.5-turbo'  # 15x cheaper\n",
        "        else:\n",
        "            model = 'gpt-4'\n",
        "        \n",
        "        return llm.generate(query, model=model)\n",
        "    \n",
        "    def estimate_complexity(self, query: str) -> str:\n",
        "        '''Estimate query complexity'''\n",
        "        # Heuristics:\n",
        "        if len(query.split()) < 50:\n",
        "            return 'simple'\n",
        "        elif any(keyword in query.lower() for keyword in ['analyze', 'compare', 'complex']):\n",
        "            return 'complex'\n",
        "        else:\n",
        "            return 'medium'\n",
        "\n",
        "# Expected: 40-50% of queries can use cheaper model → 30% cost reduction\n",
        "```\n",
        "\n",
        "**5. Monitoring & Continuous Optimization:**\n",
        "```python\n",
        "class AgentCostMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(lambda: {\n",
        "            'calls': 0,\n",
        "            'total_cost': 0,\n",
        "            'avg_tool_calls': []\n",
        "        })\n",
        "    \n",
        "    def log_execution(self, query: str, trace: dict, cost: float):\n",
        "        day_key = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.metrics[day_key]['calls'] += 1\n",
        "        self.metrics[day_key]['total_cost'] += cost\n",
        "        self.metrics[day_key]['avg_tool_calls'].append(len(trace['tool_calls']))\n",
        "    \n",
        "    def analyze_trends(self):\n",
        "        '''Identify cost optimization opportunities'''\n",
        "        # Find queries with excessive tool calls\n",
        "        expensive_patterns = []\n",
        "        \n",
        "        for query, trace in self.query_traces.items():\n",
        "            if len(trace['tool_calls']) > 8:\n",
        "                expensive_patterns.append({\n",
        "                    'query': query,\n",
        "                    'tool_calls': len(trace['tool_calls']),\n",
        "                    'cost': trace['cost']\n",
        "                })\n",
        "        \n",
        "        return expensive_patterns\n",
        "\n",
        "# Weekly review:\n",
        "# - Identify queries with >10 tool calls\n",
        "# - Check for redundant tool usage\n",
        "# - Optimize high-frequency expensive patterns\n",
        "```\n",
        "\n",
        "**Complete Optimization Results:**\n",
        "\n",
        "| Technique | Cost Reduction | Implementation Time | Complexity |\n",
        "|-----------|----------------|---------------------|------------|\n",
        "| Tool caching | 20-25% | 1-2 days | Low |\n",
        "| Model routing | 30% | 1 week | Medium |\n",
        "| Plan-and-Execute | 40% | 2-3 weeks | High |\n",
        "| Tool fusion | 50% | 3-4 weeks | High |\n",
        "\n",
        "**Recommended Approach:**\n",
        "1. Week 1: Implement caching + model routing (50% reduction, minimal risk)\n",
        "2. Week 2-3: A/B test Plan-and-Execute on 20% traffic\n",
        "3. Week 4: Gradual rollout if quality maintained\n",
        "4. Result: $0.15 → $0.08 per query (47% reduction)\n",
        "\n",
        "**Key Metrics to Monitor:**\n",
        "- Cost per query (target: $0.08)\n",
        "- Quality (BLEU score, user satisfaction)\n",
        "- Latency (ensure caching doesn't add lag)\n",
        "- Tool call distribution (watch for regressions)\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"level\": \"Senior\",\n",
        "        \"question\": \"Your conversational assistant stores conversation history in memory. After 20 turns, context windows are full and the system starts dropping early messages, causing 'memory loss.' Design a production solution.\",\n",
        "        \"answer\": \"\"\"\n",
        "**Problem Analysis:**\n",
        "\n",
        "**Current Situation:**\n",
        "- 20-turn conversation: ~4000-6000 tokens\n",
        "- Context limit: 8K tokens (leaving 2-4K for system prompt + output)\n",
        "- System dropping first 10 turns → loses important context\n",
        "- User complaints: \"You forgot what we discussed earlier\"\n",
        "\n",
        "**Requirements:**\n",
        "- Maintain conversation coherence across 50+ turns\n",
        "- Keep memory cost reasonable (<500 tokens overhead)\n",
        "- Preserve important context even from early turns\n",
        "- Handle multiple concurrent users\n",
        "\n",
        "**Production Solution: Hierarchical Memory System**\n",
        "\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class MemoryEntry:\n",
        "    turn: int\n",
        "    role: str\n",
        "    content: str\n",
        "    tokens: int\n",
        "    importance: float  # 0.0 to 1.0\n",
        "    timestamp: str\n",
        "    summary: Optional[str] = None\n",
        "\n",
        "class HierarchicalMemory:\n",
        "    '''Production conversation memory with intelligent compression'''\n",
        "    \n",
        "    def __init__(self, max_tokens: int = 2000):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.short_term = []  # Recent messages (full text)\n",
        "        self.long_term = []   # Older messages (summarized)\n",
        "        self.facts = {}       # Extracted facts/entities\n",
        "        self.current_tokens = 0\n",
        "    \n",
        "    def add_message(self, role: str, content: str, turn: int) -> None:\n",
        "        '''Add message with automatic compression'''\n",
        "        tokens = self.count_tokens(content)\n",
        "        importance = self.calculate_importance(content, role)\n",
        "        \n",
        "        entry = MemoryEntry(\n",
        "            turn=turn,\n",
        "            role=role,\n",
        "            content=content,\n",
        "            tokens=tokens,\n",
        "            importance=importance,\n",
        "            timestamp=datetime.utcnow().isoformat()\n",
        "        )\n",
        "        \n",
        "        # Add to short-term memory\n",
        "        self.short_term.append(entry)\n",
        "        self.current_tokens += tokens\n",
        "        \n",
        "        # Extract facts for long-term storage\n",
        "        if importance > 0.7:  # Important information\n",
        "            self.extract_facts(content, turn)\n",
        "        \n",
        "        # Compress if needed\n",
        "        if self.current_tokens > self.max_tokens * 0.8:  # 80% threshold\n",
        "            self.compress_memory()\n",
        "    \n",
        "    def calculate_importance(self, content: str, role: str) -> float:\n",
        "        '''Estimate message importance'''\n",
        "        score = 0.5  # Base score\n",
        "        \n",
        "        # User messages are generally more important\n",
        "        if role == 'user':\n",
        "            score += 0.2\n",
        "        \n",
        "        # Check for important keywords\n",
        "        important_keywords = ['important', 'remember', 'note', 'key', 'critical']\n",
        "        if any(kw in content.lower() for kw in important_keywords):\n",
        "            score += 0.2\n",
        "        \n",
        "        # Questions are important\n",
        "        if '?' in content:\n",
        "            score += 0.1\n",
        "        \n",
        "        # Long messages might contain important details\n",
        "        if len(content.split()) > 50:\n",
        "            score += 0.1\n",
        "        \n",
        "        return min(1.0, score)\n",
        "    \n",
        "    def compress_memory(self) -> None:\n",
        "        '''Move old messages to long-term memory with summarization'''\n",
        "        # Keep last 5 messages in short-term (most recent context)\n",
        "        if len(self.short_term) <= 5:\n",
        "            return\n",
        "        \n",
        "        # Move older messages to long-term\n",
        "        to_compress = self.short_term[:-5]\n",
        "        self.short_term = self.short_term[-5:]\n",
        "        \n",
        "        # Summarize the compressed messages\n",
        "        summary = self.summarize_messages(to_compress)\n",
        "        \n",
        "        # Store summary in long-term memory\n",
        "        self.long_term.append({\n",
        "            'turns': [m.turn for m in to_compress],\n",
        "            'summary': summary,\n",
        "            'tokens': self.count_tokens(summary),\n",
        "            'important_facts': self.get_facts_for_turns([m.turn for m in to_compress])\n",
        "        })\n",
        "        \n",
        "        # Recalculate token count\n",
        "        self.current_tokens = sum(m.tokens for m in self.short_term)\n",
        "        self.current_tokens += sum(lt['tokens'] for lt in self.long_term)\n",
        "    \n",
        "    def summarize_messages(self, messages: List[MemoryEntry]) -> str:\n",
        "        '''Summarize a group of messages'''\n",
        "        # Build conversation text\n",
        "        conv_text = '\\\\n'.join([\n",
        "            f\"{m.role}: {m.content}\" for m in messages\n",
        "        ])\n",
        "        \n",
        "        # Use LLM to summarize (with specific prompt)\n",
        "        prompt = f'''\n",
        "        Summarize this conversation segment, preserving key facts and context:\n",
        "        \n",
        "        {conv_text}\n",
        "        \n",
        "        Format as: \"In turns {messages[0].turn}-{messages[-1].turn}, the user discussed...\"\n",
        "        Keep it concise (< 100 words) but preserve important details.\n",
        "        '''\n",
        "        \n",
        "        summary = llm.generate(prompt, temperature=0.3, max_tokens=150)\n",
        "        return summary\n",
        "    \n",
        "    def extract_facts(self, content: str, turn: int) -> None:\n",
        "        '''Extract key facts for long-term storage'''\n",
        "        # Use LLM to extract structured facts\n",
        "        prompt = f'''\n",
        "        Extract key facts from this message:\n",
        "        \"{content}\"\n",
        "        \n",
        "        Return JSON: {{\"facts\": [{{\"type\": \"preference/name/date/etc\", \"value\": \"...\"}}]}}\n",
        "        '''\n",
        "        \n",
        "        try:\n",
        "            response = llm.generate(prompt, temperature=0.0)\n",
        "            facts = json.loads(response)['facts']\n",
        "            \n",
        "            for fact in facts:\n",
        "                fact_key = f\"{fact['type']}:{fact['value']}\"\n",
        "                self.facts[fact_key] = {\n",
        "                    'value': fact['value'],\n",
        "                    'type': fact['type'],\n",
        "                    'turn': turn,\n",
        "                    'last_mentioned': turn\n",
        "                }\n",
        "        except:\n",
        "            pass  # Graceful degradation if extraction fails\n",
        "    \n",
        "    def get_context_for_llm(self) -> str:\n",
        "        '''Build optimized context for LLM'''\n",
        "        context_parts = []\n",
        "        \n",
        "        # 1. Persistent facts (always included)\n",
        "        if self.facts:\n",
        "            facts_text = self._format_facts()\n",
        "            context_parts.append(f\"[Known Facts]\\\\n{facts_text}\")\n",
        "        \n",
        "        # 2. Long-term memory summaries\n",
        "        for lt_entry in self.long_term:\n",
        "            context_parts.append(f\"[History] {lt_entry['summary']}\")\n",
        "        \n",
        "        # 3. Recent conversation (full text)\n",
        "        for msg in self.short_term:\n",
        "            context_parts.append(f\"{msg.role}: {msg.content}\")\n",
        "        \n",
        "        return '\\\\n\\\\n'.join(context_parts)\n",
        "    \n",
        "    def _format_facts(self) -> str:\n",
        "        '''Format extracted facts for context'''\n",
        "        fact_lines = []\n",
        "        for fact_key, fact_data in self.facts.items():\n",
        "            fact_lines.append(f\"- {fact_data['type'].title()}: {fact_data['value']}\")\n",
        "        return '\\\\n'.join(fact_lines[:10])  # Limit to top 10 facts\n",
        "    \n",
        "    def get_metrics(self) -> dict:\n",
        "        '''Memory system metrics'''\n",
        "        return {\n",
        "            'short_term_messages': len(self.short_term),\n",
        "            'long_term_segments': len(self.long_term),\n",
        "            'total_facts': len(self.facts),\n",
        "            'current_tokens': self.current_tokens,\n",
        "            'token_utilization': self.current_tokens / self.max_tokens,\n",
        "            'compression_ratio': self._calculate_compression_ratio()\n",
        "        }\n",
        "    \n",
        "    def _calculate_compression_ratio(self) -> float:\n",
        "        '''Calculate how much we've compressed'''\n",
        "        if not self.long_term:\n",
        "            return 1.0\n",
        "        \n",
        "        # Original tokens vs compressed tokens\n",
        "        original_tokens = sum(\n",
        "            sum(self.count_tokens(m.content) for m in segment['messages'])\n",
        "            for segment in self.long_term\n",
        "        )\n",
        "        compressed_tokens = sum(lt['tokens'] for lt in self.long_term)\n",
        "        \n",
        "        return compressed_tokens / original_tokens if original_tokens > 0 else 1.0\n",
        "    \n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        '''Count tokens in text'''\n",
        "        import tiktoken\n",
        "        enc = tiktoken.encoding_for_model('gpt-4')\n",
        "        return len(enc.encode(text))\n",
        "\n",
        "\n",
        "# Alternative: Vector Memory (for very long conversations)\n",
        "class VectorMemory:\n",
        "    '''Store conversation in vector database for semantic retrieval'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vector_store = ChromaDB()\n",
        "        self.message_count = 0\n",
        "    \n",
        "    def add_message(self, role: str, content: str, turn: int) -> None:\n",
        "        '''Store message as vector'''\n",
        "        # Embed message\n",
        "        embedding = embed_model.encode([content])[0]\n",
        "        \n",
        "        # Store in vector DB\n",
        "        self.vector_store.add(\n",
        "            id=f\"turn_{turn}\",\n",
        "            embedding=embedding,\n",
        "            metadata={'role': role, 'turn': turn, 'content': content}\n",
        "        )\n",
        "        \n",
        "        self.message_count += 1\n",
        "    \n",
        "    def get_relevant_context(self, current_message: str, k: int = 5) -> List[dict]:\n",
        "        '''Retrieve relevant past messages semantically'''\n",
        "        # Embed current message\n",
        "        query_embedding = embed_model.encode([current_message])[0]\n",
        "        \n",
        "        # Retrieve similar past messages\n",
        "        results = self.vector_store.query(\n",
        "            query_embedding=query_embedding,\n",
        "            n_results=k\n",
        "        )\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def get_context_for_llm(self, current_message: str) -> str:\n",
        "        '''Build context from relevant history'''\n",
        "        # Always include last 3 messages\n",
        "        recent = self.vector_store.get_latest(n=3)\n",
        "        \n",
        "        # Add semantically relevant older messages\n",
        "        relevant = self.get_relevant_context(current_message, k=3)\n",
        "        \n",
        "        # Combine\n",
        "        context_messages = recent + relevant\n",
        "        \n",
        "        # Format\n",
        "        return '\\\\n'.join([\n",
        "            f\"[Turn {m['turn']}] {m['role']}: {m['content']}\"\n",
        "            for m in sorted(context_messages, key=lambda x: x['turn'])\n",
        "        ])\n",
        "```\n",
        "\n",
        "**Comparison of Approaches:**\n",
        "\n",
        "| Approach | Max Turns | Token Overhead | Retrieval Speed | Complexity |\n",
        "|----------|-----------|----------------|-----------------|------------|\n",
        "| Naive (drop old) | 20 | 0 | N/A | Low |\n",
        "| Full summarization | 30-40 | 200-300 | Fast | Medium |\n",
        "| Hierarchical | 50-100 | 400-500 | Fast | High |\n",
        "| Vector memory | Unlimited | 100 | 50-100ms | High |\n",
        "\n",
        "**Production Deployment:**\n",
        "\n",
        "```python\n",
        "class ProductionConversationSystem:\n",
        "    def __init__(self, user_id: str):\n",
        "        self.memory = HierarchicalMemory(max_tokens=2000)\n",
        "        self.user_id = user_id\n",
        "        self.turn_count = 0\n",
        "    \n",
        "    def chat(self, user_message: str) -> str:\n",
        "        self.turn_count += 1\n",
        "        \n",
        "        # Add user message to memory\n",
        "        self.memory.add_message('user', user_message, self.turn_count)\n",
        "        \n",
        "        # Get optimized context\n",
        "        context = self.memory.get_context_for_llm()\n",
        "        \n",
        "        # Build prompt\n",
        "        prompt = f'''\n",
        "        {context}\n",
        "        \n",
        "        user: {user_message}\n",
        "        \n",
        "        assistant:'''\n",
        "        \n",
        "        # Generate response\n",
        "        response = llm.generate(prompt)\n",
        "        \n",
        "        # Add response to memory\n",
        "        self.memory.add_message('assistant', response, self.turn_count)\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def persist_memory(self):\n",
        "        '''Save memory to database for user'''\n",
        "        memory_state = {\n",
        "            'user_id': self.user_id,\n",
        "            'turn_count': self.turn_count,\n",
        "            'short_term': [asdict(m) for m in self.memory.short_term],\n",
        "            'long_term': self.memory.long_term,\n",
        "            'facts': self.memory.facts\n",
        "        }\n",
        "        \n",
        "        db.save_conversation_state(self.user_id, memory_state)\n",
        "    \n",
        "    def restore_memory(self):\n",
        "        '''Restore memory from database'''\n",
        "        state = db.load_conversation_state(self.user_id)\n",
        "        if state:\n",
        "            self.turn_count = state['turn_count']\n",
        "            # Restore memory structures\n",
        "            # ...\n",
        "```\n",
        "\n",
        "**Key Benefits:**\n",
        "- Handles 50-100 turn conversations (vs 20 before)\n",
        "- Maintains important context from early turns via facts + summaries\n",
        "- Token overhead: 400-500 tokens (vs 4000+ for full history)\n",
        "- Compression ratio: 5-10x (500 tokens stores equivalent of 5000)\n",
        "- Cost savings: 60-70% on longer conversations\n",
        "\n",
        "**Metrics to Track:**\n",
        "- Average turns before context loss\n",
        "- User satisfaction with memory quality\n",
        "- Token usage per conversation\n",
        "- Fact extraction accuracy\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"level\": \"Staff\",\n",
        "        \"question\": \"Design a production agent system that can use 20+ tools reliably. Include tool selection optimization, error recovery, execution monitoring, and explain how to prevent tool hallucination (calling non-existent tools or wrong parameters).\",\n",
        "        \"answer\": \"\"\"\n",
        "**Production Multi-Tool Agent Architecture:**\n",
        "\n",
        "**1. Tool Registry & Validation:**\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "import inspect\n",
        "\n",
        "class ToolParameter(BaseModel):\n",
        "    name: str\n",
        "    type: str  # \"string\", \"number\", \"boolean\", \"array\", \"object\"\n",
        "    description: str\n",
        "    required: bool = True\n",
        "    enum: Optional[List[Any]] = None\n",
        "    \n",
        "class ToolDefinition(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    parameters: List[ToolParameter]\n",
        "    returns: str\n",
        "    examples: List[dict]\n",
        "    cost_estimate: float  # Relative cost (1.0 = baseline)\n",
        "    avg_latency_ms: float\n",
        "    error_rate: float\n",
        "    \n",
        "class ToolRegistry:\n",
        "    '''Central registry for all available tools'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.tools = {}\n",
        "        self.tool_usage_stats = defaultdict(lambda: {\n",
        "            'calls': 0,\n",
        "            'successes': 0,\n",
        "            'failures': 0,\n",
        "            'avg_latency': []\n",
        "        })\n",
        "    \n",
        "    def register_tool(self, func: Callable, definition: ToolDefinition):\n",
        "        '''Register a tool with validation'''\n",
        "        # Validate function signature matches definition\n",
        "        sig = inspect.signature(func)\n",
        "        param_names = set(sig.parameters.keys())\n",
        "        def_param_names = set(p.name for p in definition.parameters)\n",
        "        \n",
        "        if param_names != def_param_names:\n",
        "            raise ValueError(f\"Function signature doesn't match definition for {definition.name}\")\n",
        "        \n",
        "        self.tools[definition.name] = {\n",
        "            'function': func,\n",
        "            'definition': definition,\n",
        "            'enabled': True\n",
        "        }\n",
        "    \n",
        "    def get_tool_definitions_for_llm(self, limit: int = None) -> List[dict]:\n",
        "        '''Get tool definitions in format for LLM'''\n",
        "        # Optionally limit tools based on context\n",
        "        tools_list = list(self.tools.values())\n",
        "        \n",
        "        if limit:\n",
        "            # Prioritize by success rate and frequency\n",
        "            tools_list.sort(key=lambda t: (\n",
        "                self.tool_usage_stats[t['definition'].name]['successes'],\n",
        "                -t['definition'].error_rate\n",
        "            ), reverse=True)\n",
        "            tools_list = tools_list[:limit]\n",
        "        \n",
        "        return [\n",
        "            {\n",
        "                'name': tool['definition'].name,\n",
        "                'description': tool['definition'].description,\n",
        "                'parameters': {\n",
        "                    'type': 'object',\n",
        "                    'properties': {\n",
        "                        p.name: {\n",
        "                            'type': p.type,\n",
        "                            'description': p.description,\n",
        "                            'enum': p.enum\n",
        "                        }\n",
        "                        for p in tool['definition'].parameters\n",
        "                    },\n",
        "                    'required': [p.name for p in tool['definition'].parameters if p.required]\n",
        "                },\n",
        "                'examples': tool['definition'].examples[:2]  # Include examples!\n",
        "            }\n",
        "            for tool in tools_list\n",
        "            if tool['enabled']\n",
        "        ]\n",
        "    \n",
        "    def validate_tool_call(self, tool_name: str, args: dict) -> tuple[bool, Optional[str]]:\n",
        "        '''Validate tool call before execution'''\n",
        "        if tool_name not in self.tools:\n",
        "            return False, f\"Tool '{tool_name}' not found. Available: {list(self.tools.keys())}\"\n",
        "        \n",
        "        tool_def = self.tools[tool_name]['definition']\n",
        "        \n",
        "        # Check required parameters\n",
        "        required_params = [p.name for p in tool_def.parameters if p.required]\n",
        "        missing = set(required_params) - set(args.keys())\n",
        "        if missing:\n",
        "            return False, f\"Missing required parameters: {missing}\"\n",
        "        \n",
        "        # Validate parameter types\n",
        "        for param in tool_def.parameters:\n",
        "            if param.name in args:\n",
        "                value = args[param.name]\n",
        "                expected_type = param.type\n",
        "                \n",
        "                # Type validation\n",
        "                if expected_type == 'string' and not isinstance(value, str):\n",
        "                    return False, f\"Parameter '{param.name}' must be a string\"\n",
        "                elif expected_type == 'number' and not isinstance(value, (int, float)):\n",
        "                    return False, f\"Parameter '{param.name}' must be a number\"\n",
        "                # ... more type checks\n",
        "                \n",
        "                # Enum validation\n",
        "                if param.enum and value not in param.enum:\n",
        "                    return False, f\"Parameter '{param.name}' must be one of {param.enum}\"\n",
        "        \n",
        "        return True, None\n",
        "\n",
        "\n",
        "# Example tool definitions\n",
        "search_tool = ToolDefinition(\n",
        "    name=\"web_search\",\n",
        "    description=\"Search the internet for information. Use for current events, facts, and recent data.\",\n",
        "    parameters=[\n",
        "        ToolParameter(name=\"query\", type=\"string\", description=\"Search query\", required=True),\n",
        "        ToolParameter(name=\"num_results\", type=\"number\", description=\"Number of results (1-10)\", required=False)\n",
        "    ],\n",
        "    returns=\"List of search results with title, snippet, and URL\",\n",
        "    examples=[\n",
        "        {\"query\": \"latest news on AI\", \"num_results\": 5},\n",
        "        {\"query\": \"weather in San Francisco\"}\n",
        "    ],\n",
        "    cost_estimate=1.0,\n",
        "    avg_latency_ms=200,\n",
        "    error_rate=0.02\n",
        ")\n",
        "\n",
        "calc_tool = ToolDefinition(\n",
        "    name=\"calculator\",\n",
        "    description=\"Perform mathematical calculations. Supports +, -, *, /, **, sqrt, etc.\",\n",
        "    parameters=[\n",
        "        ToolParameter(name=\"expression\", type=\"string\", description=\"Mathematical expression to evaluate\", required=True)\n",
        "    ],\n",
        "    returns=\"Numerical result of the calculation\",\n",
        "    examples=[\n",
        "        {\"expression\": \"2 + 2\"},\n",
        "        {\"expression\": \"sqrt(144) * 3\"}\n",
        "    ],\n",
        "    cost_estimate=0.1,\n",
        "    avg_latency_ms=5,\n",
        "    error_rate=0.01\n",
        ")\n",
        "\n",
        "registry = ToolRegistry()\n",
        "registry.register_tool(web_search_function, search_tool)\n",
        "registry.register_tool(calculator_function, calc_tool)\n",
        "```\n",
        "\n",
        "**2. Intelligent Tool Selection (Prevent Hallucination):**\n",
        "\n",
        "```python\n",
        "class ToolSelectionOptimizer:\n",
        "    '''Optimize tool selection to prevent hallucination'''\n",
        "    \n",
        "    def __init__(self, registry: ToolRegistry):\n",
        "        self.registry = registry\n",
        "        self.selection_history = []\n",
        "    \n",
        "    def select_tools_for_query(self, query: str, max_tools: int = 5) -> List[dict]:\n",
        "        '''Select most relevant tools for query'''\n",
        "        # Strategy 1: Keyword matching\n",
        "        query_lower = query.lower()\n",
        "        keyword_scores = {}\n",
        "        \n",
        "        for tool_name, tool in self.registry.tools.items():\n",
        "            score = 0\n",
        "            desc = tool['definition'].description.lower()\n",
        "            \n",
        "            # Simple keyword overlap\n",
        "            query_words = set(query_lower.split())\n",
        "            desc_words = set(desc.split())\n",
        "            overlap = len(query_words & desc_words)\n",
        "            score += overlap * 0.5\n",
        "            \n",
        "            # Boost frequently successful tools\n",
        "            stats = self.registry.tool_usage_stats[tool_name]\n",
        "            if stats['calls'] > 0:\n",
        "                success_rate = stats['successes'] / stats['calls']\n",
        "                score += success_rate * 0.3\n",
        "            \n",
        "            keyword_scores[tool_name] = score\n",
        "        \n",
        "        # Select top-k tools\n",
        "        top_tools = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:max_tools]\n",
        "        tool_names = [name for name, score in top_tools if score > 0]\n",
        "        \n",
        "        # Get definitions for selected tools\n",
        "        selected_defs = [\n",
        "            self.registry.tools[name]['definition']\n",
        "            for name in tool_names\n",
        "        ]\n",
        "        \n",
        "        return self.registry.get_tool_definitions_for_llm(limit=max_tools)\n",
        "    \n",
        "    def build_tool_prompt(self, query: str, relevant_tools: List[dict]) -> str:\n",
        "        '''Build prompt with only relevant tools'''\n",
        "        prompt = f'''\n",
        "You have access to these tools:\n",
        "\n",
        "{json.dumps(relevant_tools, indent=2)}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. You can ONLY use tools listed above\n",
        "2. You MUST use exact tool names (case-sensitive)\n",
        "3. You MUST provide all required parameters\n",
        "4. Check the examples for correct usage\n",
        "5. If you need a tool that's not listed, respond: \"I don't have access to that tool\"\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "    \"thought\": \"What I need to do...\",\n",
        "    \"tool_calls\": [\n",
        "        {{\"tool\": \"exact_tool_name\", \"args\": {{...}} }}\n",
        "    ]\n",
        "}}\n",
        "'''\n",
        "        return prompt\n",
        "\n",
        "\n",
        "# Example: Dynamic tool selection\n",
        "optimizer = ToolSelectionOptimizer(registry)\n",
        "\n",
        "queries = [\n",
        "    \"What's 15% of 250?\",  # Should select calculator\n",
        "    \"Latest news on SpaceX\",  # Should select web_search\n",
        "    \"Search for Python tutorials and calculate 2+2\"  # Should select both\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    tools = optimizer.select_tools_for_query(query, max_tools=3)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Selected tools: {[t['name'] for t in tools]}\")\n",
        "    print()\n",
        "```\n",
        "\n",
        "**3. Robust Execution with Error Recovery:**\n",
        "\n",
        "```python\n",
        "class RobustToolExecutor:\n",
        "    '''Execute tools with validation, retry, and error recovery'''\n",
        "    \n",
        "    def __init__(self, registry: ToolRegistry, max_retries: int = 2):\n",
        "        self.registry = registry\n",
        "        self.max_retries = max_retries\n",
        "        self.execution_log = []\n",
        "    \n",
        "    def execute_plan(self, tool_calls: List[dict]) -> List[dict]:\n",
        "        '''Execute multiple tool calls with dependency handling'''\n",
        "        results = {}\n",
        "        failed_calls = []\n",
        "        \n",
        "        for i, call in enumerate(tool_calls):\n",
        "            tool_name = call['tool']\n",
        "            args = call['args']\n",
        "            \n",
        "            # Validate before execution\n",
        "            valid, error = self.registry.validate_tool_call(tool_name, args)\n",
        "            if not valid:\n",
        "                failed_calls.append({\n",
        "                    'tool': tool_name,\n",
        "                    'args': args,\n",
        "                    'error': error,\n",
        "                    'error_type': 'validation'\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # Execute with retry\n",
        "            result = self.execute_with_retry(tool_name, args, call_id=i)\n",
        "            \n",
        "            if result['status'] == 'success':\n",
        "                results[f\"call_{i}\"] = result['output']\n",
        "            else:\n",
        "                failed_calls.append({\n",
        "                    'tool': tool_name,\n",
        "                    'args': args,\n",
        "                    'error': result['error'],\n",
        "                    'error_type': 'execution'\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'results': results,\n",
        "            'failed_calls': failed_calls,\n",
        "            'success_rate': len(results) / len(tool_calls) if tool_calls else 0\n",
        "        }\n",
        "    \n",
        "    def execute_with_retry(self, tool_name: str, args: dict, call_id: int) -> dict:\n",
        "        '''Execute single tool with automatic retry'''\n",
        "        import time\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                # Get tool function\n",
        "                tool = self.registry.tools[tool_name]['function']\n",
        "                \n",
        "                # Execute with timeout\n",
        "                start = time.time()\n",
        "                output = self._execute_with_timeout(tool, args, timeout_seconds=30)\n",
        "                latency = (time.time() - start) * 1000\n",
        "                \n",
        "                # Log success\n",
        "                self._log_execution(tool_name, args, 'success', latency, None)\n",
        "                \n",
        "                # Update stats\n",
        "                self.registry.tool_usage_stats[tool_name]['calls'] += 1\n",
        "                self.registry.tool_usage_stats[tool_name]['successes'] += 1\n",
        "                self.registry.tool_usage_stats[tool_name]['avg_latency'].append(latency)\n",
        "                \n",
        "                return {\n",
        "                    'status': 'success',\n",
        "                    'output': output,\n",
        "                    'latency_ms': latency,\n",
        "                    'attempt': attempt + 1\n",
        "                }\n",
        "                \n",
        "            except TimeoutError as e:\n",
        "                error_msg = f\"Tool execution timed out after 30s\"\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    self._log_execution(tool_name, args, 'timeout', 0, error_msg)\n",
        "                    return {'status': 'error', 'error': error_msg, 'error_type': 'timeout'}\n",
        "                time.sleep(1)  # Brief delay before retry\n",
        "                \n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    self._log_execution(tool_name, args, 'error', 0, error_msg)\n",
        "                    return {'status': 'error', 'error': error_msg, 'error_type': 'exception'}\n",
        "                time.sleep(0.5)\n",
        "        \n",
        "        return {'status': 'error', 'error': 'Max retries exceeded'}\n",
        "    \n",
        "    def _execute_with_timeout(self, func: Callable, args: dict, timeout_seconds: int) -> Any:\n",
        "        '''Execute function with timeout'''\n",
        "        import signal\n",
        "        \n",
        "        def timeout_handler(signum, frame):\n",
        "            raise TimeoutError()\n",
        "        \n",
        "        # Set timeout\n",
        "        signal.signal(signal.SIGALRM, timeout_handler)\n",
        "        signal.alarm(timeout_seconds)\n",
        "        \n",
        "        try:\n",
        "            result = func(**args)\n",
        "        finally:\n",
        "            signal.alarm(0)  # Cancel timeout\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def _log_execution(self, tool_name: str, args: dict, status: str, latency: float, error: Optional[str]):\n",
        "        '''Log tool execution for monitoring'''\n",
        "        self.execution_log.append({\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "            'tool': tool_name,\n",
        "            'args': args,\n",
        "            'status': status,\n",
        "            'latency_ms': latency,\n",
        "            'error': error\n",
        "        })\n",
        "```\n",
        "\n",
        "**4. Agent with Tool Hallucination Prevention:**\n",
        "\n",
        "```python\n",
        "class ProductionToolAgent:\n",
        "    '''Production agent with robust tool usage'''\n",
        "    \n",
        "    def __init__(self, registry: ToolRegistry):\n",
        "        self.registry = registry\n",
        "        self.executor = RobustToolExecutor(registry)\n",
        "        self.optimizer = ToolSelectionOptimizer(registry)\n",
        "        self.failures = []\n",
        "    \n",
        "    def run(self, query: str) -> dict:\n",
        "        '''Execute query with tool usage'''\n",
        "        # Step 1: Select relevant tools\n",
        "        relevant_tools = self.optimizer.select_tools_for_query(query, max_tools=5)\n",
        "        \n",
        "        # Step 2: Generate plan\n",
        "        plan_prompt = self.optimizer.build_tool_prompt(query, relevant_tools)\n",
        "        plan_response = llm.generate(plan_prompt, temperature=0.2)\n",
        "        \n",
        "        # Step 3: Parse and validate plan\n",
        "        try:\n",
        "            plan = json.loads(plan_response)\n",
        "            tool_calls = plan.get('tool_calls', [])\n",
        "        except json.JSONDecodeError:\n",
        "            return {\n",
        "                'status': 'error',\n",
        "                'error': 'Failed to parse LLM response as JSON',\n",
        "                'raw_response': plan_response\n",
        "            }\n",
        "        \n",
        "        # Step 4: Validate tool calls\n",
        "        invalid_calls = []\n",
        "        valid_calls = []\n",
        "        \n",
        "        for call in tool_calls:\n",
        "            tool_name = call.get('tool')\n",
        "            \n",
        "            # Check if tool exists\n",
        "            if tool_name not in self.registry.tools:\n",
        "                invalid_calls.append({\n",
        "                    'tool': tool_name,\n",
        "                    'error': f\"Tool '{tool_name}' does not exist\",\n",
        "                    'available_tools': list(self.registry.tools.keys())\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # Validate parameters\n",
        "            args = call.get('args', {})\n",
        "            valid, error = self.registry.validate_tool_call(tool_name, args)\n",
        "            \n",
        "            if valid:\n",
        "                valid_calls.append(call)\n",
        "            else:\n",
        "                invalid_calls.append({\n",
        "                    'tool': tool_name,\n",
        "                    'args': args,\n",
        "                    'error': error\n",
        "                })\n",
        "        \n",
        "        # Step 5: Handle invalid calls with retry\n",
        "        if invalid_calls and not valid_calls:\n",
        "            # All calls invalid - ask LLM to fix\n",
        "            fix_prompt = f'''\n",
        "Your previous tool calls were invalid:\n",
        "{json.dumps(invalid_calls, indent=2)}\n",
        "\n",
        "Available tools:\n",
        "{json.dumps(relevant_tools, indent=2)}\n",
        "\n",
        "Please provide corrected tool calls for the query: \"{query}\"\n",
        "'''\n",
        "            corrected_response = llm.generate(fix_prompt, temperature=0.1)\n",
        "            # Parse and validate again (with recursion limit)\n",
        "            # ...\n",
        "        \n",
        "        # Step 6: Execute valid calls\n",
        "        execution_result = self.executor.execute_plan(valid_calls)\n",
        "        \n",
        "        # Step 7: Synthesize final answer\n",
        "        answer = self.synthesize_answer(query, execution_result, plan.get('thought'))\n",
        "        \n",
        "        return {\n",
        "            'status': 'success' if not execution_result['failed_calls'] else 'partial',\n",
        "            'answer': answer,\n",
        "            'tool_calls': len(valid_calls),\n",
        "            'invalid_calls': len(invalid_calls),\n",
        "            'failed_executions': len(execution_result['failed_calls']),\n",
        "            'execution_details': execution_result\n",
        "        }\n",
        "    \n",
        "    def synthesize_answer(self, query: str, execution_result: dict, thought: str) -> str:\n",
        "        '''Synthesize final answer from tool results'''\n",
        "        prompt = f'''\n",
        "Query: {query}\n",
        "\n",
        "Plan: {thought}\n",
        "\n",
        "Tool Results:\n",
        "{json.dumps(execution_result['results'], indent=2)}\n",
        "\n",
        "Synthesize a clear answer to the query based on these results.\n",
        "'''\n",
        "        \n",
        "        return llm.generate(prompt, temperature=0.5)\n",
        "```\n",
        "\n",
        "**5. Monitoring & Alert System:**\n",
        "\n",
        "```python\n",
        "class ToolMonitor:\n",
        "    '''Monitor tool usage and detect issues'''\n",
        "    \n",
        "    def __init__(self, registry: ToolRegistry):\n",
        "        self.registry = registry\n",
        "        self.alerts = []\n",
        "    \n",
        "    def check_health(self) -> List[dict]:\n",
        "        '''Check tool system health'''\n",
        "        alerts = []\n",
        "        \n",
        "        for tool_name, stats in self.registry.tool_usage_stats.items():\n",
        "            if stats['calls'] < 10:\n",
        "                continue  # Not enough data\n",
        "            \n",
        "            # Check error rate\n",
        "            error_rate = stats['failures'] / stats['calls']\n",
        "            if error_rate > 0.15:  # 15% error rate threshold\n",
        "                alerts.append({\n",
        "                    'type': 'high_error_rate',\n",
        "                    'tool': tool_name,\n",
        "                    'error_rate': error_rate,\n",
        "                    'severity': 'high'\n",
        "                })\n",
        "            \n",
        "            # Check latency regression\n",
        "            if len(stats['avg_latency']) >= 100:\n",
        "                recent_latency = np.mean(stats['avg_latency'][-100:])\n",
        "                baseline_latency = self.registry.tools[tool_name]['definition'].avg_latency_ms\n",
        "                \n",
        "                if recent_latency > baseline_latency * 2:  # 2x slower\n",
        "                    alerts.append({\n",
        "                        'type': 'latency_regression',\n",
        "                        'tool': tool_name,\n",
        "                        'current_latency': recent_latency,\n",
        "                        'baseline': baseline_latency,\n",
        "                        'severity': 'medium'\n",
        "                    })\n",
        "        \n",
        "        return alerts\n",
        "    \n",
        "    def get_tool_usage_report(self) -> dict:\n",
        "        '''Generate usage report for analysis'''\n",
        "        report = {}\n",
        "        \n",
        "        for tool_name, stats in self.registry.tool_usage_stats.items():\n",
        "            if stats['calls'] > 0:\n",
        "                report[tool_name] = {\n",
        "                    'calls': stats['calls'],\n",
        "                    'success_rate': stats['successes'] / stats['calls'],\n",
        "                    'avg_latency_ms': np.mean(stats['avg_latency']) if stats['avg_latency'] else 0,\n",
        "                    'p95_latency_ms': np.percentile(stats['avg_latency'], 95) if len(stats['avg_latency']) > 10 else 0\n",
        "                }\n",
        "        \n",
        "        return report\n",
        "```\n",
        "\n",
        "**Key Benefits:**\n",
        "- Prevents tool hallucination with validation at multiple levels\n",
        "- Handles 20+ tools by dynamically selecting relevant subset (5-7 per query)\n",
        "- Automatic retry and error recovery\n",
        "- Real-time monitoring and alerting\n",
        "- Cost optimization through tool selection\n",
        "\n",
        "**Production Metrics:**\n",
        "- Tool hallucination rate: <1% (vs 10-15% without validation)\n",
        "- Tool success rate: >95% (with retries)\n",
        "- Average tool calls per query: 2.5 (down from 5-10 with selection)\n",
        "- Cost per query: 40% reduction through smart selection\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, qa in enumerate(interview_questions_langchain, 1):\n",
        "    print(f\"\\n{'=' * 100}\")\n",
        "    print(f\"LANGCHAIN & AGENTS - Q{i} [{qa['level']} Level]\")\n",
        "    print('=' * 100)\n",
        "    print(f\"\\n{qa['question']}\\n\")\n",
        "    print(\"ANSWER:\")\n",
        "    print(qa['answer'])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3 Summary: Production LangChain & Agents\n",
        "\n",
        "### Key Takeaways for Experienced Engineers\n",
        "\n",
        "Production chain and agent systems require careful cost management, memory optimization, and robust tool orchestration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"MODULE 3: LANGCHAIN & AGENTS - KEY TAKEAWAYS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "summary = {\n",
        "    \"Cost Optimization\": [\n",
        "        \"Tool caching: 20-25% cost reduction with minimal implementation (1-2 days)\",\n",
        "        \"Model routing: 30% reduction by routing simple tasks to GPT-3.5 (15x cheaper)\",\n",
        "        \"Plan-and-Execute: 40% reduction vs ReAct (2 LLM calls vs 5-10)\",\n",
        "        \"Tool fusion: Batch multiple calls into single API request (50%+ savings)\",\n",
        "        \"Monitor tool usage: Track redundant calls (15% of calls can be eliminated)\",\n",
        "    ],\n",
        "    \"Memory Management\": [\n",
        "        \"Hierarchical memory: Short-term (recent) + Long-term (summarized) + Facts\",\n",
        "        \"Compression ratio: 5-10x (500 tokens represents 5000 tokens of history)\",\n",
        "        \"Handles 50-100 turns vs 20 with naive approach\",\n",
        "        \"Fact extraction: Preserve important context from early turns\",\n",
        "        \"Vector memory: Alternative for unlimited history with semantic retrieval\",\n",
        "    ],\n",
        "    \"Tool Orchestration\": [\n",
        "        \"Tool registry: Central validation and monitoring for all tools\",\n",
        "        \"Dynamic selection: Show LLM only 5-7 relevant tools per query (not all 20+)\",\n",
        "        \"Validation layers: Registry → Parameter types → Execution → Retry\",\n",
        "        \"Examples in prompts: Reduces tool hallucination from 10-15% to <1%\",\n",
        "        \"Automatic retry: 2-3 attempts with timeout handling (95%+ success rate)\",\n",
        "    ],\n",
        "    \"Chain Design\": [\n",
        "        \"Robust chains: Automatic retry with feedback to LLM on parse failures\",\n",
        "        \"Metrics tracking: Success rate, latency, token usage per chain execution\",\n",
        "        \"Error recovery: Graceful degradation rather than hard failures\",\n",
        "        \"Output validation: Pydantic schemas with structured parsing\",\n",
        "    ],\n",
        "    \"Production Patterns\": [\n",
        "        \"ReAct: Good for exploration, expensive (5-10 LLM calls)\",\n",
        "        \"Plan-and-Execute: Better for production, efficient (2-3 LLM calls)\",\n",
        "        \"Tool caching: Essential for repeated queries (30% cache hit rate typical)\",\n",
        "        \"Monitoring: Tool success rate, latency regression, cost per query\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "for section, points in summary.items():\n",
        "    print(f\"\\n{section}:\")\n",
        "    for point in points:\n",
        "        print(f\"  - {point}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"\\nINTERVIEW QUESTIONS SUMMARY:\")\n",
        "print(\"  - Cost Optimization: Reduce agent cost from $0.30 to $0.15 per query\")\n",
        "print(\"  - Memory Management: Hierarchical memory for 50-100 turn conversations\")\n",
        "print(\"  - Tool Orchestration: Reliable 20+ tool system with hallucination prevention\")\n",
        "print(\"  Total: 3 advanced questions (2 Senior, 1 Staff level)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"  1. Implement tool caching + model routing for immediate 50% cost reduction\")\n",
        "print(\"  2. Deploy hierarchical memory for long conversations\")\n",
        "print(\"  3. Build tool registry with validation and monitoring\")\n",
        "print(\"  4. A/B test Plan-and-Execute pattern vs ReAct\")\n",
        "print(\"  5. Continue to Module 4: LangGraph (stateful workflows, HITL)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Agent Architectures\n",
        "\n",
        "LangChain supports multiple agent patterns:\n",
        "- **ReAct**: Reasoning + Acting in interleaved steps\n",
        "- **Plan-Execute**: Plan first, then execute\n",
        "- **Self-Ask**: Breaks down complex questions\n",
        "- **Structured Chat**: Uses structured format for tool calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any, Optional, Callable\n",
        "import json\n",
        "\n",
        "class Tool:\n",
        "    '''Tool that agent can use'''\n",
        "    def __init__(self, name: str, func: Callable, description: str):\n",
        "        self.name = name\n",
        "        self.func = func\n",
        "        self.description = description\n",
        "    \n",
        "    def run(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)\n",
        "\n",
        "class ReActAgent:\n",
        "    '''ReAct pattern: Reason + Act in loop'''\n",
        "    \n",
        "    def __init__(self, llm_func: Callable, tools: List[Tool], max_iterations=5):\n",
        "        self.llm = llm_func\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.max_iterations = max_iterations\n",
        "    \n",
        "    def run(self, task: str) -> dict:\n",
        "        '''Execute ReAct loop'''\n",
        "        conversation = []\n",
        "        \n",
        "        for i in range(self.max_iterations):\n",
        "            # Reasoning step\n",
        "            prompt = self._build_prompt(task, conversation)\n",
        "            response = self.llm(prompt)\n",
        "            \n",
        "            thought, action, action_input = self._parse_response(response)\n",
        "            \n",
        "            conversation.append({\n",
        "                'step': i + 1,\n",
        "                'thought': thought,\n",
        "                'action': action,\n",
        "                'action_input': action_input\n",
        "            })\n",
        "            \n",
        "            # Check if done\n",
        "            if action == 'Final Answer':\n",
        "                return {\n",
        "                    'answer': action_input,\n",
        "                    'steps': conversation,\n",
        "                    'iterations': i + 1\n",
        "                }\n",
        "            \n",
        "            # Acting step\n",
        "            if action in self.tools:\n",
        "                observation = self.tools[action].run(action_input)\n",
        "                conversation.append({\n",
        "                    'observation': observation\n",
        "                })\n",
        "            else:\n",
        "                conversation.append({\n",
        "                    'observation': f'Error: Unknown action {action}'\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'answer': 'Max iterations reached',\n",
        "            'steps': conversation,\n",
        "            'iterations': self.max_iterations\n",
        "        }\n",
        "    \n",
        "    def _build_prompt(self, task: str, conversation: List[dict]) -> str:\n",
        "        '''Build ReAct prompt'''\n",
        "        tool_descriptions = '\\n'.join([\n",
        "            f'- {name}: {tool.description}'\n",
        "            for name, tool in self.tools.items()\n",
        "        ])\n",
        "        \n",
        "        prompt = f'''Answer the following question using this format:\n",
        "\n",
        "Thought: [Your reasoning about what to do next]\n",
        "Action: [Tool name or \"Final Answer\"]\n",
        "Action Input: [Input to the tool or final answer]\n",
        "\n",
        "Available tools:\n",
        "{tool_descriptions}\n",
        "\n",
        "Question: {task}\n",
        "\n",
        "'''\n",
        "        \n",
        "        # Add conversation history\n",
        "        for step in conversation:\n",
        "            if 'thought' in step:\n",
        "                prompt += f\"Thought: {step['thought']}\\n\"\n",
        "                prompt += f\"Action: {step['action']}\\n\"\n",
        "                prompt += f\"Action Input: {step['action_input']}\\n\"\n",
        "            if 'observation' in step:\n",
        "                prompt += f\"Observation: {step['observation']}\\n\\n\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def _parse_response(self, response: str) -> tuple:\n",
        "        '''Parse LLM response into thought, action, input'''\n",
        "        # Simplified parser\n",
        "        lines = response.strip().split('\\n')\n",
        "        \n",
        "        thought = ''\n",
        "        action = ''\n",
        "        action_input = ''\n",
        "        \n",
        "        for line in lines:\n",
        "            if line.startswith('Thought:'):\n",
        "                thought = line.replace('Thought:', '').strip()\n",
        "            elif line.startswith('Action:'):\n",
        "                action = line.replace('Action:', '').strip()\n",
        "            elif line.startswith('Action Input:'):\n",
        "                action_input = line.replace('Action Input:', '').strip()\n",
        "        \n",
        "        return thought, action, action_input\n",
        "\n",
        "# Example tools\n",
        "def calculator(expression: str) -> str:\n",
        "    '''Calculator tool'''\n",
        "    try:\n",
        "        result = eval(expression)  # UNSAFE in prod - use safe_eval\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def search(query: str) -> str:\n",
        "    '''Mock search tool'''\n",
        "    # In prod, call actual search API\n",
        "    results = {\n",
        "        'population of France': '67 million',\n",
        "        'capital of Japan': 'Tokyo',\n",
        "        'Python release year': '1991',\n",
        "    }\n",
        "    for key, value in results.items():\n",
        "        if key.lower() in query.lower():\n",
        "            return value\n",
        "    return 'No results found'\n",
        "\n",
        "# Mock LLM\n",
        "def mock_llm(prompt: str) -> str:\n",
        "    '''Mock LLM that follows ReAct format'''\n",
        "    if 'population of France' in prompt.lower():\n",
        "        if 'Observation:' not in prompt:\n",
        "            return '''Thought: I need to search for population data\n",
        "Action: search\n",
        "Action Input: population of France'''\n",
        "        else:\n",
        "            return '''Thought: I have the answer\n",
        "Action: Final Answer\n",
        "Action Input: The population of France is 67 million'''\n",
        "    return '''Thought: Task complete\n",
        "Action: Final Answer\n",
        "Action Input: Done'''\n",
        "\n",
        "# Test ReAct agent\n",
        "tools = [\n",
        "    Tool('calculator', calculator, 'Performs mathematical calculations'),\n",
        "    Tool('search', search, 'Searches for information online'),\n",
        "]\n",
        "\n",
        "agent = ReActAgent(mock_llm, tools, max_iterations=5)\n",
        "\n",
        "print('REACT AGENT DEMONSTRATION')\n",
        "print('=' * 90)\n",
        "\n",
        "task = 'What is the population of France?'\n",
        "result = agent.run(task)\n",
        "\n",
        "print(f'\\nTask: {task}\\n')\n",
        "print(f'Answer: {result[\"answer\"]}')\n",
        "print(f'Iterations: {result[\"iterations\"]}\\n')\n",
        "\n",
        "print('Steps taken:')\n",
        "for step in result['steps']:\n",
        "    if 'thought' in step:\n",
        "        print(f\"\\n  Step {step['step']}:\")\n",
        "        print(f\"    Thought: {step['thought']}\")\n",
        "        print(f\"    Action: {step['action']}\")\n",
        "        print(f\"    Input: {step['action_input']}\")\n",
        "    if 'observation' in step:\n",
        "        print(f\"    Observation: {step['observation']}\")\n",
        "\n",
        "print('\\n' + '=' * 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Memory Management\n",
        "\n",
        "Different memory types for different needs:\n",
        "- **ConversationBufferMemory**: Store all messages\n",
        "- **ConversationSummaryMemory**: Summarize old messages\n",
        "- **ConversationTokenBufferMemory**: Limit by token count\n",
        "- **VectorStoreMemory**: Semantic search over history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import tiktoken\n",
        "\n",
        "class MemoryManager:\n",
        "    '''Production memory management for conversational systems'''\n",
        "    \n",
        "    def __init__(self, strategy='sliding_window', max_tokens=2000):\n",
        "        self.strategy = strategy\n",
        "        self.max_tokens = max_tokens\n",
        "        self.messages = []\n",
        "        self.encoding = tiktoken.encoding_for_model('gpt-4')\n",
        "    \n",
        "    def add_message(self, role: str, content: str):\n",
        "        '''Add message to memory'''\n",
        "        self.messages.append({'role': role, 'content': content})\n",
        "        \n",
        "        # Apply memory strategy\n",
        "        if self.strategy == 'sliding_window':\n",
        "            self._apply_sliding_window()\n",
        "        elif self.strategy == 'summary':\n",
        "            self._apply_summary()\n",
        "        elif self.strategy == 'semantic':\n",
        "            self._apply_semantic()\n",
        "    \n",
        "    def _count_tokens(self, messages: List[dict]) -> int:\n",
        "        '''Count tokens in message list'''\n",
        "        total = 0\n",
        "        for msg in messages:\n",
        "            total += len(self.encoding.encode(msg['content']))\n",
        "        return total\n",
        "    \n",
        "    def _apply_sliding_window(self):\n",
        "        '''Keep only recent messages within token budget'''\n",
        "        while self._count_tokens(self.messages) > self.max_tokens and len(self.messages) > 2:\n",
        "            # Keep system message (first) and user message (last), remove oldest\n",
        "            if len(self.messages) > 2:\n",
        "                self.messages.pop(1)  # Remove second message (oldest non-system)\n",
        "    \n",
        "    def _apply_summary(self):\n",
        "        '''Summarize old messages when over budget'''\n",
        "        if self._count_tokens(self.messages) > self.max_tokens:\n",
        "            # Summarize messages beyond token limit\n",
        "            # (Simplified - in prod, call LLM to generate summary)\n",
        "            old_messages = self.messages[:-4]  # Keep last 4 messages\n",
        "            summary = self._generate_summary(old_messages)\n",
        "            \n",
        "            self.messages = [\n",
        "                {'role': 'system', 'content': f'Previous conversation summary: {summary}'},\n",
        "                *self.messages[-4:]\n",
        "            ]\n",
        "    \n",
        "    def _generate_summary(self, messages: List[dict]) -> str:\n",
        "        '''Generate summary of messages'''\n",
        "        # Mock summary - in prod, use LLM\n",
        "        return f'Discussed {len(messages)} topics'\n",
        "    \n",
        "    def _apply_semantic(self):\n",
        "        '''Keep semantically relevant messages'''\n",
        "        # In production, use vector similarity to keep relevant messages\n",
        "        pass\n",
        "    \n",
        "    def get_messages(self) -> List[dict]:\n",
        "        '''Get messages for LLM context'''\n",
        "        return self.messages\n",
        "    \n",
        "    def get_summary(self) -> dict:\n",
        "        '''Get memory stats'''\n",
        "        return {\n",
        "            'total_messages': len(self.messages),\n",
        "            'total_tokens': self._count_tokens(self.messages),\n",
        "            'strategy': self.strategy,\n",
        "            'max_tokens': self.max_tokens,\n",
        "        }\n",
        "\n",
        "# Test memory strategies\n",
        "print('MEMORY MANAGEMENT DEMONSTRATION')\n",
        "print('=' * 90)\n",
        "\n",
        "for strategy in ['sliding_window', 'summary']:\n",
        "    print(f'\\nStrategy: {strategy}')\n",
        "    print('-' * 90)\n",
        "    \n",
        "    memory = MemoryManager(strategy=strategy, max_tokens=500)\n",
        "    \n",
        "    # Simulate conversation\n",
        "    memory.add_message('system', 'You are a helpful assistant.')\n",
        "    memory.add_message('user', 'What is machine learning?')\n",
        "    memory.add_message('assistant', 'Machine learning is a subset of AI that enables systems to learn from data.')\n",
        "    memory.add_message('user', 'Tell me about neural networks.')\n",
        "    memory.add_message('assistant', 'Neural networks are computing systems inspired by biological neural networks.')\n",
        "    memory.add_message('user', 'What about deep learning?')\n",
        "    memory.add_message('assistant', 'Deep learning uses neural networks with multiple layers for complex patterns.')\n",
        "    memory.add_message('user', 'Explain transformers.')\n",
        "    \n",
        "    # Check memory state\n",
        "    summary = memory.get_summary()\n",
        "    print(f'  Messages retained: {summary[\"total_messages\"]}')\n",
        "    print(f'  Tokens used: {summary[\"total_tokens\"]}/{summary[\"max_tokens\"]}')\n",
        "    print(f'  Latest messages:')\n",
        "    for msg in memory.get_messages()[-2:]:\n",
        "        print(f'    {msg[\"role\"]}: {msg[\"content\"][:60]}...')\n",
        "\n",
        "print('\\n' + '=' * 90)\n",
        "print('KEY INSIGHT: Choose memory strategy based on use case')\n",
        "print('  - Sliding window: Simple, predictable token usage')\n",
        "print('  - Summary: Better context retention, more expensive')\n",
        "print('  - Semantic: Best for long conversations, most complex')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 4: Evaluation Framework\n",
        "\n",
        "Production evaluation requires:\n",
        "- **Accuracy metrics**: Exact match, F1, BLEU\n",
        "- **Semantic metrics**: BERTScore, embedding similarity\n",
        "- **Task-specific**: Groundedness, hallucination detection\n",
        "- **User metrics**: Satisfaction, task completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import Callable, List, Dict\n",
        "import numpy as np\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    '''Multi-metric evaluation framework for LLM systems'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.test_cases = []\n",
        "        self.results = []\n",
        "    \n",
        "    def add_test_case(self, \n",
        "                     name: str, \n",
        "                     input_data: str, \n",
        "                     expected_output: Any, \n",
        "                     category: str = 'general',\n",
        "                     metadata: dict = None):\n",
        "        '''Add test case with metadata'''\n",
        "        self.test_cases.append({\n",
        "            'name': name,\n",
        "            'input': input_data,\n",
        "            'expected': expected_output,\n",
        "            'category': category,\n",
        "            'metadata': metadata or {}\n",
        "        })\n",
        "    \n",
        "    def run_evaluation(self, system_func: Callable) -> pd.DataFrame:\n",
        "        '''Run all tests and collect metrics'''\n",
        "        self.results = []\n",
        "        \n",
        "        for test in self.test_cases:\n",
        "            import time\n",
        "            start = time.time()\n",
        "            \n",
        "            try:\n",
        "                actual = system_func(test['input'])\n",
        "                latency_ms = (time.time() - start) * 1000\n",
        "                \n",
        "                # Calculate multiple metrics\n",
        "                exact_match = str(actual) == str(test['expected'])\n",
        "                semantic_sim = self._semantic_similarity(str(actual), str(test['expected']))\n",
        "                \n",
        "                self.results.append({\n",
        "                    'name': test['name'],\n",
        "                    'category': test['category'],\n",
        "                    'exact_match': exact_match,\n",
        "                    'semantic_similarity': semantic_sim,\n",
        "                    'latency_ms': latency_ms,\n",
        "                    'expected': test['expected'],\n",
        "                    'actual': actual,\n",
        "                    'status': 'success',\n",
        "                    'error': None\n",
        "                })\n",
        "            \n",
        "            except Exception as e:\n",
        "                latency_ms = (time.time() - start) * 1000\n",
        "                self.results.append({\n",
        "                    'name': test['name'],\n",
        "                    'category': test['category'],\n",
        "                    'exact_match': False,\n",
        "                    'semantic_similarity': 0.0,\n",
        "                    'latency_ms': latency_ms,\n",
        "                    'expected': test['expected'],\n",
        "                    'actual': None,\n",
        "                    'status': 'error',\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(self.results)\n",
        "    \n",
        "    def _semantic_similarity(self, text1: str, text2: str) -> float:\n",
        "        '''Calculate semantic similarity (simplified)'''\n",
        "        # In production, use sentence-transformers\n",
        "        # For demo, use simple word overlap\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        \n",
        "        if not words1 or not words2:\n",
        "            return 0.0\n",
        "        \n",
        "        intersection = words1 & words2\n",
        "        union = words1 | words2\n",
        "        \n",
        "        return len(intersection) / len(union)  # Jaccard similarity\n",
        "    \n",
        "    def get_summary(self, by_category=True) -> dict:\n",
        "        '''Get evaluation summary statistics'''\n",
        "        if not self.results:\n",
        "            return {}\n",
        "        \n",
        "        df = pd.DataFrame(self.results)\n",
        "        \n",
        "        summary = {\n",
        "            'total_tests': len(df),\n",
        "            'exact_match_rate': df['exact_match'].mean(),\n",
        "            'avg_semantic_sim': df['semantic_similarity'].mean(),\n",
        "            'avg_latency_ms': df['latency_ms'].mean(),\n",
        "            'p95_latency_ms': df['latency_ms'].quantile(0.95),\n",
        "            'error_rate': (df['status'] == 'error').mean(),\n",
        "        }\n",
        "        \n",
        "        if by_category:\n",
        "            category_stats = df.groupby('category').agg({\n",
        "                'exact_match': 'mean',\n",
        "                'semantic_similarity': 'mean',\n",
        "                'latency_ms': 'mean',\n",
        "            }).to_dict()\n",
        "            summary['by_category'] = category_stats\n",
        "        \n",
        "        return summary\n",
        "    \n",
        "    def get_failing_tests(self, threshold=0.7) -> List[dict]:\n",
        "        '''Get tests with low semantic similarity'''\n",
        "        df = pd.DataFrame(self.results)\n",
        "        failing = df[df['semantic_similarity'] < threshold]\n",
        "        return failing.to_dict('records')\n",
        "    \n",
        "    def compare_runs(self, baseline_results: List[dict]) -> dict:\n",
        "        '''Compare current run against baseline'''\n",
        "        current_df = pd.DataFrame(self.results)\n",
        "        baseline_df = pd.DataFrame(baseline_results)\n",
        "        \n",
        "        # Merge on test name\n",
        "        merged = current_df.merge(baseline_df, on='name', suffixes=('_current', '_baseline'))\n",
        "        \n",
        "        comparison = {\n",
        "            'accuracy_change': merged['exact_match_current'].mean() - merged['exact_match_baseline'].mean(),\n",
        "            'latency_change_ms': merged['latency_ms_current'].mean() - merged['latency_ms_baseline'].mean(),\n",
        "            'improved_tests': len(merged[merged['semantic_similarity_current'] > merged['semantic_similarity_baseline']]),\n",
        "            'regressed_tests': len(merged[merged['semantic_similarity_current'] < merged['semantic_similarity_baseline']]),\n",
        "        }\n",
        "        \n",
        "        return comparison\n",
        "\n",
        "# Example: Evaluate Q&A system\n",
        "def simple_qa_system(question: str) -> str:\n",
        "    '''Mock Q&A system'''\n",
        "    qa_pairs = {\n",
        "        'What is the capital of France?': 'Paris',\n",
        "        'What is 2+2?': '4',\n",
        "        'Who invented Python?': 'Guido van Rossum',\n",
        "        'What is machine learning?': 'ML is a subset of AI that learns from data',\n",
        "    }\n",
        "    \n",
        "    # Fuzzy matching\n",
        "    for q, a in qa_pairs.items():\n",
        "        if q.lower() in question.lower() or question.lower() in q.lower():\n",
        "            return a\n",
        "    \n",
        "    return 'I don\\'t know'\n",
        "\n",
        "# Build evaluation suite\n",
        "print('COMPREHENSIVE EVALUATION FRAMEWORK')\n",
        "print('=' * 90)\n",
        "\n",
        "evaluator = ComprehensiveEvaluator()\n",
        "\n",
        "# Add diverse test cases\n",
        "evaluator.add_test_case('geo_1', 'What is the capital of France?', 'Paris', 'geography')\n",
        "evaluator.add_test_case('math_1', 'Calculate 2+2', '4', 'arithmetic')\n",
        "evaluator.add_test_case('tech_1', 'Who invented Python?', 'Guido van Rossum', 'technology')\n",
        "evaluator.add_test_case('ml_1', 'Explain machine learning', 'ML is a subset of AI', 'technical')\n",
        "evaluator.add_test_case('unknown_1', 'What is the meaning of life?', 'I don\\'t know', 'edge_case')\n",
        "\n",
        "# Run evaluation\n",
        "results_df = evaluator.run_evaluation(simple_qa_system)\n",
        "\n",
        "print('\\nTEST RESULTS:')\n",
        "print(results_df[['name', 'category', 'exact_match', 'semantic_similarity', 'latency_ms']].to_string(index=False))\n",
        "\n",
        "# Summary statistics\n",
        "print('\\n' + '=' * 90)\n",
        "print('SUMMARY STATISTICS:')\n",
        "summary = evaluator.get_summary()\n",
        "for key, value in summary.items():\n",
        "    if key != 'by_category':\n",
        "        if isinstance(value, float):\n",
        "            print(f'  {key}: {value:.2%}' if value < 1 else f'  {key}: {value:.2f}')\n",
        "        else:\n",
        "            print(f'  {key}: {value}')\n",
        "\n",
        "print('\\nBy Category:')\n",
        "for cat, metrics in summary.get('by_category', {}).get('exact_match', {}).items():\n",
        "    print(f'  {cat}: {metrics:.1%} accuracy')\n",
        "\n",
        "# Failing tests\n",
        "print('\\n' + '=' * 90)\n",
        "print('FAILING TESTS (semantic_sim < 0.7):')\n",
        "failing = evaluator.get_failing_tests(threshold=0.7)\n",
        "for test in failing:\n",
        "    print(f\"  {test['name']}: expected '{test['expected']}', got '{test['actual']}'\")\n",
        "\n",
        "print('\\n' + '=' * 90)\n",
        "print('KEY METRICS FOR PRODUCTION:')\n",
        "print('  - Accuracy: Core quality metric')\n",
        "print('  - Latency P95: 95th percentile response time')\n",
        "print('  - Cost: Tokens/$ per request')\n",
        "print('  - Regression detection: Compare against baseline')\n",
        "print('  - Category breakdown: Identify weak areas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Questions: LangChain & Production Chains\n",
        "\n",
        "### For Senior/Staff Engineers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "langchain_interview_questions = [\n",
        "    {\n",
        "        'level': 'Senior',\n",
        "        'question': 'Your chain has 3 sequential LLM calls. Latency is 4.5s (1.5s each). Users complain it\\'s too slow. Optimize while maintaining quality.',\n",
        "        'answer': '''\n",
        "**Current Architecture (Sequential):**\n",
        "```\n",
        "Step 1: Extract entities → 1.5s\n",
        "Step 2: Classify intent → 1.5s  \n",
        "Step 3: Generate response → 1.5s\n",
        "Total: 4.5s\n",
        "```\n",
        "\n",
        "**Optimization Strategies:**\n",
        "\n",
        "**1. Parallel Execution (Best: 1.5s, 70% improvement)**\n",
        "```python\n",
        "import asyncio\n",
        "\n",
        "class ParallelChain:\n",
        "    async def run(self, input_text: str):\n",
        "        # Run independent steps in parallel\n",
        "        entities_task = asyncio.create_task(self.extract_entities(input_text))\n",
        "        intent_task = asyncio.create_task(self.classify_intent(input_text))\n",
        "        \n",
        "        # Wait for both\n",
        "        entities, intent = await asyncio.gather(entities_task, intent_task)\n",
        "        \n",
        "        # Final step depends on previous results\n",
        "        response = await self.generate_response(input_text, entities, intent)\n",
        "        \n",
        "        return response\n",
        "\n",
        "# Latency: max(1.5s, 1.5s) + 1.5s = 3.0s (33% improvement)\n",
        "```\n",
        "\n",
        "**2. Cached Intermediate Results (2-3s, 30% improvement)**\n",
        "```python\n",
        "import hashlib\n",
        "import redis\n",
        "\n",
        "class CachedChain:\n",
        "    def __init__(self):\n",
        "        self.redis = redis.Redis()\n",
        "        self.ttl = 3600  # 1 hour\n",
        "    \n",
        "    def extract_entities(self, text: str):\n",
        "        # Cache key from input hash\n",
        "        cache_key = f'entities:{hashlib.md5(text.encode()).hexdigest()}'\n",
        "        \n",
        "        # Check cache\n",
        "        cached = self.redis.get(cache_key)\n",
        "        if cached:\n",
        "            return json.loads(cached)\n",
        "        \n",
        "        # Call LLM\n",
        "        result = llm_extract_entities(text)\n",
        "        \n",
        "        # Cache result\n",
        "        self.redis.setex(cache_key, self.ttl, json.dumps(result))\n",
        "        \n",
        "        return result\n",
        "\n",
        "# With 60% cache hit rate: 4.5s → 1.8s average\n",
        "```\n",
        "\n",
        "**3. Prompt Optimization (3.5-4s, 20% improvement)**\n",
        "```python\n",
        "# Combine steps where possible\n",
        "\n",
        "# Before (3 separate calls):\n",
        "# 1. Extract entities\n",
        "# 2. Classify intent  \n",
        "# 3. Generate response\n",
        "\n",
        "# After (2 calls):\n",
        "# 1. Extract entities + classify intent (combined)\n",
        "# 2. Generate response\n",
        "\n",
        "def combined_extraction_and_classification(text: str):\n",
        "    prompt = f'''\n",
        "    Analyze this text and return JSON:\n",
        "    {{\n",
        "      \"entities\": {{\"people\": [...], \"places\": [...]}},\n",
        "      \"intent\": \"question|statement|request\"\n",
        "    }}\n",
        "    \n",
        "    Text: {text}\n",
        "    '''\n",
        "    return llm(prompt)\n",
        "\n",
        "# Latency: 1.5s + 1.5s = 3.0s (33% improvement)\n",
        "```\n",
        "\n",
        "**4. Streaming Responses (Perceived: <1s, Actual: same)**\n",
        "```python\n",
        "import openai\n",
        "\n",
        "def stream_response(prompt: str):\n",
        "    '''Stream tokens as they're generated'''\n",
        "    for chunk in openai.ChatCompletion.create(\n",
        "        model='gpt-4',\n",
        "        messages=[{'role': 'user', 'content': prompt}],\n",
        "        stream=True\n",
        "    ):\n",
        "        if chunk.choices[0].delta.get('content'):\n",
        "            yield chunk.choices[0].delta.content\n",
        "\n",
        "# User sees first token in ~200ms\n",
        "# Much better UX even if total time is same\n",
        "```\n",
        "\n",
        "**5. Smaller Model for Simple Steps (3s, 33% improvement)**\n",
        "```python\n",
        "class HybridChain:\n",
        "    '''Use smaller/faster models for simple tasks'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.fast_model = 'gpt-3.5-turbo'  # ~0.5s\n",
        "        self.smart_model = 'gpt-4'  # ~1.5s\n",
        "    \n",
        "    def extract_entities(self, text: str):\n",
        "        # Simple extraction → fast model\n",
        "        return llm(text, model=self.fast_model)  # 0.5s\n",
        "    \n",
        "    def classify_intent(self, text: str):\n",
        "        # Simple classification → fast model\n",
        "        return llm(text, model=self.fast_model)  # 0.5s\n",
        "    \n",
        "    def generate_response(self, text: str, entities, intent):\n",
        "        # Complex generation → smart model\n",
        "        return llm(text, model=self.smart_model)  # 1.5s\n",
        "\n",
        "# Total: 0.5s + 0.5s + 1.5s = 2.5s (45% improvement)\n",
        "# Cost reduction: ~70% (GPT-3.5 is 10x cheaper)\n",
        "```\n",
        "\n",
        "**6. Complete Solution (Combined Approach):**\n",
        "```python\n",
        "class OptimizedChain:\n",
        "    '''Production-optimized chain with multiple strategies'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cache = redis.Redis()\n",
        "        self.fast_model = 'gpt-3.5-turbo'\n",
        "        self.smart_model = 'gpt-4'\n",
        "    \n",
        "    async def run(self, text: str) -> dict:\n",
        "        # Check cache first (entire chain)\n",
        "        cache_key = f'chain:{hashlib.md5(text.encode()).hexdigest()}'\n",
        "        cached = self.cache.get(cache_key)\n",
        "        if cached:\n",
        "            return json.loads(cached)  # ~5ms\n",
        "        \n",
        "        # Parallel execution of independent steps (fast model)\n",
        "        entities_task = self.extract_entities_async(text)  # 0.5s\n",
        "        intent_task = self.classify_intent_async(text)  # 0.5s\n",
        "        \n",
        "        entities, intent = await asyncio.gather(entities_task, intent_task)\n",
        "        # Parallel time: max(0.5s, 0.5s) = 0.5s\n",
        "        \n",
        "        # Final step (smart model, streaming)\n",
        "        response_stream = self.generate_response_stream(text, entities, intent)  # 1.5s\n",
        "        \n",
        "        # Start streaming to user immediately\n",
        "        response = ''\n",
        "        async for chunk in response_stream:\n",
        "            response += chunk\n",
        "            yield chunk  # Stream to user\n",
        "        \n",
        "        # Cache complete result\n",
        "        result = {'entities': entities, 'intent': intent, 'response': response}\n",
        "        self.cache.setex(cache_key, 3600, json.dumps(result))\n",
        "        \n",
        "        # Total latency: 0.5s + 1.5s = 2.0s (55% improvement)\n",
        "        # With cache (60% hit rate): 0.6 * 0.005s + 0.4 * 2.0s = 0.8s average\n",
        "        # User perceives: ~200ms (streaming)\n",
        "```\n",
        "\n",
        "**Results Summary:**\n",
        "| Strategy | Latency | Improvement | Cost Impact |\n",
        "|----------|---------|-------------|-------------|\n",
        "| Baseline | 4.5s | - | $0.009 |\n",
        "| Parallel | 3.0s | 33% | $0.009 |\n",
        "| Cache (60% hit) | 1.8s | 60% | $0.004 |\n",
        "| Hybrid models | 2.5s | 45% | $0.003 |\n",
        "| **Combined** | **2.0s** | **55%** | **$0.003** |\n",
        "| **+ Stream** | **0.2s perceived** | **95% perceived** | **$0.003** |\n",
        "\n",
        "**Recommendation:**\n",
        "- Implement combined solution\n",
        "- Monitor cache hit rate (target: 60%+)\n",
        "- Use streaming for better UX\n",
        "- A/B test: measure user satisfaction vs latency\n",
        "        ''',\n",
        "    },\n",
        "    {\n",
        "        'level': 'Staff',\n",
        "        'question': 'Design an evaluation system that runs continuously in production, catching regressions before they impact users. Include statistical significance testing.',\n",
        "        'answer': '''\n",
        "**Continuous Evaluation Architecture:**\n",
        "\n",
        "**1. Shadow Traffic Evaluation**\n",
        "```python\n",
        "class ShadowEvaluator:\n",
        "    '''Run new model versions on production traffic without affecting users'''\n",
        "    \n",
        "    def __init__(self, primary_model, shadow_model):\n",
        "        self.primary = primary_model\n",
        "        self.shadow = shadow_model\n",
        "        self.metrics_store = MetricsStore()\n",
        "    \n",
        "    async def handle_request(self, request: dict) -> dict:\n",
        "        # Serve from primary model\n",
        "        primary_response = await self.primary.generate(request)\n",
        "        \n",
        "        # Shadow evaluation (async, doesn't block)\n",
        "        asyncio.create_task(self._shadow_eval(request, primary_response))\n",
        "        \n",
        "        return primary_response\n",
        "    \n",
        "    async def _shadow_eval(self, request: dict, primary_response: dict):\n",
        "        '''Evaluate shadow model in background'''\n",
        "        try:\n",
        "            # Generate shadow response\n",
        "            shadow_response = await self.shadow.generate(request)\n",
        "            \n",
        "            # Compare metrics\n",
        "            metrics = {\n",
        "                'timestamp': time.time(),\n",
        "                'primary_latency': primary_response['latency_ms'],\n",
        "                'shadow_latency': shadow_response['latency_ms'],\n",
        "                'primary_tokens': primary_response['tokens'],\n",
        "                'shadow_tokens': shadow_response['tokens'],\n",
        "                'semantic_similarity': self._semantic_sim(\n",
        "                    primary_response['text'],\n",
        "                    shadow_response['text']\n",
        "                ),\n",
        "            }\n",
        "            \n",
        "            self.metrics_store.record('shadow_eval', metrics)\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.metrics_store.record('shadow_error', {'error': str(e)})\n",
        "```\n",
        "\n",
        "**2. Golden Dataset Evaluation**\n",
        "```python\n",
        "class GoldenDatasetEvaluator:\n",
        "    '''Continuously evaluate on curated test set'''\n",
        "    \n",
        "    def __init__(self, golden_dataset_path: str):\n",
        "        self.golden_dataset = self.load_golden_dataset(golden_dataset_path)\n",
        "        self.evaluation_interval = 3600  # 1 hour\n",
        "    \n",
        "    async def run_continuous_eval(self):\n",
        "        '''Run evaluation every hour'''\n",
        "        while True:\n",
        "            results = await self.evaluate()\n",
        "            \n",
        "            # Check for regressions\n",
        "            if self.detect_regression(results):\n",
        "                await self.alert_team(results)\n",
        "            \n",
        "            await asyncio.sleep(self.evaluation_interval)\n",
        "    \n",
        "    async def evaluate(self) -> dict:\n",
        "        '''Run evaluation on golden dataset'''\n",
        "        results = []\n",
        "        \n",
        "        for test_case in self.golden_dataset:\n",
        "            try:\n",
        "                prediction = await self.model.generate(test_case['input'])\n",
        "                \n",
        "                metrics = {\n",
        "                    'test_id': test_case['id'],\n",
        "                    'exact_match': prediction == test_case['expected'],\n",
        "                    'bleu_score': self.calculate_bleu(prediction, test_case['expected']),\n",
        "                    'semantic_sim': self.semantic_similarity(prediction, test_case['expected']),\n",
        "                    'latency_ms': prediction['latency_ms'],\n",
        "                    'tokens': prediction['tokens'],\n",
        "                }\n",
        "                \n",
        "                results.append(metrics)\n",
        "                \n",
        "            except Exception as e:\n",
        "                results.append({'test_id': test_case['id'], 'error': str(e)})\n",
        "        \n",
        "        return self.aggregate_results(results)\n",
        "```\n",
        "\n",
        "**3. Statistical Significance Testing**\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "\n",
        "class StatisticalTester:\n",
        "    '''Test if changes are statistically significant'''\n",
        "    \n",
        "    def __init__(self, min_sample_size=100, alpha=0.05):\n",
        "        self.min_sample_size = min_sample_size\n",
        "        self.alpha = alpha  # Significance level\n",
        "    \n",
        "    def compare_metrics(self, baseline: List[float], candidate: List[float]) -> dict:\n",
        "        '''Compare two model versions with statistical tests'''\n",
        "        \n",
        "        if len(baseline) < self.min_sample_size or len(candidate) < self.min_sample_size:\n",
        "            return {'status': 'insufficient_data', 'p_value': None}\n",
        "        \n",
        "        # T-test for means\n",
        "        t_statistic, p_value = stats.ttest_ind(baseline, candidate)\n",
        "        \n",
        "        # Effect size (Cohen's d)\n",
        "        effect_size = self.cohens_d(baseline, candidate)\n",
        "        \n",
        "        # Determine significance\n",
        "        is_significant = p_value < self.alpha\n",
        "        \n",
        "        # Practical significance (effect size)\n",
        "        is_practically_significant = abs(effect_size) > 0.2  # Small effect\n",
        "        \n",
        "        return {\n",
        "            'p_value': p_value,\n",
        "            'is_statistically_significant': is_significant,\n",
        "            'effect_size': effect_size,\n",
        "            'is_practically_significant': is_practically_significant,\n",
        "            'baseline_mean': np.mean(baseline),\n",
        "            'candidate_mean': np.mean(candidate),\n",
        "            'relative_change': (np.mean(candidate) - np.mean(baseline)) / np.mean(baseline),\n",
        "        }\n",
        "    \n",
        "    def cohens_d(self, group1: List[float], group2: List[float]) -> float:\n",
        "        '''Calculate Cohen\\'s d effect size'''\n",
        "        n1, n2 = len(group1), len(group2)\n",
        "        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
        "        \n",
        "        # Pooled standard deviation\n",
        "        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
        "        \n",
        "        # Cohen's d\n",
        "        d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
        "        \n",
        "        return d\n",
        "```\n",
        "\n",
        "**4. Regression Detection**\n",
        "```python\n",
        "class RegressionDetector:\n",
        "    '''Detect regressions using multiple signals'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.baseline_metrics = self.load_baseline()\n",
        "        self.stat_tester = StatisticalTester()\n",
        "    \n",
        "    def detect_regression(self, current_metrics: dict) -> bool:\n",
        "        '''Check if current metrics indicate regression'''\n",
        "        regressions = []\n",
        "        \n",
        "        for metric_name in ['accuracy', 'latency_p95', 'cost_per_query']:\n",
        "            baseline = self.baseline_metrics[metric_name]\n",
        "            current = current_metrics[metric_name]\n",
        "            \n",
        "            # Statistical test\n",
        "            test_result = self.stat_tester.compare_metrics(baseline, current)\n",
        "            \n",
        "            # Check for regression\n",
        "            if metric_name == 'accuracy':\n",
        "                # Lower is bad\n",
        "                if test_result['is_statistically_significant'] and test_result['relative_change'] < -0.02:\n",
        "                    regressions.append({\n",
        "                        'metric': metric_name,\n",
        "                        'change': test_result['relative_change'],\n",
        "                        'p_value': test_result['p_value'],\n",
        "                    })\n",
        "            \n",
        "            elif metric_name in ['latency_p95', 'cost_per_query']:\n",
        "                # Higher is bad\n",
        "                if test_result['is_statistically_significant'] and test_result['relative_change'] > 0.1:\n",
        "                    regressions.append({\n",
        "                        'metric': metric_name,\n",
        "                        'change': test_result['relative_change'],\n",
        "                        'p_value': test_result['p_value'],\n",
        "                    })\n",
        "        \n",
        "        return len(regressions) > 0, regressions\n",
        "```\n",
        "\n",
        "**5. Complete System**\n",
        "```python\n",
        "class ContinuousEvaluationSystem:\n",
        "    '''Production continuous evaluation system'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.shadow_eval = ShadowEvaluator(primary_model, shadow_model)\n",
        "        self.golden_eval = GoldenDatasetEvaluator('golden_dataset.json')\n",
        "        self.regression_detector = RegressionDetector()\n",
        "        self.alert_service = AlertService()\n",
        "    \n",
        "    async def start(self):\n",
        "        '''Start continuous evaluation'''\n",
        "        # Run evaluations in parallel\n",
        "        await asyncio.gather(\n",
        "            self.golden_eval.run_continuous_eval(),\n",
        "            self.monitor_shadow_traffic(),\n",
        "            self.monitor_user_feedback()\n",
        "        )\n",
        "    \n",
        "    async def monitor_shadow_traffic(self):\n",
        "        '''Monitor shadow evaluation results'''\n",
        "        while True:\n",
        "            # Aggregate last hour of shadow evals\n",
        "            metrics = await self.shadow_eval.get_metrics(window_hours=1)\n",
        "            \n",
        "            # Detect regressions\n",
        "            has_regression, details = self.regression_detector.detect_regression(metrics)\n",
        "            \n",
        "            if has_regression:\n",
        "                await self.alert_service.send_alert(\n",
        "                    severity='high',\n",
        "                    title='Regression detected in shadow evaluation',\n",
        "                    details=details\n",
        "                )\n",
        "            \n",
        "            await asyncio.sleep(300)  # Check every 5 minutes\n",
        "    \n",
        "    async def monitor_user_feedback(self):\n",
        "        '''Monitor implicit user feedback'''\n",
        "        while True:\n",
        "            feedback_metrics = await self.collect_feedback_metrics()\n",
        "            \n",
        "            # Metrics: thumbs up/down, session abandonment, retry rate\n",
        "            if feedback_metrics['satisfaction_score'] < 0.7:\n",
        "                await self.alert_service.send_alert(\n",
        "                    severity='medium',\n",
        "                    title='User satisfaction drop detected',\n",
        "                    details=feedback_metrics\n",
        "                )\n",
        "            \n",
        "            await asyncio.sleep(600)  # Check every 10 minutes\n",
        "```\n",
        "\n",
        "**Key Metrics to Monitor:**\n",
        "1. **Accuracy**: Exact match, BLEU, semantic similarity\n",
        "2. **Latency**: P50, P95, P99\n",
        "3. **Cost**: Tokens per query, $ per query\n",
        "4. **User satisfaction**: Thumbs up/down rate, retry rate\n",
        "5. **Error rate**: Parse failures, timeouts\n",
        "\n",
        "**Alert Conditions:**\n",
        "- Accuracy drop > 2% (p < 0.05)\n",
        "- Latency P95 increase > 10% (p < 0.05)\n",
        "- Cost increase > 15% (p < 0.05)\n",
        "- User satisfaction < 70%\n",
        "- Error rate > 5%\n",
        "\n",
        "**Best Practices:**\n",
        "1. Use shadow traffic for pre-deployment testing\n",
        "2. Maintain golden dataset (200-500 diverse cases)\n",
        "3. Require statistical significance (p < 0.05) + practical significance (effect size > 0.2)\n",
        "4. Monitor user feedback as ultimate signal\n",
        "5. Automate rollback on critical regressions\n",
        "        ''',\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, qa in enumerate(langchain_interview_questions, 1):\n",
        "    print(f'\\n{'=' * 100}')\n",
        "    print(f'Q{i} [{qa[\"level\"]} Level]')\n",
        "    print('=' * 100)\n",
        "    print(f'\\n{qa[\"question\"]}\\n')\n",
        "    print('ANSWER:')\n",
        "    print(qa['answer'])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
