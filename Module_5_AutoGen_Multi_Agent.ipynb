{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5: AutoGen\n\n## Applied AI Scientist Field Notes - Expanded Edition\n\n---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Module 5: AutoGen - Conversational Multi-Agent Systems\n\n### Topics\n1. Agent roles and personas\n2. Conversational patterns\n3. Code generation and execution\n4. Error correction loops\n5. Multi-agent collaboration\n6. Human-in-the-loop\n\n---"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "%pip install -q pyautogen\n\nprint('AutoGen installed!')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 1: Multi-Agent Conversations\n\nAutoGen pattern:\n- **UserProxy**: Represents user, can execute code\n- **AssistantAgent**: LLM-powered agent\n- **Conversation loop**: Agents alternate until termination\n- **Code execution**: Sandboxed Python execution\n- **Error feedback**: Failed code triggers refinement"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class Agent:\n    '''Simple agent for multi-agent conversations'''\n    \n    def __init__(self, name: str, role: str, system_message: str):\n        self.name = name\n        self.role = role\n        self.system_message = system_message\n        self.history = []\n    \n    def respond(self, message: str) -> str:\n        self.history.append({'role': 'user', 'content': message})\n        \n        # Mock responses based on role\n        if self.role == 'coder':\n            response = 'def fibonacci(n):\\n    return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)'\n        elif self.role == 'executor':\n            response = 'Execution result: Success, output=55'\n        elif self.role == 'reviewer':\n            response = 'Code quality: Good. Tests passed.'\n        else:\n            response = f'{self.name}: Acknowledged'\n        \n        self.history.append({'role': 'assistant', 'content': response})\n        return response\n\nclass MultiAgentSystem:\n    def __init__(self):\n        self.agents = {}\n        self.log = []\n    \n    def add_agent(self, agent: Agent):\n        self.agents[agent.name] = agent\n    \n    def run(self, task: str, max_turns=5):\n        self.log = []\n        current_msg = task\n        \n        for turn in range(max_turns):\n            for name in self.agents.keys():\n                agent = self.agents[name]\n                response = agent.respond(current_msg)\n                \n                self.log.append({\n                    'turn': turn + 1,\n                    'agent': name,\n                    'message': response[:80] + '...' if len(response) > 80 else response\n                })\n                \n                current_msg = response\n                \n                if 'TERMINATE' in response or 'done' in response.lower():\n                    return self.log\n        \n        return self.log\n\n# Example: Coder-Executor-Reviewer workflow\nprint('Multi-Agent System Demo')\nprint('=' * 80)\n\ncoder = Agent('Coder', 'coder', 'Write code to solve problems')\nexecutor = Agent('Executor', 'executor', 'Execute code and return results')\nreviewer = Agent('Reviewer', 'reviewer', 'Review code quality')\n\nmas = MultiAgentSystem()\nmas.add_agent(coder)\nmas.add_agent(executor)\nmas.add_agent(reviewer)\n\ntask = 'Calculate fibonacci(10)'\nconversation = mas.run(task, max_turns=2)\n\nprint(f'\\nTask: {task}\\n')\nfor entry in conversation:\n    print(f'Turn {entry[\"turn\"]} | {entry[\"agent\"]}: {entry[\"message\"]}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 2: Code Execution with Safety\n\nAutoGen's power comes from code execution, but this requires careful sandboxing:\n- **Docker containers**: Isolated execution environment\n- **Resource limits**: CPU, memory, time constraints\n- **Network isolation**: Prevent data exfiltration\n- **Filesystem restrictions**: Read-only mounts\n- **Allowlist**: Only approved libraries"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nimport tempfile\nimport os\nimport signal\nfrom typing import Dict, Tuple\nimport json\n\nclass SafeCodeExecutor:\n    '''Secure code execution sandbox'''\n    \n    def __init__(self, timeout_seconds=5, max_memory_mb=512):\n        self.timeout = timeout_seconds\n        self.max_memory = max_memory_mb * 1024 * 1024  # Convert to bytes\n        \n        # Allowed imports\n        self.allowed_imports = {\n            'math', 'random', 'datetime', 'json', 'collections',\n            'itertools', 'functools', 're', 'string', 'typing'\n        }\n        \n        # Blocked operations\n        self.blocked_patterns = [\n            'import os', 'import sys', 'import subprocess',\n            'exec(', 'eval(', 'compile(',\n            '__import__', 'open(', 'file(',\n            'socket', 'urllib', 'requests',\n        ]\n    \n    def validate_code(self, code: str) -> Tuple[bool, str]:\n        '''Pre-execution code validation'''\n        code_lower = code.lower()\n        \n        # Check for blocked patterns\n        for pattern in self.blocked_patterns:\n            if pattern in code_lower:\n                return False, f'Blocked operation: {pattern}'\n        \n        # Check imports\n        import re\n        imports = re.findall(r'import (\\w+)', code)\n        imports.extend(re.findall(r'from (\\w+) import', code))\n        \n        for imp in imports:\n            if imp not in self.allowed_imports:\n                return False, f'Disallowed import: {imp}'\n        \n        return True, 'Valid'\n    \n    def execute(self, code: str) -> Dict[str, Any]:\n        '''Execute code in sandbox'''\n        \n        # Validate first\n        is_valid, message = self.validate_code(code)\n        if not is_valid:\n            return {\n                'status': 'blocked',\n                'error': message,\n                'output': None\n            }\n        \n        # Create temporary file\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(code)\n            temp_file = f.name\n        \n        try:\n            # Execute with resource limits\n            # In production, use Docker or similar container isolation\n            result = subprocess.run(\n                ['python3', temp_file],\n                capture_output=True,\n                text=True,\n                timeout=self.timeout,\n                # Resource limits (Linux only)\n                preexec_fn=self._set_resource_limits if os.name == 'posix' else None\n            )\n            \n            return {\n                'status': 'success' if result.returncode == 0 else 'error',\n                'output': result.stdout,\n                'error': result.stderr if result.returncode != 0 else None,\n                'returncode': result.returncode\n            }\n            \n        except subprocess.TimeoutExpired:\n            return {\n                'status': 'timeout',\n                'error': f'Execution exceeded {self.timeout}s timeout',\n                'output': None\n            }\n        \n        except Exception as e:\n            return {\n                'status': 'error',\n                'error': str(e),\n                'output': None\n            }\n        \n        finally:\n            # Cleanup\n            try:\n                os.unlink(temp_file)\n            except:\n                pass\n    \n    def _set_resource_limits(self):\n        '''Set resource limits for subprocess (Linux)'''\n        try:\n            import resource\n            \n            # Memory limit\n            resource.setrlimit(resource.RLIMIT_AS, (self.max_memory, self.max_memory))\n            \n            # CPU time limit\n            resource.setrlimit(resource.RLIMIT_CPU, (self.timeout, self.timeout))\n            \n            # No file creation\n            resource.setrlimit(resource.RLIMIT_FSIZE, (0, 0))\n            \n        except ImportError:\n            pass  # Not on Linux\n\n# Test safe executor\nprint('SAFE CODE EXECUTION DEMONSTRATION')\nprint('=' * 90)\n\nexecutor = SafeCodeExecutor(timeout_seconds=2, max_memory_mb=128)\n\ntest_cases = [\n    # Safe code\n    '''\nimport math\nresult = math.sqrt(16)\nprint(f\"Square root of 16: {result}\")\n    ''',\n    \n    # Blocked import\n    '''\nimport os\nprint(os.listdir('/'))\n    ''',\n    \n    # Blocked operation\n    '''\neval(\"print('hello')\")\n    ''',\n    \n    # Timeout\n    '''\nimport time\ntime.sleep(10)\nprint('done')\n    ''',\n]\n\nfor i, code in enumerate(test_cases, 1):\n    print(f'\\nTest {i}:')\n    print(f'Code: {code.strip()[:60]}...')\n    \n    result = executor.execute(code)\n    \n    print(f'Status: {result[\"status\"]}')\n    if result['output']:\n        print(f'Output: {result[\"output\"].strip()}')\n    if result['error']:\n        print(f'Error: {result[\"error\"][:80]}...')\n    print('-' * 90)\n\nprint('\\n' + '=' * 90)\nprint('KEY SECURITY PRINCIPLES:')\nprint('  - Never execute arbitrary code without validation')\nprint('  - Use containerization (Docker) for true isolation')\nprint('  - Set strict resource limits (memory, CPU, time)')\nprint('  - Allowlist imports, block dangerous operations')\nprint('  - Network isolation to prevent exfiltration')\nprint('  - Audit log all code execution attempts')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 3: Error Correction Loop\n\nAutoGen excels at iterative refinement:\n1. **Coder** generates code\n2. **Executor** runs code, captures errors\n3. **Coder** sees error, fixes code\n4. Repeat until success or max attempts"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CodeGenerationAgent:\n    '''Agent that generates and refines code'''\n    \n    def __init__(self, llm_func: Callable):\n        self.llm = llm_func\n        self.generation_history = []\n    \n    def generate(self, task: str, error_feedback: str = None) -> str:\n        '''Generate or refine code based on feedback'''\n        \n        if error_feedback is None:\n            # Initial generation\n            prompt = f'''Write Python code to accomplish this task:\n{task}\n\nProvide working code with proper error handling.'''\n        else:\n            # Refinement based on error\n            prompt = f'''The previous code failed with this error:\n{error_feedback}\n\nOriginal task: {task}\n\nFix the code to handle this error. Provide the complete corrected code.'''\n        \n        code = self.llm(prompt)\n        self.generation_history.append(code)\n        \n        return code\n\nclass ErrorCorrectionLoop:\n    '''Iterative code generation with error correction'''\n    \n    def __init__(self, coder: CodeGenerationAgent, executor: SafeCodeExecutor, max_iterations=5):\n        self.coder = coder\n        self.executor = executor\n        self.max_iterations = max_iterations\n        self.execution_log = []\n    \n    def run(self, task: str) -> Dict[str, Any]:\n        '''Execute error correction loop'''\n        error_feedback = None\n        \n        for iteration in range(self.max_iterations):\n            print(f'\\nIteration {iteration + 1}/{self.max_iterations}')\n            print('-' * 70)\n            \n            # Generate code\n            code = self.coder.generate(task, error_feedback)\n            print(f'Generated code ({len(code)} chars)')\n            \n            # Execute\n            exec_result = self.executor.execute(code)\n            \n            self.execution_log.append({\n                'iteration': iteration + 1,\n                'code': code,\n                'exec_result': exec_result,\n                'timestamp': datetime.utcnow().isoformat()\n            })\n            \n            if exec_result['status'] == 'success':\n                print(f'\u2713 Execution successful!')\n                print(f'Output: {exec_result[\"output\"]}')\n                return {\n                    'status': 'success',\n                    'code': code,\n                    'output': exec_result['output'],\n                    'iterations': iteration + 1,\n                    'execution_log': self.execution_log\n                }\n            \n            elif exec_result['status'] == 'blocked':\n                print(f'\u2717 Code blocked: {exec_result[\"error\"]}')\n                error_feedback = exec_result['error']\n            \n            else:\n                print(f'\u2717 Execution failed: {exec_result[\"error\"][:100]}')\n                error_feedback = exec_result['error']\n        \n        # Max iterations reached\n        return {\n            'status': 'max_iterations',\n            'code': self.coder.generation_history[-1] if self.coder.generation_history else None,\n            'error': 'Failed to generate working code after maximum iterations',\n            'iterations': self.max_iterations,\n            'execution_log': self.execution_log\n        }\n\n# Mock LLM for code generation\ndef mock_code_llm(prompt: str) -> str:\n    '''Mock LLM that generates code (sometimes with bugs)'''\n    import random\n    \n    if 'fibonacci' in prompt.lower():\n        if 'error' in prompt.lower() or 'fix' in prompt.lower():\n            # Fixed version\n            return '''def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nresult = fibonacci(10)\nprint(f\"Fibonacci(10) = {result}\")'''\n        else:\n            # Buggy version\n            return '''def fibonacci(n):\n    return fibonacci(n-1) + fibonacci(n-2)  # Missing base case!\n\nresult = fibonacci(10)\nprint(f\"Result: {result}\")'''\n    \n    return 'print(\"Hello, World!\")'\n\n# Demo error correction loop\nprint('\\nERROR CORRECTION LOOP DEMONSTRATION')\nprint('=' * 90)\n\ncoder = CodeGenerationAgent(mock_code_llm)\nexecutor = SafeCodeExecutor(timeout_seconds=2)\nloop = ErrorCorrectionLoop(coder, executor, max_iterations=3)\n\ntask = 'Write a function to calculate the 10th Fibonacci number and print the result'\nresult = loop.run(task)\n\nprint('\\n' + '=' * 90)\nprint(f'Final status: {result[\"status\"]}')\nprint(f'Iterations required: {result[\"iterations\"]}')\nif result['status'] == 'success':\n    print(f'Final output: {result[\"output\"]}')\n\nprint('\\n' + '=' * 90)\nprint('KEY BENEFITS OF ERROR CORRECTION:')\nprint('  - Automatically fixes common bugs (syntax, logic, runtime)')\nprint('  - No human intervention needed for ~80% of errors')\nprint('  - Learns from failures within the conversation')\nprint('  - Typical success rate: 70-90% within 3 iterations')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 4: Advanced Multi-Agent Orchestration\n\nComplex multi-agent patterns:\n- **Sequential**: Agents execute in order\n- **Parallel**: Multiple agents work simultaneously\n- **Hierarchical**: Manager delegates to specialists\n- **Group chat**: All agents can contribute\n- **Debate**: Agents argue until consensus"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List, Dict, Callable\nfrom collections import defaultdict\nimport time\n\nclass AdvancedAgent:\n    '''Agent with expertise and voting capability'''\n    \n    def __init__(self, name: str, role: str, expertise: List[str], llm_func: Callable):\n        self.name = name\n        self.role = role\n        self.expertise = expertise\n        self.llm = llm_func\n        self.conversation_history = []\n        self.contributions = 0\n    \n    def should_respond(self, message: str) -> bool:\n        '''Decide if agent should respond based on expertise'''\n        # Check if message relates to agent's expertise\n        return any(exp.lower() in message.lower() for exp in self.expertise)\n    \n    def respond(self, message: str, conversation_context: List[dict]) -> str:\n        '''Generate response based on conversation context'''\n        # Build prompt with context\n        context_str = '\\n'.join([f\"{m['agent']}: {m['message']}\" for m in conversation_context[-5:]])\n        \n        prompt = f'''You are {self.name}, a {self.role}.\nYour expertise: {', '.join(self.expertise)}\n\nConversation so far:\n{context_str}\n\nNew message: {message}\n\nYour response:'''\n        \n        response = self.llm(prompt)\n        self.contributions += 1\n        \n        return response\n    \n    def vote(self, proposal: str) -> Tuple[bool, str]:\n        '''Vote on a proposal'''\n        # In production, LLM would evaluate proposal\n        # For demo, random vote\n        import random\n        approve = random.random() > 0.3\n        reason = 'Looks good' if approve else 'Needs improvement'\n        return approve, reason\n\nclass GroupChatOrchestrator:\n    '''Orchestrate group chat between multiple agents'''\n    \n    def __init__(self, agents: List[AdvancedAgent], selection_mode='auto'):\n        self.agents = agents\n        self.selection_mode = selection_mode\n        self.conversation_log = []\n    \n    def select_next_speaker(self, current_message: str) -> AdvancedAgent:\n        '''Select which agent should respond next'''\n        \n        if self.selection_mode == 'round_robin':\n            # Each agent speaks in turn\n            return self.agents[len(self.conversation_log) % len(self.agents)]\n        \n        elif self.selection_mode == 'auto':\n            # Agent with relevant expertise responds\n            candidates = [a for a in self.agents if a.should_respond(current_message)]\n            \n            if candidates:\n                # Select agent with most contributions (balance participation)\n                return min(candidates, key=lambda a: a.contributions)\n            else:\n                # Default to first agent\n                return self.agents[0]\n        \n        elif self.selection_mode == 'all':\n            # All agents respond (parallel)\n            return None  # Special case\n    \n    def run_discussion(self, initial_task: str, max_turns=10) -> Dict[str, Any]:\n        '''Run multi-agent discussion'''\n        \n        current_message = initial_task\n        self.conversation_log = []\n        \n        for turn in range(max_turns):\n            # Select speaker\n            speaker = self.select_next_speaker(current_message)\n            \n            if speaker is None:  # All agents respond\n                responses = []\n                for agent in self.agents:\n                    response = agent.respond(current_message, self.conversation_log)\n                    responses.append({\n                        'agent': agent.name,\n                        'role': agent.role,\n                        'message': response\n                    })\n                    self.conversation_log.append(responses[-1])\n                \n                # Synthesize responses\n                current_message = self._synthesize_responses(responses)\n            \n            else:  # Single agent responds\n                response = speaker.respond(current_message, self.conversation_log)\n                \n                self.conversation_log.append({\n                    'turn': turn + 1,\n                    'agent': speaker.name,\n                    'role': speaker.role,\n                    'message': response\n                })\n                \n                current_message = response\n            \n            # Check for termination\n            if any(keyword in current_message.lower() for keyword in ['terminate', 'complete', 'done', 'finished']):\n                break\n        \n        return {\n            'conversation': self.conversation_log,\n            'turns': len(self.conversation_log),\n            'final_message': current_message\n        }\n    \n    def _synthesize_responses(self, responses: List[dict]) -> str:\n        '''Combine multiple agent responses'''\n        synthesis = 'Multiple perspectives:\\n'\n        for r in responses:\n            synthesis += f\"- {r['agent']}: {r['message'][:60]}...\\n\"\n        return synthesis\n\nclass ConsensusOrchestrator:\n    '''Reach consensus through voting'''\n    \n    def __init__(self, agents: List[AdvancedAgent]):\n        self.agents = agents\n    \n    def reach_consensus(self, proposal: str, required_majority: float = 0.6) -> Dict[str, Any]:\n        '''Get agent votes and determine consensus'''\n        votes = []\n        \n        print(f'\\nProposal: {proposal[:80]}...')\n        print('-' * 70)\n        \n        for agent in self.agents:\n            approve, reason = agent.vote(proposal)\n            votes.append({\n                'agent': agent.name,\n                'approve': approve,\n                'reason': reason\n            })\n            \n            vote_str = '\u2713 Approve' if approve else '\u2717 Reject'\n            print(f'{agent.name}: {vote_str} - {reason}')\n        \n        # Calculate results\n        approval_count = sum(1 for v in votes if v['approve'])\n        approval_rate = approval_count / len(votes)\n        \n        consensus_reached = approval_rate >= required_majority\n        \n        print(f'\\nVotes: {approval_count}/{len(votes)} ({approval_rate:.0%})')\n        print(f'Consensus: {\"REACHED\" if consensus_reached else \"NOT REACHED\"}')\n        \n        return {\n            'consensus': consensus_reached,\n            'approval_rate': approval_rate,\n            'votes': votes\n        }\n\n# Mock LLM for agents\ndef mock_agent_llm(prompt: str) -> str:\n    '''Mock LLM for agent responses'''\n    if 'architect' in prompt.lower():\n        return 'We should use microservices architecture for scalability.'\n    elif 'security' in prompt.lower():\n        return 'Add authentication and encryption at all layers.'\n    elif 'devops' in prompt.lower():\n        return 'Deploy using Kubernetes with auto-scaling.'\n    return 'I agree with the approach.'\n\n# Demo: Group chat\nprint('\\nMULTI-AGENT GROUP CHAT DEMONSTRATION')\nprint('=' * 90)\n\nagents = [\n    AdvancedAgent('Alice', 'System Architect', ['architecture', 'design', 'scalability'], mock_agent_llm),\n    AdvancedAgent('Bob', 'Security Engineer', ['security', 'authentication', 'encryption'], mock_agent_llm),\n    AdvancedAgent('Carol', 'DevOps Engineer', ['deployment', 'infrastructure', 'kubernetes'], mock_agent_llm),\n]\n\norchestrator = GroupChatOrchestrator(agents, selection_mode='auto')\n\ntask = 'Design a secure and scalable microservices architecture'\nresult = orchestrator.run_discussion(task, max_turns=6)\n\nprint(f'\\nTask: {task}')\nprint(f'Turns: {result[\"turns\"]}\\n')\nfor entry in result['conversation'][:5]:  # Show first 5\n    print(f\"Turn {entry.get('turn', '?')}: {entry['agent']} ({entry['role']})\")\n    print(f\"  {entry['message'][:80]}...\\n\")\n\nprint('\\n' + '=' * 90)\n\n# Demo: Consensus voting\nprint('\\nCONSENSUS VOTING DEMONSTRATION')\nprint('=' * 90)\n\nconsensus_orch = ConsensusOrchestrator(agents)\nproposal = 'Deploy the system using serverless functions instead of containers'\nresult = consensus_orch.reach_consensus(proposal, required_majority=0.67)\n\nif result['consensus']:\n    print('\\n\u2192 Proceeding with proposal')\nelse:\n    print('\\n\u2192 Proposal rejected, needs revision')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Interview Questions: AutoGen Production Systems\n\n### For Senior/Staff Engineers"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "autogen_interview_questions = [\n    {\n        'level': 'Senior',\n        'question': 'Your AutoGen code generation system has a 30% success rate on the first attempt, 60% after 2 retries, and 80% after 3 retries. However, each retry costs $0.05 and takes 3 seconds. A competitor offers 90% success but costs $0.30 per request. Which should you use and why?',\n        'answer': '''\n**Cost-Benefit Analysis:**\n\n**Option A: AutoGen with retries**\n- Attempt 1: 30% success @ $0.05 = $0.015 average\n- Attempt 2: 30% more succeed @ $0.10 = $0.030 average (of the 70% that try)\n- Attempt 3: 20% more succeed @ $0.15 = $0.030 average (of the 40% that try)\n\n**Expected cost calculation:**\n```python\ndef calculate_expected_cost():\n    cost_per_attempt = 0.05\n    \n    # Expected cost = sum(P(need N attempts) * cost_of_N_attempts)\n    expected_cost = (\n        0.30 * (1 * cost_per_attempt) +  # 30% succeed on attempt 1\n        0.30 * (2 * cost_per_attempt) +  # 30% succeed on attempt 2\n        0.20 * (3 * cost_per_attempt) +  # 20% succeed on attempt 3\n        0.20 * (3 * cost_per_attempt)    # 20% fail after 3 attempts\n    )\n    \n    return expected_cost\n\nexpected_cost = calculate_expected_cost()\nprint(f\"Expected cost per request: ${expected_cost:.3f}\")\n# Result: $0.105 per request\n\nexpected_latency = (\n    0.30 * 3 +    # 30% done in 3s\n    0.30 * 6 +    # 30% done in 6s\n    0.40 * 9      # 40% done in 9s (includes 20% failures)\n)\nprint(f\"Expected latency: {expected_latency:.1f}s\")\n# Result: 6.3s average\n```\n\n**Option B: Competitor**\n- Success rate: 90%\n- Cost: $0.30\n- Latency: ~3s (assumed)\n\n**Decision Matrix:**\n\n| Metric | AutoGen (3 retries) | Competitor | Winner |\n|--------|---------------------|------------|--------|\n| Success Rate | 80% | 90% | Competitor (+10%) |\n| Cost per success | $0.131 | $0.30 | AutoGen (58% cheaper) |\n| Latency (avg) | 6.3s | 3.0s | Competitor (52% faster) |\n| Latency (P95) | 9.0s | 3.0s | Competitor (67% faster) |\n\n**Recommendation depends on use case:**\n\n**Choose AutoGen IF:**\n- Cost is primary concern (high volume, tight budget)\n- Latency is acceptable (batch processing, async workflows)\n- 80% success rate meets requirements\n- Can implement result caching (improves effective success rate)\n\n**Choose Competitor IF:**\n- Latency critical (real-time applications)\n- 80% success rate too low (high-stakes domain)\n- User-facing (poor UX with retries)\n- Simpler ops (no retry logic needed)\n\n**Hybrid Solution (Best of Both):**\n```python\nclass HybridCodeGen:\n    '''Use AutoGen with fallback to competitor'''\n    \n    def __init__(self):\n        self.autogen = AutoGenSystem(max_retries=2)  # Try twice\n        self.competitor = CompetitorAPI()\n        self.cache = ResponseCache()\n    \n    async def generate(self, task: str) -> dict:\n        # Check cache first\n        cached = self.cache.get(task)\n        if cached:\n            return {'source': 'cache', 'result': cached, 'cost': 0, 'latency_ms': 5}\n        \n        # Try AutoGen first (cheaper)\n        start = time.time()\n        result = await self.autogen.generate(task)\n        \n        if result['status'] == 'success':\n            latency = (time.time() - start) * 1000\n            self.cache.set(task, result['code'])\n            return {\n                'source': 'autogen',\n                'result': result['code'],\n                'cost': result['cost'],\n                'latency_ms': latency\n            }\n        \n        # Fallback to competitor (more reliable but expensive)\n        print('AutoGen failed, falling back to competitor')\n        result = await self.competitor.generate(task)\n        \n        latency = (time.time() - start) * 1000\n        self.cache.set(task, result)\n        \n        return {\n            'source': 'competitor',\n            'result': result,\n            'cost': 0.30,\n            'latency_ms': latency\n        }\n\n# Expected metrics with hybrid:\nhybrid_metrics = {\n    'success_rate': 0.80 + (0.20 * 0.90),  # 80% from AutoGen + 90% of remaining 20%\n    # = 98% total\n    \n    'cost_per_request': 0.80 * 0.105 + 0.20 * 0.30,  # Weighted average\n    # = $0.144 (52% cheaper than competitor, slightly more than pure AutoGen)\n    \n    'latency_avg': 0.80 * 6.3 + 0.20 * 9.0,  # AutoGen latency + fallback\n    # = 6.84s\n}\n\nprint('\\nCOST-BENEFIT ANALYSIS SUMMARY:')\nprint('=' * 80)\nprint(f\"Pure AutoGen: 80% success, $0.105 avg, 6.3s latency\")\nprint(f\"Competitor:   90% success, $0.300 avg, 3.0s latency\")\nprint(f\"Hybrid:       98% success, $0.144 avg, 6.8s latency\")\nprint('=' * 80)\nprint('\\nRECOMMENDATION: Hybrid approach')\nprint('  - Best success rate (98%)')\nprint('  - Reasonable cost ($0.144)')\nprint('  - Acceptable latency for most use cases')\nprint('  - Can add caching for further improvement')\n```\n        ''',\n    },\n    {\n        'level': 'Staff',\n        'question': 'Design a multi-agent system for automated code review that checks style, security, performance, and tests. Include consensus mechanisms, escalation logic, and integration with CI/CD.',\n        'answer': '''\n**Automated Code Review Multi-Agent System:**\n\n**1. Agent Team Structure:**\n```python\n@dataclass\nclass ReviewAgent:\n    name: str\n    role: str\n    focus_areas: List[str]\n    severity_thresholds: Dict[str, int]  # What issues require human escalation\n\n# Define specialist agents\nagents = [\n    ReviewAgent(\n        name='StyleBot',\n        role='Code Style Reviewer',\n        focus_areas=['formatting', 'naming', 'documentation'],\n        severity_thresholds={'critical': 0, 'high': 5, 'medium': 10}\n    ),\n    ReviewAgent(\n        name='SecurityBot',\n        role='Security Analyst',\n        focus_areas=['vulnerabilities', 'injection', 'authentication', 'secrets'],\n        severity_thresholds={'critical': 0, 'high': 1, 'medium': 3}\n    ),\n    ReviewAgent(\n        name='PerformanceBot',\n        role='Performance Specialist',\n        focus_areas=['complexity', 'memory', 'database', 'caching'],\n        severity_thresholds={'critical': 0, 'high': 2, 'medium': 5}\n    ),\n    ReviewAgent(\n        name='TestBot',\n        role='Test Coverage Analyst',\n        focus_areas=['coverage', 'edge_cases', 'integration_tests'],\n        severity_thresholds={'critical': 0, 'high': 3, 'medium': 7}\n    ),\n]\n```\n\n**2. Review Orchestration:**\n```python\nfrom typing import List, Dict\nfrom enum import Enum\n\nclass ReviewSeverity(Enum):\n    CRITICAL = 'critical'\n    HIGH = 'high'\n    MEDIUM = 'medium'\n    LOW = 'low'\n    INFO = 'info'\n\nclass ReviewFinding:\n    def __init__(self, agent: str, severity: ReviewSeverity, message: str, \n                 file: str, line: int, suggestion: str = None):\n        self.agent = agent\n        self.severity = severity\n        self.message = message\n        self.file = file\n        self.line = line\n        self.suggestion = suggestion\n\nclass CodeReviewOrchestrator:\n    '''Orchestrate multi-agent code review'''\n    \n    def __init__(self, agents: List[ReviewAgent]):\n        self.agents = agents\n        self.executor = CodeExecutor()\n    \n    async def review_pr(self, pr_data: dict) -> dict:\n        '''Review pull request with all agents in parallel'''\n        \n        # Step 1: Parallel review by all agents\n        review_tasks = [\n            self._agent_review(agent, pr_data)\n            for agent in self.agents\n        ]\n        \n        agent_findings = await asyncio.gather(*review_tasks)\n        \n        # Step 2: Aggregate findings\n        all_findings = [f for findings in agent_findings for f in findings]\n        \n        # Step 3: Detect conflicts between agents\n        conflicts = self._detect_conflicts(agent_findings)\n        \n        # Step 4: Consensus on approval\n        decision = await self._make_decision(all_findings, conflicts)\n        \n        # Step 5: Check escalation\n        needs_human = self._check_escalation(all_findings, conflicts)\n        \n        return {\n            'pr_id': pr_data['pr_id'],\n            'decision': decision,\n            'findings': all_findings,\n            'conflicts': conflicts,\n            'needs_human_review': needs_human,\n            'review_time_ms': 0  # Track actual time\n        }\n    \n    async def _agent_review(self, agent: ReviewAgent, pr_data: dict) -> List[ReviewFinding]:\n        '''Single agent performs review'''\n        findings = []\n        \n        # Run agent-specific checks\n        if agent.role == 'Security Analyst':\n            # Security checks\n            security_findings = await self._run_security_analysis(pr_data)\n            findings.extend(security_findings)\n        \n        elif agent.role == 'Performance Specialist':\n            # Performance checks\n            perf_findings = await self._run_performance_analysis(pr_data)\n            findings.extend(perf_findings)\n        \n        # ... other agents\n        \n        return findings\n    \n    async def _run_security_analysis(self, pr_data: dict) -> List[ReviewFinding]:\n        '''Run security analysis'''\n        findings = []\n        \n        # Static analysis\n        code = pr_data['diff']\n        \n        # Check for common vulnerabilities\n        if 'eval(' in code:\n            findings.append(ReviewFinding(\n                agent='SecurityBot',\n                severity=ReviewSeverity.CRITICAL,\n                message='Use of eval() detected - code injection risk',\n                file=pr_data['file'],\n                line=0,\n                suggestion='Use ast.literal_eval() or json.loads() instead'\n            ))\n        \n        if 'password' in code.lower() and '=' in code:\n            findings.append(ReviewFinding(\n                agent='SecurityBot',\n                severity=ReviewSeverity.HIGH,\n                message='Possible hardcoded password detected',\n                file=pr_data['file'],\n                line=0,\n                suggestion='Use environment variables or secrets manager'\n            ))\n        \n        return findings\n    \n    def _detect_conflicts(self, agent_findings: List[List[ReviewFinding]]) -> List[dict]:\n        '''Detect conflicting recommendations'''\n        # E.g., Performance says \"cache everything\", Security says \"don't cache PII\"\n        conflicts = []\n        \n        # Simplified conflict detection\n        for i, findings_a in enumerate(agent_findings):\n            for j, findings_b in enumerate(agent_findings[i+1:], i+1):\n                # Check if recommendations contradict\n                # (In production, use NLP to detect semantic conflicts)\n                pass\n        \n        return conflicts\n    \n    async def _make_decision(self, findings: List[ReviewFinding], conflicts: List[dict]) -> str:\n        '''Decide: approve, reject, or request changes'''\n        \n        # Count by severity\n        severity_counts = defaultdict(int)\n        for f in findings:\n            severity_counts[f.severity] += 1\n        \n        # Decision logic\n        if severity_counts[ReviewSeverity.CRITICAL] > 0:\n            return 'reject'  # Any critical issue = reject\n        \n        if severity_counts[ReviewSeverity.HIGH] > 3:\n            return 'reject'  # Too many high-severity issues\n        \n        if len(conflicts) > 0:\n            return 'request_changes'  # Conflicts need resolution\n        \n        if severity_counts[ReviewSeverity.HIGH] > 0 or severity_counts[ReviewSeverity.MEDIUM] > 5:\n            return 'request_changes'\n        \n        return 'approve'\n    \n    def _check_escalation(self, findings: List[ReviewFinding], conflicts: List[dict]) -> bool:\n        '''Determine if human review required'''\n        \n        # Escalate if:\n        # 1. Any critical security issue\n        if any(f.severity == ReviewSeverity.CRITICAL and 'Security' in f.agent for f in findings):\n            return True\n        \n        # 2. Conflicting recommendations from agents\n        if len(conflicts) > 2:\n            return True\n        \n        # 3. Unusual patterns (ML-based anomaly detection)\n        # if self.anomaly_detector.is_anomalous(pr_data):\n        #     return True\n        \n        return False\n```\n\n**3. Integration with CI/CD:**\n```python\nclass CICDIntegration:\n    '''Integrate with GitHub/GitLab CI/CD pipeline'''\n    \n    def __init__(self, code_review_system: CodeReviewOrchestrator):\n        self.reviewer = code_review_system\n    \n    async def on_pr_created(self, pr_webhook: dict):\n        '''Handle PR creation webhook'''\n        pr_id = pr_webhook['pr_id']\n        \n        # Start review (async)\n        review_task = asyncio.create_task(\n            self.reviewer.review_pr(pr_webhook)\n        )\n        \n        # Post initial comment\n        await self.post_comment(pr_id, 'Code review in progress...')\n        \n        # Wait for review\n        review_result = await review_task\n        \n        # Post findings\n        await self.post_review_results(pr_id, review_result)\n        \n        # Update PR status\n        if review_result['needs_human_review']:\n            await self.request_human_review(pr_id)\n        elif review_result['decision'] == 'approve':\n            await self.approve_pr(pr_id)\n        else:\n            await self.request_changes(pr_id, review_result['findings'])\n    \n    async def post_review_results(self, pr_id: str, review: dict):\n        '''Format and post review as PR comment'''\n        \n        # Group findings by severity\n        by_severity = defaultdict(list)\n        for finding in review['findings']:\n            by_severity[finding.severity.value].append(finding)\n        \n        # Build markdown comment\n        comment = f'''## Automated Code Review Results\n\n**Decision: {review['decision'].upper()}**\n\n'''        \n        for severity in ['critical', 'high', 'medium', 'low']:\n            if severity in by_severity:\n                icon = {'critical': '\ud83d\udea8', 'high': '\u26a0\ufe0f', 'medium': '\ud83d\udca1', 'low': '\u2139\ufe0f'}[severity]\n                comment += f'''\\n### {icon} {severity.capitalize()} Issues ({len(by_severity[severity])})\n\n'''\n                \n                for finding in by_severity[severity][:10]:  # Show top 10\n                    comment += f'''**{finding.agent}**: {finding.message}\n'''\n                    if finding.suggestion:\n                        comment += f'''  *Suggestion:* {finding.suggestion}\n'''\n                    comment += '\\n'\n        \n        if review['needs_human_review']:\n            comment += '''\\n---\n**\u26a0\ufe0f Human review required** due to critical issues or conflicts.\n'''\n        \n        # Post to GitHub API\n        await github_api.post_comment(pr_id, comment)\n```\n\n**4. Caching and Performance:**\n```python\nclass IntelligentCache:\n    '''Cache code review results'''\n    \n    def __init__(self):\n        self.cache = {}\n        self.hit_count = 0\n        self.miss_count = 0\n    \n    def get_cache_key(self, pr_data: dict) -> str:\n        '''Generate cache key from code hash'''\n        import hashlib\n        \n        # Hash the diff\n        diff_hash = hashlib.sha256(pr_data['diff'].encode()).hexdigest()\n        \n        # Include relevant metadata\n        key = f\"{diff_hash}_{pr_data['language']}_{pr_data['framework']}\"\n        \n        return key\n    \n    def get(self, pr_data: dict) -> Optional[dict]:\n        '''Get cached review if available'''\n        key = self.get_cache_key(pr_data)\n        \n        if key in self.cache:\n            self.hit_count += 1\n            return self.cache[key]\n        \n        self.miss_count += 1\n        return None\n    \n    def set(self, pr_data: dict, review_result: dict):\n        '''Cache review result'''\n        key = self.get_cache_key(pr_data)\n        self.cache[key] = review_result\n    \n    def get_hit_rate(self) -> float:\n        total = self.hit_count + self.miss_count\n        return self.hit_count / total if total > 0 else 0\n```\n\n**5. Production Metrics:**\n```python\nclass ReviewMetrics:\n    '''Track review system performance'''\n    \n    def __init__(self):\n        self.metrics = {\n            'reviews_completed': 0,\n            'auto_approved': 0,\n            'requested_changes': 0,\n            'rejected': 0,\n            'escalated_to_human': 0,\n            'avg_review_time_ms': [],\n            'findings_by_severity': defaultdict(int),\n        }\n    \n    def record_review(self, review_result: dict):\n        self.metrics['reviews_completed'] += 1\n        \n        if review_result['decision'] == 'approve':\n            self.metrics['auto_approved'] += 1\n        elif review_result['decision'] == 'reject':\n            self.metrics['rejected'] += 1\n        else:\n            self.metrics['requested_changes'] += 1\n        \n        if review_result['needs_human_review']:\n            self.metrics['escalated_to_human'] += 1\n        \n        # Track findings\n        for finding in review_result['findings']:\n            self.metrics['findings_by_severity'][finding.severity.value] += 1\n    \n    def get_summary(self) -> dict:\n        return {\n            'total_reviews': self.metrics['reviews_completed'],\n            'auto_approval_rate': self.metrics['auto_approved'] / self.metrics['reviews_completed'] if self.metrics['reviews_completed'] > 0 else 0,\n            'escalation_rate': self.metrics['escalated_to_human'] / self.metrics['reviews_completed'] if self.metrics['reviews_completed'] > 0 else 0,\n            'avg_review_time_sec': np.mean(self.metrics['avg_review_time_ms']) / 1000 if self.metrics['avg_review_time_ms'] else 0,\n            'findings_by_severity': dict(self.metrics['findings_by_severity']),\n        }\n```\n\n**6. Complete System Flow:**\n```\n1. PR Created \u2192 Webhook\n2. Extract diff, files changed, language\n3. Parallel agent reviews (4 agents \u00d7 10s = 10s total)\n4. Aggregate findings\n5. Detect conflicts (2s)\n6. Make decision (1s)\n7. Check escalation criteria\n8. Post results to PR (1s)\n9. If approved \u2192 merge, else \u2192 block\n\nTotal time: ~15s for complete review\n```\n\n**7. Consensus Mechanisms:**\n```python\nclass ReviewConsensus:\n    '''Handle conflicts between agents'''\n    \n    def resolve_conflict(self, conflict: dict) -> dict:\n        '''Resolve conflicting recommendations'''\n        \n        agent_a = conflict['agent_a']\n        agent_b = conflict['agent_b']\n        issue = conflict['issue']\n        \n        # Priority rules\n        priority = {\n            'SecurityBot': 10,  # Security has highest priority\n            'PerformanceBot': 7,\n            'TestBot': 5,\n            'StyleBot': 3,\n        }\n        \n        # Higher priority agent wins\n        if priority[agent_a['name']] > priority[agent_b['name']]:\n            return {'winner': agent_a, 'reason': 'higher_priority'}\n        else:\n            return {'winner': agent_b, 'reason': 'higher_priority'}\n```\n\n**Expected Results:**\n- **Review time**: 10-20 seconds (vs. hours for human)\n- **Accuracy**: 85-90% (catches most common issues)\n- **False positive rate**: 10-15% (acceptable with human escalation)\n- **Cost**: $0.10-0.20 per review\n- **Developer satisfaction**: High (fast feedback, fewer bike-shedding comments)\n\n**Production Deployment:**\n```yaml\n# .github/workflows/ai-code-review.yml\nname: AI Code Review\n\non:\n  pull_request:\n    types: [opened, synchronize]\n\njobs:\n  ai-review:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n      \n      - name: Run AI Code Review\n        run: |\n          python ai_review_system.py \\\\\n            --pr-id ${{ github.event.pull_request.number }} \\\\\n            --base ${{ github.event.pull_request.base.sha }} \\\\\n            --head ${{ github.event.pull_request.head.sha }}\n      \n      - name: Post Results\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const fs = require('fs');\n            const review = JSON.parse(fs.readFileSync('review_result.json'));\n            \n            // Post review comment\n            await github.rest.issues.createComment({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              issue_number: context.issue.number,\n              body: review.formatted_comment\n            });\n            \n            // Update PR status\n            if (review.needs_human_review) {\n              await github.rest.pulls.requestReviewers({\n                pull_number: context.issue.number,\n                reviewers: ['senior-engineer']\n              });\n            }\n```\n\n**Key Benefits:**\n1. **Fast feedback**: 15s vs. hours/days\n2. **Consistent**: Same criteria every time\n3. **Comprehensive**: Checks style, security, performance, tests in parallel\n4. **Cost-effective**: $0.15 per review vs. $50 human-hour\n5. **Scalable**: Handle 1000s of PRs/day\n6. **Learning**: Improves over time with feedback\n        ''',\n    },\n]\n\nfor i, qa in enumerate(autogen_interview_questions, 1):\n    print(f'\\n{'=' * 100}')\n    print(f'Q{i} [{qa[\"level\"]} Level]')\n    print('=' * 100)\n    print(f'\\n{qa[\"question\"]}\\n')\n    print('ANSWER:')\n    print(qa['answer'])\n    print()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Module 5 Summary and Key Takeaways\n\n### AutoGen for Production\n\nAutoGen is powerful for conversational, iterative workflows, especially code generation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('MODULE 5: AUTOGEN - KEY TAKEAWAYS')\nprint('=' * 100)\n\nsummary = {\n    '1. Core Strengths': [\n        'Conversational multi-agent coordination',\n        'Code generation with automatic error correction',\n        'Iterative refinement through feedback loops',\n        '70-80% success rate with 3 retries',\n        'Best for: coding tasks, data analysis, test generation',\n    ],\n    '2. Code Execution Safety': [\n        'Never execute arbitrary code without validation',\n        'Use containerization (Docker) for true isolation',\n        'Set resource limits: memory, CPU, time, network',\n        'Allowlist imports, block dangerous operations (eval, exec, os, subprocess)',\n        'Audit log all execution attempts',\n        'Implement idempotency checks for retries',\n    ],\n    '3. Error Correction': [\n        'Success rates: 30% \u2192 60% \u2192 80% with 3 attempts',\n        'Cost: $0.05 per attempt (manage budget with max_retries)',\n        'Latency: 3s per attempt (use async for better UX)',\n        'Hybrid approach: AutoGen + fallback = 98% success',\n        'Cache successful code for common patterns',\n    ],\n    '4. Multi-Agent Patterns': [\n        'Sequential: Coder \u2192 Executor \u2192 Reviewer (single workflow)',\n        'Parallel: Multiple specialists analyze simultaneously',\n        'Group Chat: Dynamic selection based on expertise',\n        'Consensus: Voting mechanism for decisions',\n        'Hierarchical: Manager delegates to specialists',\n    ],\n    '5. Production Considerations': [\n        'Termination conditions: max turns, success signal, timeout',\n        'Agent selection: round-robin, expertise-based, dynamic',\n        'Conversation memory: Limited to recent turns (token budget)',\n        'Cost control: Monitor token usage per conversation',\n        'Human escalation: For conflicts, critical issues, or max iterations',\n    ],\n    'Production Checklist': [\n        '[x] Secure code execution sandbox',\n        '[x] Resource limits (CPU, memory, time)',\n        '[x] Error correction loop with max iterations',\n        '[x] Audit logging for all executions',\n        '[x] Consensus mechanisms for multi-agent decisions',\n        '[x] Human escalation for edge cases',\n        '[x] Performance monitoring (latency, success rate, cost)',\n        '[x] CI/CD integration for automated workflows',\n    ],\n}\n\nfor section, points in summary.items():\n    print(f'\\n{section}:')\n    for point in points:\n        print(f'  - {point}')\n\nprint('\\n' + '=' * 100)\nprint('\\nCOMPARISON: AutoGen vs. Other Frameworks')\nprint('=' * 100)\n\ncomparison = {\n    'AutoGen': {\n        'Best for': 'Code generation, iterative refinement',\n        'Strength': 'Error correction loop, execution feedback',\n        'Weakness': 'Limited to conversational pattern',\n        'When to use': 'Coding tasks, data analysis, test generation',\n    },\n    'LangGraph': {\n        'Best for': 'Complex workflows with state',\n        'Strength': 'Flexible routing, checkpointing, HITL',\n        'Weakness': 'More complex to set up',\n        'When to use': 'Business processes, multi-step reasoning',\n    },\n    'CrewAI': {\n        'Best for': 'Role-based task delegation',\n        'Strength': 'Clear role separation, sequential/hierarchical',\n        'Weakness': 'Less flexible than LangGraph',\n        'When to use': 'Content creation, research workflows',\n    },\n}\n\nfor framework, details in comparison.items():\n    print(f'\\n{framework}:')\n    for key, value in details.items():\n        print(f'  {key}: {value}')\n\nprint('\\n' + '=' * 100)\nprint('\\nNEXT STEPS:')\nprint('  1. Implement safe code execution sandbox')\nprint('  2. Build multi-agent system for your use case')\nprint('  3. Set up monitoring and cost tracking')\nprint('  4. Test error correction loop on real tasks')\nprint('  5. Move to Module 6: CrewAI (role-based orchestration)')\nprint('\\n' + '=' * 100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 5: Production AutoGen Patterns\n\nKey patterns for production:\n- **Termination conditions**: Clear success/failure criteria\n- **Cost control**: Token budgets and limits\n- **Conversation pruning**: Manage context length\n- **Agent specialization**: Each agent has clear role\n- **Output validation**: Ensure quality before returning"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import tiktoken\nfrom typing import List, Dict, Optional\n\nclass ProductionAutoGenSystem:\n    '''Production-ready AutoGen with cost and quality controls'''\n    \n    def __init__(self, max_tokens_per_conversation=4000, max_cost_per_task=1.0):\n        self.max_tokens = max_tokens_per_conversation\n        self.max_cost = max_cost_per_task\n        self.encoding = tiktoken.encoding_for_model('gpt-4')\n        self.agents = {}\n        self.conversation_history = []\n        self.total_cost = 0.0\n        self.total_tokens = 0\n    \n    def add_agent(self, agent):\n        '''Add agent to system'''\n        self.agents[agent.name] = agent\n    \n    def run_task(self, task: str, max_turns=10) -> dict:\n        '''Execute task with cost and token controls'''\n        self.conversation_history = [{'role': 'user', 'content': task}]\n        self.total_cost = 0.0\n        self.total_tokens = 0\n        \n        for turn in range(max_turns):\n            # Check termination conditions\n            if self._should_terminate():\n                return self._build_result('terminated', turn)\n            \n            # Check budget\n            if self.total_cost >= self.max_cost:\n                return self._build_result('budget_exceeded', turn)\n            \n            if self.total_tokens >= self.max_tokens:\n                return self._build_result('token_limit_exceeded', turn)\n            \n            # Select next agent\n            agent = self._select_agent()\n            \n            # Generate response\n            response = agent.respond(self.conversation_history[-1]['content'], self.conversation_history)\n            \n            # Track costs\n            tokens = self._count_tokens(response)\n            cost = self._calculate_cost(tokens)\n            self.total_tokens += tokens\n            self.total_cost += cost\n            \n            # Add to history\n            self.conversation_history.append({\n                'role': 'assistant',\n                'agent': agent.name,\n                'content': response,\n                'tokens': tokens,\n                'cost': cost\n            })\n            \n            # Check if task complete\n            if self._task_complete(response):\n                return self._build_result('success', turn + 1)\n            \n            # Prune conversation if too long\n            self._prune_conversation()\n        \n        return self._build_result('max_turns_reached', max_turns)\n    \n    def _count_tokens(self, text: str) -> int:\n        '''Count tokens in text'''\n        return len(self.encoding.encode(text))\n    \n    def _calculate_cost(self, tokens: int, model='gpt-4') -> float:\n        '''Calculate cost for tokens'''\n        # GPT-4: $0.03 input + $0.06 output per 1K tokens\n        return tokens * 0.00003  # Simplified\n    \n    def _should_terminate(self) -> bool:\n        '''Check if conversation should terminate'''\n        if not self.conversation_history:\n            return False\n        \n        last_message = self.conversation_history[-1]['content'].lower()\n        \n        # Termination keywords\n        termination_signals = [\n            'terminate',\n            'task complete',\n            'finished',\n            'done',\n            'no further action needed'\n        ]\n        \n        return any(signal in last_message for signal in termination_signals)\n    \n    def _task_complete(self, response: str) -> bool:\n        '''Check if task is successfully completed'''\n        # In production, use more sophisticated completion detection\n        completion_signals = ['complete', 'finished', 'done', 'success']\n        return any(signal in response.lower() for signal in completion_signals)\n    \n    def _prune_conversation(self):\n        '''Prune old messages to stay within token budget'''\n        total_tokens = sum(self._count_tokens(m['content']) for m in self.conversation_history)\n        \n        while total_tokens > self.max_tokens and len(self.conversation_history) > 3:\n            # Keep first message (task) and last 2 messages\n            # Remove second-oldest message\n            removed = self.conversation_history.pop(1)\n            total_tokens -= self._count_tokens(removed['content'])\n    \n    def _select_agent(self):\n        '''Select next agent (simplified)'''\n        # In production, use sophisticated selection\n        return list(self.agents.values())[0]\n    \n    def _build_result(self, status: str, turns: int) -> dict:\n        '''Build final result'''\n        return {\n            'status': status,\n            'turns': turns,\n            'total_cost': self.total_cost,\n            'total_tokens': self.total_tokens,\n            'conversation': self.conversation_history,\n            'final_response': self.conversation_history[-1]['content'] if self.conversation_history else None\n        }\n    \n    def get_execution_report(self) -> str:\n        '''Generate detailed execution report'''\n        report = f'''\\nExecution Report: {self.workflow_name}\n{'=' * 80}\n\nConversation Statistics:\n  Total turns: {len(self.conversation_history)}\n  Total tokens: {self.total_tokens}\n  Total cost: ${self.total_cost:.4f}\n  Avg tokens/turn: {self.total_tokens / len(self.conversation_history) if self.conversation_history else 0:.0f}\n\nAgent Participation:\n'''\n        \n        agent_turns = defaultdict(int)\n        for msg in self.conversation_history:\n            if 'agent' in msg:\n                agent_turns[msg['agent']] += 1\n        \n        for agent, turns in agent_turns.items():\n            report += f'  {agent}: {turns} turns\\n'\n        \n        report += '=' * 80\n        \n        return report\n\n# Demo\nprint('PRODUCTION AUTOGEN WITH COST CONTROLS')\nprint('=' * 90)\n\nclass MockAgent:\n    def __init__(self, name):\n        self.name = name\n    \n    def respond(self, message, history):\n        # Mock response\n        if len(history) >= 5:\n            return f'{self.name}: Task complete!'\n        return f'{self.name}: Working on it... (turn {len(history)})'\n\nsystem = ProductionAutoGenSystem(max_tokens_per_conversation=2000, max_cost_per_task=0.50)\nsystem.add_agent(MockAgent('Assistant'))\n\ntask = 'Analyze this dataset and generate insights'\nresult = system.run_task(task, max_turns=6)\n\nprint(f'\\nStatus: {result[\"status\"]}')\nprint(f'Turns: {result[\"turns\"]}')\nprint(f'Total cost: ${result[\"total_cost\"]:.4f}')\nprint(f'Total tokens: {result[\"total_tokens\"]}')\n\nprint(system.get_execution_report())\n\nprint('\\n' + '=' * 90)\nprint('KEY PRODUCTION CONTROLS:')\nprint('  - Token budgets prevent runaway costs')\nprint('  - Cost tracking per conversation')\nprint('  - Automatic conversation pruning')\nprint('  - Clear termination conditions')\nprint('  - Execution reports for debugging')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## AutoGen Framework - Core Components\n\n### Based on 8-Hour Curriculum\n\nThis section covers all core AutoGen components with production-ready implementations:\n- AssistantAgent\n- UserProxyAgent\n- CodeExecutionAgent\n- Role definition & specialization\n- LLM integration\n- Tools and functions\n- Multi-agent orchestration\n- Memory management\n- Output parsing"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 1: AssistantAgent\n\nThe AssistantAgent is an LLM-powered agent that generates responses, plans actions, and uses tools."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List, Dict, Any, Optional, Callable\nimport json\n\nclass AssistantAgent:\n    '''\n    AssistantAgent: LLM-powered agent for reasoning and response generation.\n    \n    Key capabilities:\n    - Generate responses using LLM\n    - Use tools and functions\n    - Maintain conversation context\n    - Support role specialization\n    '''\n    \n    def __init__(self,\n                 name: str,\n                 system_message: str,\n                 llm_config: dict,\n                 function_map: Optional[Dict[str, Callable]] = None):\n        self.name = name\n        self.system_message = system_message\n        self.llm_config = llm_config\n        self.function_map = function_map or {}\n        self.conversation_history = []\n        \n        # Add system message to history\n        self.conversation_history.append({\n            'role': 'system',\n            'content': system_message\n        })\n    \n    def generate_reply(self, messages: List[Dict], context: Dict = None) -> str:\n        '''\n        Generate reply using LLM.\n        \n        In production, this would call actual LLM API.\n        For demonstration, we simulate responses.\n        '''\n        \n        last_message = messages[-1]['content'] if messages else ''\n        \n        # Build prompt with conversation history\n        prompt = self._build_prompt(messages)\n        \n        # Call LLM (simulated)\n        response = self._call_llm(prompt)\n        \n        # Check if function call needed\n        if self._requires_function_call(response):\n            function_result = self._execute_function(response)\n            # Incorporate function result\n            response = self._incorporate_function_result(response, function_result)\n        \n        return response\n    \n    def _build_prompt(self, messages: List[Dict]) -> str:\n        '''Build prompt from conversation history'''\n        prompt_parts = [self.system_message]\n        \n        for msg in messages[-5:]:  # Last 5 messages for context\n            role = msg['role']\n            content = msg['content']\n            prompt_parts.append(f'{role}: {content}')\n        \n        return '\\n'.join(prompt_parts)\n    \n    def _call_llm(self, prompt: str) -> str:\n        '''\n        Call LLM API.\n        \n        In production:\n        import openai\n        response = openai.ChatCompletion.create(\n            model=self.llm_config['model'],\n            messages=[{'role': 'user', 'content': prompt}],\n            temperature=self.llm_config.get('temperature', 0.7)\n        )\n        return response.choices[0].message.content\n        '''\n        # Mock response\n        if 'code' in prompt.lower():\n            return 'I will write code to solve this. Let me use the code_executor function.'\n        return f'{self.name} responding to the request.'\n    \n    def _requires_function_call(self, response: str) -> bool:\n        '''Check if response indicates function call'''\n        function_indicators = ['function:', 'call function', 'use function', 'execute:']\n        return any(indicator in response.lower() for indicator in function_indicators)\n    \n    def _execute_function(self, response: str) -> Any:\n        '''Execute function if mentioned in response'''\n        # Parse function call from response\n        # In production, use structured function calling\n        for func_name in self.function_map.keys():\n            if func_name in response.lower():\n                return self.function_map[func_name]()\n        return None\n    \n    def _incorporate_function_result(self, response: str, function_result: Any) -> str:\n        '''Incorporate function result into response'''\n        return f\"{response}\\nFunction result: {function_result}\"\n\n# Example AssistantAgent\nprint('ASSISTANTAGENT DEMONSTRATION')\nprint('=' * 90)\n\nassistant = AssistantAgent(\n    name='CodingAssistant',\n    system_message='You are a helpful coding assistant. You write clean, well-documented code.',\n    llm_config={'model': 'gpt-4', 'temperature': 0.3},\n    function_map={\n        'execute_code': lambda: 'Code executed successfully',\n        'search_docs': lambda: 'Found relevant documentation',\n    }\n)\n\ntest_messages = [\n    {'role': 'user', 'content': 'Write a function to calculate fibonacci numbers'}\n]\n\nresponse = assistant.generate_reply(test_messages)\nprint(f'User: {test_messages[0][\"content\"]}')\nprint(f'{assistant.name}: {response}')\n\nprint('\\n' + '=' * 90)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 2: UserProxyAgent\n\nUserProxyAgent represents the user and can execute code on their behalf."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class UserProxyAgent:\n    '''\n    UserProxyAgent: Represents user and executes code.\n    \n    Key capabilities:\n    - Proxy for human user\n    - Execute code in safe environment\n    - Collect user input\n    - Provide execution feedback\n    - Terminate conversations\n    '''\n    \n    def __init__(self,\n                 name: str,\n                 code_execution_config: dict,\n                 human_input_mode: str = 'NEVER',\n                 max_consecutive_auto_reply: int = 10):\n        self.name = name\n        self.code_execution_config = code_execution_config\n        self.human_input_mode = human_input_mode  # 'ALWAYS', 'NEVER', 'TERMINATE'\n        self.max_consecutive_auto_reply = max_consecutive_auto_reply\n        self.auto_reply_count = 0\n        self.executor = self._init_executor()\n    \n    def _init_executor(self):\n        '''Initialize code executor'''\n        # Use SafeCodeExecutor from earlier\n        return SafeCodeExecutor(\n            timeout_seconds=self.code_execution_config.get('timeout', 60),\n            max_memory_mb=self.code_execution_config.get('max_memory_mb', 512)\n        )\n    \n    def generate_reply(self, messages: List[Dict], sender) -> Optional[str]:\n        '''\n        Generate reply (execute code or get user input).\n        '''\n        \n        # Check if max auto-replies reached\n        if self.auto_reply_count >= self.max_consecutive_auto_reply:\n            return self._get_human_input('Max auto-replies reached. Your input:')\n        \n        last_message = messages[-1]['content']\n        \n        # Check if code execution requested\n        if self._contains_code(last_message):\n            code = self._extract_code(last_message)\n            execution_result = self.executor.execute(code)\n            \n            self.auto_reply_count += 1\n            \n            if execution_result['status'] == 'success':\n                return f\"Execution successful:\\n{execution_result['output']}\"\n            else:\n                return f\"Execution failed:\\n{execution_result['error']}\"\n        \n        # Check if human input needed\n        if self.human_input_mode == 'ALWAYS':\n            return self._get_human_input('Your input:')\n        \n        # Check for termination\n        if self._should_terminate(last_message):\n            return 'TERMINATE'\n        \n        self.auto_reply_count += 1\n        return None  # No reply needed\n    \n    def _contains_code(self, message: str) -> bool:\n        '''Check if message contains code to execute'''\n        return '```python' in message or '```' in message\n    \n    def _extract_code(self, message: str) -> str:\n        '''Extract code from message'''\n        # Extract from markdown code blocks\n        if '```python' in message:\n            code = message.split('```python')[1].split('```')[0]\n        elif '```' in message:\n            code = message.split('```')[1].split('```')[0]\n        else:\n            code = message\n        \n        return code.strip()\n    \n    def _should_terminate(self, message: str) -> bool:\n        '''Check if conversation should terminate'''\n        termination_keywords = ['TERMINATE', 'task complete', 'finished', 'done with task']\n        return any(kw.lower() in message.lower() for kw in termination_keywords)\n    \n    def _get_human_input(self, prompt: str) -> str:\n        '''Get input from human (simulated)'''\n        # In production: input() or UI integration\n        return f'[HUMAN INPUT: {prompt}]'\n    \n    def reset_auto_reply_count(self):\n        '''Reset auto-reply counter'''\n        self.auto_reply_count = 0\n\n# Example UserProxyAgent\nprint('USERPROXYAGENT DEMONSTRATION')\nprint('=' * 90)\n\nuser_proxy = UserProxyAgent(\n    name='UserProxy',\n    code_execution_config={\n        'timeout': 60,\n        'work_dir': './workspace',\n        'use_docker': False,  # Set True in production\n    },\n    human_input_mode='NEVER',\n    max_consecutive_auto_reply=5\n)\n\n# Test code execution\ncode_message = '''```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))\n```'''\n\ntest_messages = [{'role': 'assistant', 'content': code_message}]\nresponse = user_proxy.generate_reply(test_messages, sender='assistant')\n\nprint(f'Assistant sent code:\\n{code_message[:80]}...')\nprint(f'\\n{user_proxy.name} response:\\n{response[:150]}...')\n\nprint('\\n' + '=' * 90)\nprint('KEY USERPROXYAGENT FEATURES:')\nprint('  - Executes code automatically')\nprint('  - Can require human input at decision points')\nprint('  - Terminates conversations based on keywords')\nprint('  - Limits auto-replies to prevent infinite loops')\nprint('  - Provides execution feedback to other agents')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 3: CodeExecutionAgent\n\nSpecialized agent for code execution with advanced sandboxing and analysis."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CodeExecutionAgent:\n    '''\n    CodeExecutionAgent: Specialized for code execution and analysis.\n    \n    Features:\n    - Execute code safely in sandbox\n    - Analyze code for issues (security, performance)\n    - Provide detailed execution reports\n    - Handle runtime errors\n    - Test code before execution\n    '''\n    \n    def __init__(self, name: str, work_dir: str = './workspace', use_docker: bool = True):\n        self.name = name\n        self.work_dir = work_dir\n        self.use_docker = use_docker\n        self.executor = SafeCodeExecutor(timeout_seconds=60, max_memory_mb=512)\n        self.execution_history = []\n    \n    def execute_code(self, code: str, language: str = 'python') -> dict:\n        '''\n        Execute code with comprehensive analysis.\n        '''\n        \n        # Pre-execution analysis\n        analysis = self._analyze_code(code)\n        \n        if analysis['security_issues']:\n            return {\n                'status': 'blocked',\n                'reason': 'security_issues',\n                'issues': analysis['security_issues'],\n                'code': code\n            }\n        \n        # Execute\n        exec_result = self.executor.execute(code)\n        \n        # Post-execution analysis\n        result = {\n            'status': exec_result['status'],\n            'output': exec_result.get('output'),\n            'error': exec_result.get('error'),\n            'code': code,\n            'analysis': analysis,\n            'execution_time_ms': exec_result.get('execution_time_ms', 0)\n        }\n        \n        # Store in history\n        self.execution_history.append(result)\n        \n        return result\n    \n    def _analyze_code(self, code: str) -> dict:\n        '''Analyze code for issues before execution'''\n        issues = {\n            'security_issues': [],\n            'performance_warnings': [],\n            'style_suggestions': [],\n        }\n        \n        # Security checks\n        dangerous_patterns = ['os.system', 'subprocess', 'eval(', 'exec(', '__import__']\n        for pattern in dangerous_patterns:\n            if pattern in code:\n                issues['security_issues'].append(f'Dangerous operation: {pattern}')\n        \n        # Performance checks\n        if 'while True:' in code and 'break' not in code:\n            issues['performance_warnings'].append('Potential infinite loop detected')\n        \n        # Count nested loops (O(n\u00b2) or worse)\n        nested_loops = code.count('for ') + code.count('while ')\n        if nested_loops >= 2:\n            issues['performance_warnings'].append(f'Nested loops detected (possible O(n\u00b2) or worse)')\n        \n        return issues\n    \n    def test_code(self, code: str, test_cases: List[dict]) -> dict:\n        '''Test code with multiple test cases'''\n        results = []\n        \n        for test_case in test_cases:\n            # Modify code to use test input\n            test_code = code + f\"\\nprint({test_case['function_call']})\"\n            \n            result = self.execute_code(test_code)\n            \n            if result['status'] == 'success':\n                output = result['output'].strip()\n                expected = str(test_case['expected'])\n                passed = output == expected\n                \n                results.append({\n                    'test': test_case['name'],\n                    'passed': passed,\n                    'expected': expected,\n                    'actual': output\n                })\n            else:\n                results.append({\n                    'test': test_case['name'],\n                    'passed': False,\n                    'error': result['error']\n                })\n        \n        passed_count = sum(1 for r in results if r['passed'])\n        \n        return {\n            'total_tests': len(test_cases),\n            'passed': passed_count,\n            'failed': len(test_cases) - passed_count,\n            'pass_rate': passed_count / len(test_cases),\n            'results': results\n        }\n    \n    def get_execution_report(self) -> str:\n        '''Generate execution report'''\n        total = len(self.execution_history)\n        successful = sum(1 for e in self.execution_history if e['status'] == 'success')\n        \n        report = f'''\\nCode Execution Report - {self.name}\n{'=' * 70}\nTotal executions: {total}\nSuccessful: {successful}\nFailed: {total - successful}\nSuccess rate: {successful / total * 100:.1f}% if total > 0 else 0\n{'=' * 70}\n'''\n        return report\n\n# Demo CodeExecutionAgent\nprint('CODEEXECUTIONAGENT DEMONSTRATION')\nprint('=' * 90)\n\ncode_agent = CodeExecutionAgent(\n    name='CodeExecutor',\n    work_dir='./workspace',\n    use_docker=False\n)\n\n# Example code\nfibonacci_code = '''def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n'''\n\n# Execute code\nresult = code_agent.execute_code(fibonacci_code)\n\nprint(f'Execution status: {result[\"status\"]}')\nprint(f'Security issues: {len(result[\"analysis\"][\"security_issues\"])}')\nprint(f'Performance warnings: {len(result[\"analysis\"][\"performance_warnings\"])}')\n\n# Test code\ntest_cases = [\n    {'name': 'test_base_case', 'function_call': 'fibonacci(0)', 'expected': '0'},\n    {'name': 'test_small', 'function_call': 'fibonacci(5)', 'expected': '5'},\n    {'name': 'test_medium', 'function_call': 'fibonacci(10)', 'expected': '55'},\n]\n\ntest_result = code_agent.test_code(fibonacci_code, test_cases)\n\nprint(f'\\nTest Results: {test_result[\"passed\"]}/{test_result[\"total_tests\"]} passed ({test_result[\"pass_rate\"]:.0%})')\n\nprint(code_agent.get_execution_report())\n\nprint('=' * 90)\nprint('CODEEXECUTIONAGENT BEST PRACTICES:')\nprint('  - Always analyze code before execution')\nprint('  - Use Docker for true isolation in production')\nprint('  - Set resource limits (CPU, memory, time)')\nprint('  - Test code with multiple test cases')\nprint('  - Maintain execution history for debugging')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 4: Role Definition & Specialization\n\nDefining specialized roles for agents to create expert teams."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class RoleDefinition:\n    '''\n    Define specialized agent roles with expertise and constraints.\n    '''\n    \n    def __init__(self,\n                 role_name: str,\n                 expertise_areas: List[str],\n                 responsibilities: List[str],\n                 constraints: List[str],\n                 system_message_template: str):\n        self.role_name = role_name\n        self.expertise_areas = expertise_areas\n        self.responsibilities = responsibilities\n        self.constraints = constraints\n        self.system_message_template = system_message_template\n    \n    def build_system_message(self, **kwargs) -> str:\n        '''Build system message for agent'''\n        base_message = self.system_message_template.format(**kwargs)\n        \n        full_message = f'''Role: {self.role_name}\n\nExpertise:\n{chr(10).join(f'- {area}' for area in self.expertise_areas)}\n\nResponsibilities:\n{chr(10).join(f'- {resp}' for resp in self.responsibilities)}\n\nConstraints:\n{chr(10).join(f'- {const}' for const in self.constraints)}\n\n{base_message}\n'''\n        return full_message\n\n# Define specialized roles\nprint('ROLE DEFINITION & SPECIALIZATION')\nprint('=' * 90)\n\n# Define role templates\nroles = {\n    'Senior_Developer': RoleDefinition(\n        role_name='Senior Software Developer',\n        expertise_areas=['Python', 'System Design', 'Best Practices', 'Testing'],\n        responsibilities=[\n            'Write clean, maintainable code',\n            'Follow design patterns',\n            'Include error handling',\n            'Add documentation and tests'\n        ],\n        constraints=[\n            'Never use deprecated libraries',\n            'Always handle exceptions',\n            'Code must be PEP8 compliant',\n            'Include type hints'\n        ],\n        system_message_template='You are a senior developer. {task}'\n    ),\n    \n    'Code_Reviewer': RoleDefinition(\n        role_name='Code Reviewer',\n        expertise_areas=['Code Quality', 'Security', 'Performance', 'Testing'],\n        responsibilities=[\n            'Review code for bugs',\n            'Check security vulnerabilities',\n            'Assess performance',\n            'Verify test coverage'\n        ],\n        constraints=[\n            'Be objective and constructive',\n            'Cite specific issues',\n            'Provide improvement suggestions',\n            'Check against style guide'\n        ],\n        system_message_template='You are a code reviewer. {task}'\n    ),\n    \n    'QA_Engineer': RoleDefinition(\n        role_name='QA Engineer',\n        expertise_areas=['Testing', 'Edge Cases', 'Test Design', 'Quality Assurance'],\n        responsibilities=[\n            'Design comprehensive test cases',\n            'Identify edge cases',\n            'Verify requirements',\n            'Report defects'\n        ],\n        constraints=[\n            'Cover happy path and edge cases',\n            'Include boundary testing',\n            'Document test scenarios',\n            'Verify expected behavior'\n        ],\n        system_message_template='You are a QA engineer. {task}'\n    ),\n}\n\n# Create specialized agents\nspecialized_agents = {}\n\nfor role_key, role_def in roles.items():\n    system_msg = role_def.build_system_message(task='Focus on your area of expertise.')\n    \n    agent = AssistantAgent(\n        name=role_key,\n        system_message=system_msg,\n        llm_config={'model': 'gpt-4', 'temperature': 0.3}\n    )\n    \n    specialized_agents[role_key] = agent\n    \n    print(f'\\nAgent: {role_def.role_name}')\n    print(f'Expertise: {', '.join(role_def.expertise_areas)}')\n    print(f'Responsibilities: {len(role_def.responsibilities)}')\n\nprint('\\n' + '=' * 90)\nprint('ROLE SPECIALIZATION BENEFITS:')\nprint('  - Clear separation of concerns')\nprint('  - Domain expertise per agent')\nprint('  - Better output quality')\nprint('  - Easier debugging (know which agent did what)')\nprint('  - Scalable team structure')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Component 5: Multi-Agent Orchestration Patterns\n\nDifferent orchestration patterns for different use cases."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from enum import Enum\n\nclass OrchestrationMode(Enum):\n    SEQUENTIAL = 'sequential'  # One after another\n    ROUND_ROBIN = 'round_robin'  # Take turns\n    GROUP_CHAT = 'group_chat'  # Dynamic selection\n    HIERARCHICAL = 'hierarchical'  # Manager delegates\n\nclass AutoGenOrchestrator:\n    '''\n    Orchestrate multiple AutoGen agents with different patterns.\n    '''\n    \n    def __init__(self, agents: List, mode: OrchestrationMode = OrchestrationMode.SEQUENTIAL):\n        self.agents = {agent.name: agent for agent in agents}\n        self.mode = mode\n        self.conversation_log = []\n        self.current_speaker_index = 0\n    \n    def run(self, initial_message: str, max_turns: int = 10) -> dict:\n        '''Run orchestrated conversation'''\n        \n        self.conversation_log = [{'role': 'user', 'content': initial_message}]\n        \n        for turn in range(max_turns):\n            # Select next speaker\n            speaker = self._select_next_speaker()\n            \n            if not speaker:\n                break\n            \n            # Generate response\n            response = speaker.generate_reply(self.conversation_log)\n            \n            if response is None:\n                continue\n            \n            if response == 'TERMINATE':\n                print(f'\\nConversation terminated by {speaker.name}')\n                break\n            \n            # Add to log\n            self.conversation_log.append({\n                'role': 'assistant',\n                'name': speaker.name,\n                'content': response\n            })\n            \n            print(f'\\nTurn {turn + 1} - {speaker.name}:')\n            print(f'  {response[:100]}...' if len(response) > 100 else f'  {response}')\n            \n            # Check termination\n            if self._should_terminate():\n                break\n        \n        return {\n            'conversation': self.conversation_log,\n            'total_turns': turn + 1,\n            'mode': self.mode.value\n        }\n    \n    def _select_next_speaker(self):\n        '''Select next agent based on orchestration mode'''\n        \n        if self.mode == OrchestrationMode.SEQUENTIAL:\n            # Each agent speaks once in order\n            agent_list = list(self.agents.values())\n            if self.current_speaker_index < len(agent_list):\n                speaker = agent_list[self.current_speaker_index]\n                self.current_speaker_index += 1\n                return speaker\n            return None\n        \n        elif self.mode == OrchestrationMode.ROUND_ROBIN:\n            # Agents take turns\n            agent_list = list(self.agents.values())\n            speaker = agent_list[self.current_speaker_index % len(agent_list)]\n            self.current_speaker_index += 1\n            return speaker\n        \n        elif self.mode == OrchestrationMode.GROUP_CHAT:\n            # Most relevant agent speaks (based on last message)\n            return self._select_by_expertise()\n        \n        elif self.mode == OrchestrationMode.HIERARCHICAL:\n            # Manager delegates\n            return self._manager_selects()\n    \n    def _select_by_expertise(self):\n        '''Select agent with relevant expertise (group chat)'''\n        if not self.conversation_log:\n            return list(self.agents.values())[0]\n        \n        last_message = self.conversation_log[-1]['content'].lower()\n        \n        # Match keywords to agent expertise (simplified)\n        keyword_to_agent = {\n            'code': 'Senior_Developer',\n            'test': 'QA_Engineer',\n            'review': 'Code_Reviewer',\n            'security': 'Code_Reviewer',\n            'bug': 'Senior_Developer',\n        }\n        \n        for keyword, agent_name in keyword_to_agent.items():\n            if keyword in last_message and agent_name in self.agents:\n                return self.agents[agent_name]\n        \n        # Default to first agent\n        return list(self.agents.values())[0]\n    \n    def _manager_selects(self):\n        '''Manager decides which agent to use'''\n        # In production, manager agent uses LLM to decide\n        return list(self.agents.values())[0]\n    \n    def _should_terminate(self) -> bool:\n        '''Check if conversation should end'''\n        if not self.conversation_log:\n            return False\n        \n        last_content = self.conversation_log[-1]['content'].lower()\n        return 'terminate' in last_content or 'task complete' in last_content\n\n# Demo different orchestration modes\nprint('MULTI-AGENT ORCHESTRATION PATTERNS')\nprint('=' * 90)\n\n# Create agent team\ndeveloper = AssistantAgent(\n    'Developer',\n    'You write code.',\n    {'model': 'gpt-4', 'temperature': 0.3}\n)\n\nreviewer = AssistantAgent(\n    'Reviewer',\n    'You review code for quality.',\n    {'model': 'gpt-4', 'temperature': 0.3}\n)\n\ntester = AssistantAgent(\n    'Tester',\n    'You create test cases.',\n    {'model': 'gpt-4', 'temperature': 0.3}\n)\n\n# Test sequential mode\nprint('\\n1. SEQUENTIAL MODE:')\nprint('-' * 80)\norchestrator_seq = AutoGenOrchestrator(\n    [developer, reviewer, tester],\n    mode=OrchestrationMode.SEQUENTIAL\n)\n\ntask = 'Create a function to validate email addresses'\nresult = orchestrator_seq.run(task, max_turns=3)\nprint(f'Completed in {result[\"total_turns\"]} turns')\n\nprint('\\n' + '=' * 90)\nprint('ORCHESTRATION MODE GUIDE:')\nprint('  - SEQUENTIAL: Fixed order (best for pipelines)')\nprint('  - ROUND_ROBIN: Fair participation (best for brainstorming)')\nprint('  - GROUP_CHAT: Dynamic selection (best for complex tasks)')\nprint('  - HIERARCHICAL: Manager delegates (best for large teams)')"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}