{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 2: RAG Systems\n",
        "\n",
        "## Applied AI Scientist Field Notes - Expanded Edition\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 2: Retrieval-Augmented Generation (RAG) - Production Implementation\n",
        "\n",
        "### Topics Covered\n",
        "1. Advanced chunking strategies\n",
        "2. Vector embeddings and similarity search\n",
        "3. Hybrid search (BM25 + Dense)\n",
        "4. Security: RBAC and audit logging\n",
        "5. Reranking and fusion\n",
        "6. Production optimization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q chromadb sentence-transformers faiss-cpu rank-bm25\n",
        "%pip install -q pandas numpy\n",
        "\n",
        "print('Dependencies installed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Document Chunking Strategies\n",
        "\n",
        "Chunking is critical for RAG quality. Different strategies:\n",
        "- **Fixed-size**: Simple but breaks semantics\n",
        "- **Sentence-aware**: Respects boundaries\n",
        "- **Semantic**: Based on topic shifts\n",
        "- **Recursive**: Multi-level splitting\n",
        "- **Structure-aware**: Preserves document structure (headers, lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "import hashlib\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    text: str\n",
        "    start_idx: int\n",
        "    end_idx: int\n",
        "    chunk_id: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class AdvancedChunker:\n",
        "    '''Production-grade document chunking'''\n",
        "    \n",
        "    def __init__(self, chunk_size=512, overlap=50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def chunk_by_sentences(self, text: str, doc_id: str) -> List[Chunk]:\n",
        "        '''Chunk respecting sentence boundaries'''\n",
        "        import re\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        \n",
        "        chunks = []\n",
        "        current = []\n",
        "        current_len = 0\n",
        "        start_idx = 0\n",
        "        \n",
        "        for sent in sentences:\n",
        "            if current_len + len(sent) > self.chunk_size and current:\n",
        "                chunk_text = ' '.join(current)\n",
        "                chunks.append(Chunk(\n",
        "                    text=chunk_text,\n",
        "                    start_idx=start_idx,\n",
        "                    end_idx=start_idx + len(chunk_text),\n",
        "                    chunk_id=f'{doc_id}_c{len(chunks)}',\n",
        "                    metadata={'method': 'sentence', 'doc_id': doc_id}\n",
        "                ))\n",
        "                \n",
        "                # Overlap\n",
        "                if self.overlap > 0 and len(current) > 1:\n",
        "                    current = current[-1:]\n",
        "                    current_len = len(current[0])\n",
        "                else:\n",
        "                    current = []\n",
        "                    current_len = 0\n",
        "                \n",
        "                start_idx += len(chunk_text) - current_len\n",
        "            \n",
        "            current.append(sent)\n",
        "            current_len += len(sent)\n",
        "        \n",
        "        if current:\n",
        "            chunk_text = ' '.join(current)\n",
        "            chunks.append(Chunk(\n",
        "                text=chunk_text,\n",
        "                start_idx=start_idx,\n",
        "                end_idx=start_idx + len(chunk_text),\n",
        "                chunk_id=f'{doc_id}_c{len(chunks)}',\n",
        "                metadata={'method': 'sentence', 'doc_id': doc_id}\n",
        "            ))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "# Test\n",
        "chunker = AdvancedChunker(chunk_size=200, overlap=30)\n",
        "sample = 'AI agents are software systems. They use LLMs for reasoning. They can use tools. They maintain state across interactions.'\n",
        "chunks = chunker.chunk_by_sentences(sample, 'doc1')\n",
        "\n",
        "print(f'Created {len(chunks)} chunks:')\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f'  Chunk {i+1}: {len(c.text)} chars - {c.text[:60]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Production RAG System with Security\n",
        "\n",
        "Key features:\n",
        "- Role-based access control (RBAC)\n",
        "- Audit logging\n",
        "- Metadata filtering\n",
        "- Citation tracking\n",
        "- Cost monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "class SecureRAG:\n",
        "    '''Production RAG with RBAC and observability'''\n",
        "    \n",
        "    def __init__(self, embedding_model='all-MiniLM-L6-v2'):\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.client = chromadb.Client()\n",
        "        self.collection = self.client.get_or_create_collection('secure_docs')\n",
        "        self.audit_log = []\n",
        "    \n",
        "    def ingest(self, text: str, doc_id: str, allowed_roles: set, metadata: dict = None):\n",
        "        '''Ingest document with access control'''\n",
        "        chunker = AdvancedChunker(400, 50)\n",
        "        chunks = chunker.chunk_by_sentences(text, doc_id)\n",
        "        \n",
        "        texts = [c.text for c in chunks]\n",
        "        embeddings = self.embedding_model.encode(texts).tolist()\n",
        "        \n",
        "        ids = []\n",
        "        metas = []\n",
        "        \n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = f'{doc_id}_{i}_{hashlib.md5(chunk.text.encode()).hexdigest()[:8]}'\n",
        "            ids.append(chunk_id)\n",
        "            \n",
        "            meta = {\n",
        "                'doc_id': doc_id,\n",
        "                'allowed_roles': ','.join(allowed_roles),\n",
        "                'ingested_at': datetime.utcnow().isoformat()\n",
        "            }\n",
        "            if metadata:\n",
        "                meta.update(metadata)\n",
        "            metas.append(meta)\n",
        "        \n",
        "        self.collection.add(ids=ids, embeddings=embeddings, documents=texts, metadatas=metas)\n",
        "        self._log('INGEST', doc_id=doc_id, chunks=len(ids))\n",
        "        return ids\n",
        "    \n",
        "    def retrieve(self, query: str, user_role: str, top_k=5):\n",
        "        '''Retrieve with RBAC enforcement'''\n",
        "        query_emb = self.embedding_model.encode([query])[0].tolist()\n",
        "        results = self.collection.query(query_embeddings=[query_emb], n_results=top_k * 2)\n",
        "        \n",
        "        filtered = []\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            meta = results['metadatas'][0][i]\n",
        "            allowed = set(meta.get('allowed_roles', '').split(','))\n",
        "            \n",
        "            if user_role in allowed or 'public' in allowed:\n",
        "                filtered.append({\n",
        "                    'text': results['documents'][0][i],\n",
        "                    'metadata': meta,\n",
        "                    'similarity': 1 - results['distances'][0][i]\n",
        "                })\n",
        "                if len(filtered) >= top_k:\n",
        "                    break\n",
        "        \n",
        "        self._log('RETRIEVE', query=query[:50], role=user_role, results=len(filtered))\n",
        "        return filtered\n",
        "    \n",
        "    def _log(self, action, **kwargs):\n",
        "        self.audit_log.append({\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "            'action': action,\n",
        "            'log_id': str(uuid.uuid4()),\n",
        "            **kwargs\n",
        "        })\n",
        "    \n",
        "    def get_audit_log(self, last_n=10):\n",
        "        return self.audit_log[-last_n:]\n",
        "\n",
        "# Example usage\n",
        "print('Initializing Secure RAG...')\n",
        "rag = SecureRAG()\n",
        "\n",
        "# Ingest with different access levels\n",
        "rag.ingest('Company holidays: Jan 1, Jul 4, Dec 25', 'holidays', {'public', 'employee'})\n",
        "rag.ingest('Leave policy: 15 days after 1 year tenure', 'leave', {'employee', 'hr'})\n",
        "rag.ingest('L4 salary: $150K-$180K', 'compensation', {'hr'})\n",
        "\n",
        "print('\\nTesting RBAC retrieval...')\n",
        "queries = [\n",
        "    ('holidays', 'public'),\n",
        "    ('leave policy', 'employee'),\n",
        "    ('L4 salary', 'employee'),  # blocked\n",
        "    ('L4 salary', 'hr')  # allowed\n",
        "]\n",
        "\n",
        "for q, role in queries:\n",
        "    results = rag.retrieve(q, role, top_k=1)\n",
        "    print(f'Query: {q:20} | Role: {role:10} | Results: {len(results)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Questions: RAG Systems - Part 1 (Chunking & Retrieval)\n",
        "\n",
        "### For Experienced Professionals\n",
        "\n",
        "Understanding production RAG requires deep knowledge of chunking strategies, retrieval methods, and performance optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "interview_questions_rag_part1 = [\n",
        "    {\n",
        "        \"level\": \"Senior\",\n",
        "        \"question\": \"Your RAG system has 92% retrieval recall but users complain answers are 'incomplete' or 'miss important details.' The retrieved chunks are semantically relevant. What's likely wrong and how do you fix it?\",\n",
        "        \"answer\": \"\"\"\n",
        "**Root Cause Analysis:**\n",
        "\n",
        "The problem is likely **chunk boundary issues** - semantically relevant chunks are retrieved, but critical context is split across chunk boundaries.\n",
        "\n",
        "**Example Scenario:**\n",
        "```\n",
        "Chunk 1 (retrieved): \"...the refund policy applies to purchases within 30 days.\"\n",
        "Chunk 2 (not retrieved): \"However, electronics have a different policy requiring return within 14 days with original packaging.\"\n",
        "```\n",
        "\n",
        "User asks: \"What's the electronics refund policy?\"\n",
        "System retrieves Chunk 1 (mentions \"refund\"), but misses the specific exception for electronics.\n",
        "\n",
        "**Diagnostic Steps:**\n",
        "\n",
        "1. **Analyze Failed Cases:**\n",
        "   ```python\n",
        "   def diagnose_incomplete_answers(query: str, retrieved_chunks: List[str], ground_truth: str) -> dict:\n",
        "       # Check if answer spans multiple chunks\n",
        "       all_chunks = get_all_chunks_for_doc(doc_id)\n",
        "       \n",
        "       # Find which chunks contain ground truth info\n",
        "       relevant_chunk_ids = []\n",
        "       for i, chunk in enumerate(all_chunks):\n",
        "           if contains_answer_info(chunk, ground_truth):\n",
        "               relevant_chunk_ids.append(i)\n",
        "       \n",
        "       # Check if relevant chunks are adjacent but not all retrieved\n",
        "       retrieved_ids = [c.id for c in retrieved_chunks]\n",
        "       missing_adjacent = [\n",
        "           id for id in relevant_chunk_ids \n",
        "           if id not in retrieved_ids and (id-1 in retrieved_ids or id+1 in retrieved_ids)\n",
        "       ]\n",
        "       \n",
        "       return {\n",
        "           \"total_relevant\": len(relevant_chunk_ids),\n",
        "           \"retrieved_relevant\": len([id for id in relevant_chunk_ids if id in retrieved_ids]),\n",
        "           \"missing_adjacent\": len(missing_adjacent),\n",
        "           \"boundary_issue\": len(missing_adjacent) > 0\n",
        "       }\n",
        "   ```\n",
        "\n",
        "2. **Check Chunk Size Distribution:**\n",
        "   - Too small: Fragments concepts\n",
        "   - Too large: Dilutes relevance scores\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**1. Sentence Window Retrieval (Immediate, ~40% improvement):**\n",
        "```python\n",
        "class SentenceWindowRetriever:\n",
        "    '''Retrieve small chunks but return with context window.'''\n",
        "    \n",
        "    def __init__(self, chunk_size=256, window_sentences=3):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.window_sentences = window_sentences\n",
        "    \n",
        "    def retrieve_with_context(self, query: str, top_k: int = 5) -> List[dict]:\n",
        "        # Step 1: Retrieve small, focused chunks\n",
        "        small_chunks = self.retrieve_small_chunks(query, top_k)\n",
        "        \n",
        "        # Step 2: Expand each chunk with surrounding sentences\n",
        "        expanded_chunks = []\n",
        "        for chunk in small_chunks:\n",
        "            expanded = self.expand_chunk(\n",
        "                chunk,\n",
        "                before=self.window_sentences,\n",
        "                after=self.window_sentences\n",
        "            )\n",
        "            expanded_chunks.append(expanded)\n",
        "        \n",
        "        return expanded_chunks\n",
        "    \n",
        "    def expand_chunk(self, chunk: dict, before: int, after: int) -> dict:\n",
        "        '''Add context from surrounding sentences.'''\n",
        "        doc_id = chunk['metadata']['doc_id']\n",
        "        chunk_idx = chunk['metadata']['chunk_idx']\n",
        "        \n",
        "        # Get original document\n",
        "        doc = self.get_document(doc_id)\n",
        "        all_chunks = self.chunk_document(doc)\n",
        "        \n",
        "        # Expand window\n",
        "        start_idx = max(0, chunk_idx - before)\n",
        "        end_idx = min(len(all_chunks), chunk_idx + after + 1)\n",
        "        \n",
        "        expanded_text = ' '.join([\n",
        "            all_chunks[i].text for i in range(start_idx, end_idx)\n",
        "        ])\n",
        "        \n",
        "        return {\n",
        "            'text': expanded_text,\n",
        "            'original_chunk': chunk['text'],\n",
        "            'context_added': True,\n",
        "            'metadata': chunk['metadata']\n",
        "        }\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Retrieve precise matches (small chunks)\n",
        "- Return complete context (window expansion)\n",
        "- 20-40% improvement in answer completeness\n",
        "\n",
        "**2. Hierarchical Chunking (Medium-term, ~50% improvement):**\n",
        "```python\n",
        "class HierarchicalChunker:\n",
        "    '''Multi-level chunking: paragraph -> section -> document.'''\n",
        "    \n",
        "    def chunk_hierarchical(self, doc: str, doc_id: str) -> List[dict]:\n",
        "        chunks = []\n",
        "        \n",
        "        # Level 1: Paragraph chunks (for retrieval)\n",
        "        paragraphs = doc.split('\\\\n\\\\n')\n",
        "        \n",
        "        # Level 2: Section chunks (for context)\n",
        "        sections = self.split_by_headers(doc)\n",
        "        \n",
        "        for i, para in enumerate(paragraphs):\n",
        "            # Find parent section\n",
        "            parent_section = self.find_parent_section(para, sections)\n",
        "            \n",
        "            chunks.append({\n",
        "                'chunk_id': f'{doc_id}_p{i}',\n",
        "                'text': para,  # Small chunk for embedding\n",
        "                'parent_text': parent_section,  # Large context for LLM\n",
        "                'level': 'paragraph',\n",
        "                'metadata': {\n",
        "                    'doc_id': doc_id,\n",
        "                    'section_title': parent_section.split('\\\\n')[0]\n",
        "                }\n",
        "            })\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def retrieve_hierarchical(self, query: str) -> List[dict]:\n",
        "        # Retrieve small paragraph chunks\n",
        "        para_chunks = self.retrieve_paragraphs(query, top_k=10)\n",
        "        \n",
        "        # Return parent section for each chunk\n",
        "        return [\n",
        "            {\n",
        "                'retrieval_chunk': chunk['text'],\n",
        "                'context_for_llm': chunk['parent_text'],  # Full section\n",
        "                'metadata': chunk['metadata']\n",
        "            }\n",
        "            for chunk in para_chunks\n",
        "        ]\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Retrieve granular (paragraph-level)\n",
        "- Provide comprehensive context (section-level)\n",
        "- Preserves document structure\n",
        "\n",
        "**3. Overlapping with Semantic Similarity (Long-term, ~60% improvement):**\n",
        "```python\n",
        "class SmartOverlapChunker:\n",
        "    '''Intelligent overlap based on semantic coherence.'''\n",
        "    \n",
        "    def chunk_with_smart_overlap(self, doc: str) -> List[dict]:\n",
        "        sentences = self.split_sentences(doc)\n",
        "        chunks = []\n",
        "        \n",
        "        i = 0\n",
        "        while i < len(sentences):\n",
        "            # Build chunk up to max size\n",
        "            chunk_sentences = []\n",
        "            chunk_len = 0\n",
        "            \n",
        "            while i < len(sentences) and chunk_len < self.max_chunk_size:\n",
        "                chunk_sentences.append(sentences[i])\n",
        "                chunk_len += len(sentences[i])\n",
        "                i += 1\n",
        "            \n",
        "            # Look ahead: should we include more for completeness?\n",
        "            if i < len(sentences):\n",
        "                next_sentence = sentences[i]\n",
        "                \n",
        "                # Check semantic coherence\n",
        "                if self.should_extend_chunk(chunk_sentences, next_sentence):\n",
        "                    # Critical info continues, extend chunk\n",
        "                    chunk_sentences.append(next_sentence)\n",
        "                    i += 1\n",
        "            \n",
        "            chunks.append({\n",
        "                'text': ' '.join(chunk_sentences),\n",
        "                'sentences': chunk_sentences\n",
        "            })\n",
        "            \n",
        "            # Smart backtrack for overlap\n",
        "            # Include last 2-3 sentences in next chunk if semantically connected\n",
        "            if i < len(sentences):\n",
        "                overlap_sentences = self.determine_overlap(chunk_sentences, sentences[i:i+3])\n",
        "                i -= len(overlap_sentences)\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def should_extend_chunk(self, current: List[str], next_sent: str) -> bool:\n",
        "        '''Check if next sentence completes the concept.'''\n",
        "        # Check for continuation markers\n",
        "        continuation_markers = ['however', 'additionally', 'furthermore', 'except']\n",
        "        if any(marker in next_sent.lower() for marker in continuation_markers):\n",
        "            return True\n",
        "        \n",
        "        # Check semantic similarity\n",
        "        current_text = ' '.join(current[-2:])  # Last 2 sentences\n",
        "        similarity = self.compute_similarity(current_text, next_sent)\n",
        "        \n",
        "        return similarity > 0.8  # High similarity = same concept\n",
        "```\n",
        "\n",
        "**Metrics to Track:**\n",
        "```python\n",
        "metrics = {\n",
        "    \"retrieval_recall\": 0.92,  # Chunks retrieved / relevant chunks\n",
        "    \"answer_completeness\": 0.65,  # Before fix (user complaint)\n",
        "    \"answer_completeness_target\": 0.90,  # Target\n",
        "    \n",
        "    # After sentence window retrieval\n",
        "    \"answer_completeness_v2\": 0.85,  # +20%\n",
        "    \n",
        "    # After hierarchical chunking\n",
        "    \"answer_completeness_v3\": 0.92,  # +27%\n",
        "}\n",
        "```\n",
        "\n",
        "**Evaluation:**\n",
        "- Manual review of 100 failed cases\n",
        "- Measure: Does answer cover all relevant details in ground truth?\n",
        "- A/B test: Old chunking vs new strategy\n",
        "- Track: User satisfaction, follow-up question rate (should decrease)\n",
        "\n",
        "**Key Insight:**\n",
        "High retrieval recall \u2260 good answers. The problem isn't **which** chunks you retrieve, but **how much context** each chunk contains.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"level\": \"Senior\",\n",
        "        \"question\": \"You're building a RAG system for a legal document corpus (10K documents, avg 50 pages each). Your vector DB (FAISS) retrieval takes 800ms at P95. The business requirement is <200ms P95. Walk through your optimization strategy.\",\n",
        "        \"answer\": \"\"\"\n",
        "**Performance Baseline:**\n",
        "- Corpus: 10K docs \u00d7 50 pages \u00d7 ~3 chunks/page = ~1.5M chunks\n",
        "- Current P95 latency: 800ms\n",
        "- Target P95: 200ms (4x improvement needed)\n",
        "\n",
        "**Root Cause Analysis:**\n",
        "\n",
        "1. **Profile the Pipeline:**\n",
        "```python\n",
        "import time\n",
        "\n",
        "def profile_retrieval(query: str):\n",
        "    timings = {}\n",
        "    \n",
        "    start = time.time()\n",
        "    query_embedding = embed_model.encode([query])[0]\n",
        "    timings['embedding'] = time.time() - start\n",
        "    \n",
        "    start = time.time()\n",
        "    results = faiss_index.search(query_embedding, k=50)\n",
        "    timings['vector_search'] = time.time() - start\n",
        "    \n",
        "    start = time.time()\n",
        "    metadata = [get_metadata(id) for id in results.ids]\n",
        "    timings['metadata_fetch'] = time.time() - start\n",
        "    \n",
        "    start = time.time()\n",
        "    reranked = reranker.rank(query, [r.text for r in results])\n",
        "    timings['reranking'] = time.time() - start\n",
        "    \n",
        "    return timings\n",
        "\n",
        "# Run on 1000 queries\n",
        "timings = [profile_retrieval(q) for q in test_queries]\n",
        "p95 = {k: np.percentile([t[k] for t in timings], 95) for k in timings[0].keys()}\n",
        "\n",
        "print(p95)\n",
        "# {'embedding': 50ms, 'vector_search': 600ms, 'metadata_fetch': 100ms, 'reranking': 50ms}\n",
        "```\n",
        "\n",
        "**Bottleneck Identified: Vector search (600ms)**\n",
        "\n",
        "**Optimization Strategy:**\n",
        "\n",
        "**Phase 1: Index Optimization (Expected: 800ms \u2192 400ms)**\n",
        "\n",
        "```python\n",
        "import faiss\n",
        "\n",
        "class OptimizedFAISSIndex:\n",
        "    def __init__(self, dimension: int, num_vectors: int):\n",
        "        self.dimension = dimension\n",
        "        self.num_vectors = num_vectors\n",
        "        \n",
        "        # Strategy 1: Use IVF (Inverted File Index) for fast approximate search\n",
        "        # Instead of flat L2 search (exhaustive), use clustering\n",
        "        \n",
        "        n_clusters = int(np.sqrt(num_vectors))  # ~1,225 clusters for 1.5M vectors\n",
        "        \n",
        "        # Quantizer: coarse clustering\n",
        "        quantizer = faiss.IndexFlatL2(dimension)\n",
        "        \n",
        "        # IVF index: search only nearby clusters\n",
        "        self.index = faiss.IndexIVFFlat(\n",
        "            quantizer, \n",
        "            dimension, \n",
        "            n_clusters,\n",
        "            faiss.METRIC_L2\n",
        "        )\n",
        "        \n",
        "        # Train index on representative sample\n",
        "        print(\"Training index...\")\n",
        "        # training_vectors = sample_embeddings(100K)  # 100K samples sufficient\n",
        "        # self.index.train(training_vectors)\n",
        "    \n",
        "    def optimize_search_params(self):\n",
        "        '''Tune nprobe for latency/accuracy tradeoff'''\n",
        "        # nprobe: number of clusters to search\n",
        "        # Higher nprobe = more accurate but slower\n",
        "        \n",
        "        self.index.nprobe = 32  # Search 32 clusters (vs all 1,225)\n",
        "        # Accuracy: ~95% vs exhaustive search\n",
        "        # Speed: ~20x faster\n",
        "    \n",
        "    def add_with_ids(self, embeddings, ids):\n",
        "        '''Add vectors to index'''\n",
        "        self.index.add_with_ids(embeddings, ids)\n",
        "\n",
        "# Replace flat index with IVF\n",
        "# Before: IndexFlatL2 (exhaustive search, 600ms)\n",
        "# After: IndexIVFFlat with nprobe=32 (~30ms)\n",
        "```\n",
        "\n",
        "**Expected Improvement: 600ms \u2192 30ms for vector search**\n",
        "\n",
        "**Phase 2: Metadata Co-location (Expected: 400ms \u2192 300ms)**\n",
        "\n",
        "```python\n",
        "class ColocatedIndex:\n",
        "    '''Store metadata with vectors to avoid separate fetch'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.index = OptimizedFAISSIndex(768, 1_500_000)\n",
        "        # Store metadata in memory-mapped file for fast access\n",
        "        self.metadata_store = {}  # In production: RocksDB or similar\n",
        "    \n",
        "    def add_documents(self, chunks: List[dict]):\n",
        "        embeddings = []\n",
        "        ids = []\n",
        "        \n",
        "        for chunk in chunks:\n",
        "            embeddings.append(chunk['embedding'])\n",
        "            ids.append(chunk['id'])\n",
        "            \n",
        "            # Store metadata adjacent to ID\n",
        "            self.metadata_store[chunk['id']] = {\n",
        "                'text': chunk['text'],\n",
        "                'doc_id': chunk['doc_id'],\n",
        "                'page': chunk['page'],\n",
        "                # Critical: don't store large fields here\n",
        "            }\n",
        "        \n",
        "        self.index.add_with_ids(np.array(embeddings), np.array(ids))\n",
        "    \n",
        "    def search(self, query_embedding, k=50):\n",
        "        # Single operation: vector search + metadata fetch\n",
        "        distances, ids = self.index.search(query_embedding, k)\n",
        "        \n",
        "        # Fast in-memory metadata lookup (no DB call)\n",
        "        results = [\n",
        "            {\n",
        "                'id': ids[i],\n",
        "                'score': distances[i],\n",
        "                'metadata': self.metadata_store[ids[i]]\n",
        "            }\n",
        "            for i in range(len(ids))\n",
        "        ]\n",
        "        return results\n",
        "\n",
        "# Expected improvement: 100ms metadata fetch \u2192 <5ms\n",
        "```\n",
        "\n",
        "**Phase 3: Hybrid Search Parallelization (Expected: 300ms \u2192 220ms)**\n",
        "\n",
        "```python\n",
        "import concurrent.futures\n",
        "\n",
        "class ParallelHybridSearch:\n",
        "    def __init__(self):\n",
        "        self.vector_index = ColocatedIndex()\n",
        "        self.bm25_index = BM25Index()\n",
        "    \n",
        "    def search(self, query: str, k=50):\n",
        "        '''Run vector and BM25 search in parallel'''\n",
        "        query_embedding = self.embed(query)\n",
        "        \n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "            # Run both searches concurrently\n",
        "            vector_future = executor.submit(\n",
        "                self.vector_index.search, query_embedding, k\n",
        "            )\n",
        "            bm25_future = executor.submit(\n",
        "                self.bm25_index.search, query, k\n",
        "            )\n",
        "            \n",
        "            vector_results = vector_future.result()\n",
        "            bm25_results = bm25_future.result()\n",
        "        \n",
        "        # Fusion\n",
        "        return self.reciprocal_rank_fusion(vector_results, bm25_results)\n",
        "```\n",
        "\n",
        "**Phase 4: Embedding Caching (Expected: 220ms \u2192 180ms)**\n",
        "\n",
        "```python\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "\n",
        "class CachedEmbedding:\n",
        "    def __init__(self):\n",
        "        self.cache = {}  # In production: Redis\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    \n",
        "    def embed(self, text: str):\n",
        "        # Hash query for cache key\n",
        "        cache_key = hashlib.md5(text.encode()).hexdigest()\n",
        "        \n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "        \n",
        "        # Compute embedding\n",
        "        embedding = self.embedding_model.encode([text])[0]\n",
        "        \n",
        "        # Cache (TTL: 1 hour)\n",
        "        self.cache[cache_key] = embedding\n",
        "        \n",
        "        return embedding\n",
        "\n",
        "# For repeated/similar queries, saves 50ms embedding time\n",
        "```\n",
        "\n",
        "**Phase 5: Two-Tier Retrieval (Expected: 180ms \u2192 <150ms)**\n",
        "\n",
        "```python\n",
        "class TwoTierRetrieval:\n",
        "    '''Filter by metadata first, then vector search on subset'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Tier 1: Fast metadata filter (ElasticSearch, etc.)\n",
        "        self.metadata_index = ElasticSearchIndex()\n",
        "        \n",
        "        # Tier 2: Vector search on filtered subset\n",
        "        self.vector_indices = {}  # Partitioned by document type\n",
        "    \n",
        "    def search(self, query: str, filters: dict = None):\n",
        "        # Tier 1: Metadata filter (10ms)\n",
        "        # Filter by: document type, date range, author, category\n",
        "        if filters:\n",
        "            candidate_doc_ids = self.metadata_index.filter(filters)\n",
        "            # Reduces search space: 1.5M \u2192 100K vectors\n",
        "        else:\n",
        "            candidate_doc_ids = None\n",
        "        \n",
        "        # Tier 2: Vector search on subset\n",
        "        if candidate_doc_ids:\n",
        "            # Search only relevant partition\n",
        "            doc_type = filters.get('doc_type')\n",
        "            results = self.vector_indices[doc_type].search(query, k=50)\n",
        "        else:\n",
        "            results = self.main_index.search(query, k=50)\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Use case: \"Find refund policy documents from 2023\"\n",
        "# Without filter: Search 1.5M vectors (30ms with IVF)\n",
        "# With filter: Search 50K vectors (2ms)\n",
        "```\n",
        "\n",
        "**Final Architecture:**\n",
        "```python\n",
        "class ProductionRAG:\n",
        "    def __init__(self):\n",
        "        self.embedding_cache = CachedEmbedding()\n",
        "        self.two_tier_search = TwoTierRetrieval()\n",
        "    \n",
        "    def retrieve(self, query: str, filters: dict = None, k: int = 5):\n",
        "        # Step 1: Get/cache embedding (5ms cached, 50ms uncached)\n",
        "        query_emb = self.embedding_cache.embed(query)\n",
        "        \n",
        "        # Step 2: Two-tier filtered search (10-30ms)\n",
        "        results = self.two_tier_search.search(query, filters)\n",
        "        \n",
        "        # Step 3: In-memory re-ranking of top results (20ms)\n",
        "        reranked = self.fast_rerank(query, results[:20])\n",
        "        \n",
        "        return reranked[:k]\n",
        "```\n",
        "\n",
        "**Performance Summary:**\n",
        "| Stage | Before | After | Technique |\n",
        "|-------|--------|-------|-----------|\n",
        "| Embedding | 50ms | 5ms (cached) | LRU cache |\n",
        "| Vector search | 600ms | 10-30ms | IVF + partitioning |\n",
        "| Metadata fetch | 100ms | <5ms | Co-location |\n",
        "| Re-ranking | 50ms | 20ms | Only top-20 |\n",
        "| **Total P95** | **800ms** | **150ms** | **5.3x improvement** |\n",
        "\n",
        "**Cost-Benefit Analysis:**\n",
        "- Development time: 2-3 weeks\n",
        "- Infrastructure: Minimal (FAISS is free, runs on same hardware)\n",
        "- Accuracy impact: <2% (from approximate search)\n",
        "- Maintenance: Low (no new dependencies)\n",
        "\n",
        "**Monitoring:**\n",
        "```python\n",
        "metrics = {\n",
        "    'p50_latency_ms': 80,\n",
        "    'p95_latency_ms': 150,\n",
        "    'p99_latency_ms': 220,\n",
        "    'cache_hit_rate': 0.35,  # 35% of queries cached\n",
        "    'avg_search_space': 50_000,  # Down from 1.5M\n",
        "    'accuracy_vs_exhaustive': 0.98,  # 98% recall\n",
        "}\n",
        "```\n",
        "\n",
        "**Key Insight:**\n",
        "The biggest win comes from IVF indexing (20x speedup). Co-location and caching provide incremental gains. Always profile first to find the actual bottleneck.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"level\": \"Staff\",\n",
        "        \"question\": \"Design a production RAG evaluation framework that measures relevance, groundedness, faithfulness, and cost. Include both offline and online evaluation strategies, and explain how you'd catch regressions before they hit users.\",\n",
        "        \"answer\": \"\"\"\n",
        "**Complete RAG Evaluation Framework:**\n",
        "\n",
        "**1. Metrics Taxonomy:**\n",
        "\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class RAGMetrics:\n",
        "    # Retrieval Quality\n",
        "    retrieval_recall: float  # % of relevant docs retrieved\n",
        "    retrieval_precision: float  # % of retrieved docs that are relevant\n",
        "    retrieval_mrr: float  # Mean Reciprocal Rank\n",
        "    retrieval_ndcg: float  # Normalized Discounted Cumulative Gain\n",
        "    \n",
        "    # Generation Quality\n",
        "    answer_relevance: float  # Does answer address the question?\n",
        "    answer_faithfulness: float  # Is answer supported by retrieved docs?\n",
        "    answer_groundedness: float  # No hallucinations\n",
        "    answer_completeness: float  # Covers all aspects of question\n",
        "    \n",
        "    # Business Metrics\n",
        "    user_satisfaction: Optional[float]  # Thumbs up/down\n",
        "    task_completion_rate: Optional[float]  # Did user get what they needed?\n",
        "    follow_up_question_rate: Optional[float]  # Lower is better\n",
        "    \n",
        "    # Operational Metrics\n",
        "    latency_p50_ms: float\n",
        "    latency_p95_ms: float\n",
        "    cost_per_query: float  # Embedding + LLM + infrastructure\n",
        "    \n",
        "    # Safety\n",
        "    pii_leakage_detected: bool\n",
        "    injection_attempt_blocked: bool\n",
        "```\n",
        "\n",
        "**2. Offline Evaluation (Pre-Deployment):**\n",
        "\n",
        "```python\n",
        "class OfflineRAGEvaluator:\n",
        "    '''Comprehensive offline evaluation before deployment'''\n",
        "    \n",
        "    def __init__(self, test_dataset: List[dict]):\n",
        "        '''\n",
        "        test_dataset format:\n",
        "        {\n",
        "            \"query\": \"What's the refund policy?\",\n",
        "            \"ground_truth_answer\": \"30 days with receipt\",\n",
        "            \"relevant_doc_ids\": [\"doc_123\", \"doc_456\"],\n",
        "            \"expected_citations\": [\"doc_123\"]\n",
        "        }\n",
        "        '''\n",
        "        self.test_dataset = test_dataset\n",
        "        self.results = []\n",
        "    \n",
        "    def evaluate_retrieval(self, rag_system) -> dict:\n",
        "        '''Measure retrieval quality'''\n",
        "        recalls = []\n",
        "        precisions = []\n",
        "        mrrs = []\n",
        "        \n",
        "        for case in self.test_dataset:\n",
        "            # Retrieve documents\n",
        "            retrieved = rag_system.retrieve(case['query'], k=10)\n",
        "            retrieved_ids = [doc['id'] for doc in retrieved]\n",
        "            relevant_ids = case['relevant_doc_ids']\n",
        "            \n",
        "            # Recall: % of relevant docs retrieved\n",
        "            recall = len(set(retrieved_ids) & set(relevant_ids)) / len(relevant_ids)\n",
        "            recalls.append(recall)\n",
        "            \n",
        "            # Precision: % of retrieved docs that are relevant\n",
        "            precision = len(set(retrieved_ids) & set(relevant_ids)) / len(retrieved_ids)\n",
        "            precisions.append(precision)\n",
        "            \n",
        "            # MRR: 1 / rank of first relevant doc\n",
        "            for i, doc_id in enumerate(retrieved_ids, 1):\n",
        "                if doc_id in relevant_ids:\n",
        "                    mrrs.append(1 / i)\n",
        "                    break\n",
        "            else:\n",
        "                mrrs.append(0)\n",
        "        \n",
        "        return {\n",
        "            'recall@10': np.mean(recalls),\n",
        "            'precision@10': np.mean(precisions),\n",
        "            'mrr': np.mean(mrrs),\n",
        "        }\n",
        "    \n",
        "    def evaluate_generation(self, rag_system) -> dict:\n",
        "        '''Measure generation quality using LLM-as-judge'''\n",
        "        \n",
        "        relevance_scores = []\n",
        "        faithfulness_scores = []\n",
        "        groundedness_scores = []\n",
        "        \n",
        "        for case in self.test_dataset:\n",
        "            # Generate answer\n",
        "            retrieved_docs = rag_system.retrieve(case['query'], k=5)\n",
        "            answer = rag_system.generate(case['query'], retrieved_docs)\n",
        "            \n",
        "            # Evaluate relevance (does answer address question?)\n",
        "            relevance = self._evaluate_relevance(case['query'], answer)\n",
        "            relevance_scores.append(relevance)\n",
        "            \n",
        "            # Evaluate faithfulness (is answer supported by docs?)\n",
        "            faithfulness = self._evaluate_faithfulness(answer, retrieved_docs)\n",
        "            faithfulness_scores.append(faithfulness)\n",
        "            \n",
        "            # Evaluate groundedness (no hallucinations)\n",
        "            groundedness = self._evaluate_groundedness(\n",
        "                answer, \n",
        "                retrieved_docs, \n",
        "                case['ground_truth_answer']\n",
        "            )\n",
        "            groundedness_scores.append(groundedness)\n",
        "        \n",
        "        return {\n",
        "            'answer_relevance': np.mean(relevance_scores),\n",
        "            'answer_faithfulness': np.mean(faithfulness_scores),\n",
        "            'answer_groundedness': np.mean(groundedness_scores),\n",
        "        }\n",
        "    \n",
        "    def _evaluate_relevance(self, query: str, answer: str) -> float:\n",
        "        '''Use LLM to judge if answer is relevant to query'''\n",
        "        prompt = f'''\n",
        "        Query: {query}\n",
        "        Answer: {answer}\n",
        "        \n",
        "        Is the answer relevant to the query? Rate 0.0 to 1.0.\n",
        "        Consider:\n",
        "        - Does it address the question asked?\n",
        "        - Is it on-topic?\n",
        "        - Does it provide useful information?\n",
        "        \n",
        "        Return ONLY a number between 0.0 and 1.0.\n",
        "        '''\n",
        "        \n",
        "        score_str = llm.generate(prompt, temperature=0.0)\n",
        "        return float(score_str.strip())\n",
        "    \n",
        "    def _evaluate_faithfulness(self, answer: str, docs: List[dict]) -> float:\n",
        "        '''Check if answer claims are supported by retrieved docs'''\n",
        "        prompt = f'''\n",
        "        Retrieved Documents:\n",
        "        {chr(10).join([f\"[{i+1}] {d['text']}\" for i, d in enumerate(docs)])}\n",
        "        \n",
        "        Answer: {answer}\n",
        "        \n",
        "        Are all claims in the answer supported by the documents?\n",
        "        Rate 0.0 (unsupported) to 1.0 (fully supported).\n",
        "        \n",
        "        Return ONLY a number between 0.0 and 1.0.\n",
        "        '''\n",
        "        \n",
        "        score_str = llm.generate(prompt, temperature=0.0)\n",
        "        return float(score_str.strip())\n",
        "    \n",
        "    def _evaluate_groundedness(self, answer: str, docs: List[dict], ground_truth: str) -> float:\n",
        "        '''Check for hallucinations by comparing to ground truth'''\n",
        "        # Extract claims from answer\n",
        "        answer_claims = self._extract_claims(answer)\n",
        "        gt_claims = self._extract_claims(ground_truth)\n",
        "        \n",
        "        # Check if answer adds unsupported claims\n",
        "        hallucinated_claims = []\n",
        "        for claim in answer_claims:\n",
        "            if not self._is_supported(claim, docs) and claim not in gt_claims:\n",
        "                hallucinated_claims.append(claim)\n",
        "        \n",
        "        # Score: 1.0 if no hallucinations, decreases with each hallucination\n",
        "        score = 1.0 - (len(hallucinated_claims) / max(len(answer_claims), 1))\n",
        "        return max(0.0, score)\n",
        "    \n",
        "    def _extract_claims(self, text: str) -> List[str]:\n",
        "        '''Extract factual claims from text'''\n",
        "        # Use LLM to extract claims\n",
        "        prompt = f'''\n",
        "        Extract atomic factual claims from this text:\n",
        "        {text}\n",
        "        \n",
        "        Return as a JSON list of strings.\n",
        "        '''\n",
        "        response = llm.generate(prompt, temperature=0.0)\n",
        "        return json.loads(response)\n",
        "    \n",
        "    def _is_supported(self, claim: str, docs: List[dict]) -> bool:\n",
        "        '''Check if claim is supported by documents'''\n",
        "        docs_text = '\\\\n\\\\n'.join([d['text'] for d in docs])\n",
        "        \n",
        "        prompt = f'''\n",
        "        Documents:\n",
        "        {docs_text}\n",
        "        \n",
        "        Claim: {claim}\n",
        "        \n",
        "        Is this claim supported by the documents? Answer: yes or no\n",
        "        '''\n",
        "        \n",
        "        response = llm.generate(prompt, temperature=0.0).lower()\n",
        "        return 'yes' in response\n",
        "    \n",
        "    def run_full_evaluation(self, rag_system) -> RAGMetrics:\n",
        "        '''Run complete evaluation suite'''\n",
        "        retrieval_metrics = self.evaluate_retrieval(rag_system)\n",
        "        generation_metrics = self.evaluate_generation(rag_system)\n",
        "        \n",
        "        # Measure operational metrics\n",
        "        latencies = []\n",
        "        costs = []\n",
        "        \n",
        "        for case in self.test_dataset[:100]:  # Sample for latency measurement\n",
        "            start = time.time()\n",
        "            result = rag_system.query(case['query'])\n",
        "            latencies.append((time.time() - start) * 1000)\n",
        "            costs.append(result['cost'])\n",
        "        \n",
        "        return RAGMetrics(\n",
        "            retrieval_recall=retrieval_metrics['recall@10'],\n",
        "            retrieval_precision=retrieval_metrics['precision@10'],\n",
        "            retrieval_mrr=retrieval_metrics['mrr'],\n",
        "            retrieval_ndcg=0.0,  # Implement if needed\n",
        "            \n",
        "            answer_relevance=generation_metrics['answer_relevance'],\n",
        "            answer_faithfulness=generation_metrics['answer_faithfulness'],\n",
        "            answer_groundedness=generation_metrics['answer_groundedness'],\n",
        "            answer_completeness=0.0,  # Implement if needed\n",
        "            \n",
        "            user_satisfaction=None,  # Only available online\n",
        "            task_completion_rate=None,\n",
        "            follow_up_question_rate=None,\n",
        "            \n",
        "            latency_p50_ms=np.percentile(latencies, 50),\n",
        "            latency_p95_ms=np.percentile(latencies, 95),\n",
        "            cost_per_query=np.mean(costs),\n",
        "            \n",
        "            pii_leakage_detected=False,\n",
        "            injection_attempt_blocked=False,\n",
        "        )\n",
        "```\n",
        "\n",
        "**3. Online Evaluation (Production Monitoring):**\n",
        "\n",
        "```python\n",
        "class OnlineRAGMonitor:\n",
        "    '''Real-time production monitoring'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics_buffer = []\n",
        "        self.baseline_metrics = self.load_baseline()\n",
        "    \n",
        "    def log_query(self, query: str, answer: str, retrieved_docs: List[dict], \n",
        "                  latency_ms: float, cost: float, user_feedback: Optional[dict] = None):\n",
        "        '''Log every production query for analysis'''\n",
        "        \n",
        "        self.metrics_buffer.append({\n",
        "            'timestamp': datetime.utcnow().isoformat(),\n",
        "            'query': query,\n",
        "            'answer': answer,\n",
        "            'num_docs_retrieved': len(retrieved_docs),\n",
        "            'latency_ms': latency_ms,\n",
        "            'cost': cost,\n",
        "            'user_feedback': user_feedback,\n",
        "        })\n",
        "        \n",
        "        # Flush to database every 100 queries\n",
        "        if len(self.metrics_buffer) >= 100:\n",
        "            self.flush_metrics()\n",
        "    \n",
        "    def detect_regression(self, window_minutes: int = 60) -> Optional[dict]:\n",
        "        '''Detect quality regressions in real-time'''\n",
        "        \n",
        "        # Get recent metrics\n",
        "        recent_metrics = self.get_recent_metrics(window_minutes)\n",
        "        \n",
        "        # Compare to baseline\n",
        "        alerts = []\n",
        "        \n",
        "        # Check latency regression\n",
        "        current_p95 = np.percentile([m['latency_ms'] for m in recent_metrics], 95)\n",
        "        if current_p95 > self.baseline_metrics['latency_p95'] * 1.2:  # 20% degradation\n",
        "            alerts.append({\n",
        "                'type': 'latency_regression',\n",
        "                'current': current_p95,\n",
        "                'baseline': self.baseline_metrics['latency_p95'],\n",
        "                'severity': 'high'\n",
        "            })\n",
        "        \n",
        "        # Check user satisfaction regression\n",
        "        with_feedback = [m for m in recent_metrics if m['user_feedback']]\n",
        "        if len(with_feedback) >= 20:  # Need minimum sample size\n",
        "            satisfaction = np.mean([\n",
        "                1.0 if m['user_feedback'].get('thumbs_up') else 0.0\n",
        "                for m in with_feedback\n",
        "            ])\n",
        "            \n",
        "            if satisfaction < self.baseline_metrics['user_satisfaction'] - 0.1:  # 10% drop\n",
        "                alerts.append({\n",
        "                    'type': 'satisfaction_regression',\n",
        "                    'current': satisfaction,\n",
        "                    'baseline': self.baseline_metrics['user_satisfaction'],\n",
        "                    'severity': 'critical'\n",
        "                })\n",
        "        \n",
        "        # Check cost spike\n",
        "        avg_cost = np.mean([m['cost'] for m in recent_metrics])\n",
        "        if avg_cost > self.baseline_metrics['avg_cost'] * 1.5:  # 50% increase\n",
        "            alerts.append({\n",
        "                'type': 'cost_spike',\n",
        "                'current': avg_cost,\n",
        "                'baseline': self.baseline_metrics['avg_cost'],\n",
        "                'severity': 'medium'\n",
        "            })\n",
        "        \n",
        "        return alerts if alerts else None\n",
        "    \n",
        "    def run_shadow_evaluation(self, sample_rate: float = 0.01):\n",
        "        '''Continuously evaluate random sample in production'''\n",
        "        \n",
        "        for query_data in self.stream_queries():\n",
        "            if random.random() < sample_rate:\n",
        "                # Run full evaluation on this query\n",
        "                eval_result = self.evaluate_single_query(\n",
        "                    query_data['query'],\n",
        "                    query_data['answer'],\n",
        "                    query_data['retrieved_docs']\n",
        "                )\n",
        "                \n",
        "                # Store for analysis\n",
        "                self.store_evaluation(eval_result)\n",
        "                \n",
        "                # Check for anomalies\n",
        "                if eval_result['faithfulness'] < 0.7:  # Threshold\n",
        "                    self.alert_low_quality(query_data, eval_result)\n",
        "```\n",
        "\n",
        "**4. Pre-Deployment Regression Testing:**\n",
        "\n",
        "```python\n",
        "class RegressionTestSuite:\n",
        "    '''Catch regressions before deployment'''\n",
        "    \n",
        "    def __init__(self, golden_dataset_path: str):\n",
        "        # Golden dataset: curated high-quality examples\n",
        "        self.golden_dataset = self.load_golden_dataset(golden_dataset_path)\n",
        "        self.baseline_results = self.load_baseline_results()\n",
        "    \n",
        "    def run_regression_tests(self, new_rag_system) -> dict:\n",
        "        '''Run before each deployment'''\n",
        "        \n",
        "        results = {\n",
        "            'passed': [],\n",
        "            'failed': [],\n",
        "            'degraded': []\n",
        "        }\n",
        "        \n",
        "        for test_case in self.golden_dataset:\n",
        "            # Run new system\n",
        "            new_answer = new_rag_system.query(test_case['query'])\n",
        "            \n",
        "            # Compare to baseline\n",
        "            baseline_answer = self.baseline_results[test_case['id']]\n",
        "            \n",
        "            # Exact match test (for critical queries)\n",
        "            if test_case.get('exact_match_required'):\n",
        "                if new_answer == baseline_answer:\n",
        "                    results['passed'].append(test_case['id'])\n",
        "                else:\n",
        "                    results['failed'].append({\n",
        "                        'id': test_case['id'],\n",
        "                        'reason': 'exact_match_failed',\n",
        "                        'expected': baseline_answer,\n",
        "                        'got': new_answer\n",
        "                    })\n",
        "            \n",
        "            # Semantic similarity test\n",
        "            else:\n",
        "                similarity = self.compute_semantic_similarity(new_answer, baseline_answer)\n",
        "                \n",
        "                if similarity >= 0.95:\n",
        "                    results['passed'].append(test_case['id'])\n",
        "                elif similarity >= 0.85:\n",
        "                    results['degraded'].append({\n",
        "                        'id': test_case['id'],\n",
        "                        'similarity': similarity,\n",
        "                        'warning': 'slight_degradation'\n",
        "                    })\n",
        "                else:\n",
        "                    results['failed'].append({\n",
        "                        'id': test_case['id'],\n",
        "                        'reason': 'semantic_drift',\n",
        "                        'similarity': similarity\n",
        "                    })\n",
        "        \n",
        "        # Pass/fail criteria\n",
        "        pass_rate = len(results['passed']) / len(self.golden_dataset)\n",
        "        degradation_rate = len(results['degraded']) / len(self.golden_dataset)\n",
        "        \n",
        "        return {\n",
        "            'passed': pass_rate >= 0.95,  # 95% pass rate required\n",
        "            'pass_rate': pass_rate,\n",
        "            'degradation_rate': degradation_rate,\n",
        "            'failures': results['failed'],\n",
        "            'degradations': results['degraded']\n",
        "        }\n",
        "    \n",
        "    def run_ab_test(self, new_system, traffic_percentage: float = 0.05):\n",
        "        '''Gradual rollout with A/B testing'''\n",
        "        \n",
        "        # Route 5% of traffic to new system\n",
        "        # Compare metrics between old and new\n",
        "        \n",
        "        ab_results = {\n",
        "            'system_a': {'queries': [], 'metrics': []},\n",
        "            'system_b': {'queries': [], 'metrics': []}\n",
        "        }\n",
        "        \n",
        "        # Collect data for 24 hours\n",
        "        # ...\n",
        "        \n",
        "        # Statistical significance test\n",
        "        from scipy import stats\n",
        "        \n",
        "        satisfaction_a = [m['user_satisfaction'] for m in ab_results['system_a']['metrics']]\n",
        "        satisfaction_b = [m['user_satisfaction'] for m in ab_results['system_b']['metrics']]\n",
        "        \n",
        "        t_stat, p_value = stats.ttest_ind(satisfaction_a, satisfaction_b)\n",
        "        \n",
        "        if p_value < 0.05 and np.mean(satisfaction_b) > np.mean(satisfaction_a):\n",
        "            return {'decision': 'deploy', 'improvement': np.mean(satisfaction_b) - np.mean(satisfaction_a)}\n",
        "        elif p_value < 0.05 and np.mean(satisfaction_b) < np.mean(satisfaction_a):\n",
        "            return {'decision': 'rollback', 'degradation': np.mean(satisfaction_a) - np.mean(satisfaction_b)}\n",
        "        else:\n",
        "            return {'decision': 'inconclusive', 'p_value': p_value}\n",
        "```\n",
        "\n",
        "**5. Complete Deployment Pipeline:**\n",
        "\n",
        "```\n",
        "1. Offline Evaluation (Golden Dataset)\n",
        "   \u251c\u2500 Run regression tests\n",
        "   \u251c\u2500 Check pass rate >= 95%\n",
        "   \u2514\u2500 If passed \u2192 Continue\n",
        "\n",
        "2. Staging Environment (Shadow Mode)\n",
        "   \u251c\u2500 Mirror 100% of production traffic\n",
        "   \u251c\u2500 Compare metrics to production system\n",
        "   \u2514\u2500 If no degradation \u2192 Continue\n",
        "\n",
        "3. Canary Deployment (5% traffic)\n",
        "   \u251c\u2500 Route 5% real traffic to new system\n",
        "   \u251c\u2500 Monitor for 24 hours\n",
        "   \u251c\u2500 Check: latency, cost, satisfaction\n",
        "   \u2514\u2500 If metrics good \u2192 Continue\n",
        "\n",
        "4. Gradual Rollout\n",
        "   \u251c\u2500 5% \u2192 25% \u2192 50% \u2192 100%\n",
        "   \u251c\u2500 Monitor at each stage\n",
        "   \u2514\u2500 Automatic rollback if regression detected\n",
        "\n",
        "5. Post-Deployment Monitoring\n",
        "   \u251c\u2500 Continuous shadow evaluation (1% sample)\n",
        "   \u251c\u2500 Real-time regression detection\n",
        "   \u2514\u2500 Weekly deep-dive analysis\n",
        "```\n",
        "\n",
        "**Key Metrics Dashboard:**\n",
        "- Retrieval Recall/Precision (updated daily)\n",
        "- Answer Faithfulness (sampled 1%)\n",
        "- User Satisfaction (from feedback)\n",
        "- P95 Latency, Cost/query\n",
        "- Regression alerts (real-time)\n",
        "\n",
        "**SLOs (Service Level Objectives):**\n",
        "- Retrieval Recall: >= 90%\n",
        "- Answer Faithfulness: >= 85%\n",
        "- User Satisfaction: >= 80%\n",
        "- P95 Latency: <= 200ms\n",
        "- Cost per query: <= $0.01\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, qa in enumerate(interview_questions_rag_part1, 1):\n",
        "    print(f\"\\n{'=' * 100}\")\n",
        "    print(f\"RAG SYSTEMS - Q{i} [{qa['level']} Level]\")\n",
        "    print('=' * 100)\n",
        "    print(f\"\\n{qa['question']}\\n\")\n",
        "    print(\"ANSWER:\")\n",
        "    print(qa['answer'])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"MODULE 2: RAG SYSTEMS - KEY TAKEAWAYS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "summary = {\n",
        "    \"Chunking Strategies\": [\n",
        "        \"Sentence-window retrieval: Retrieve small chunks, return with context (40% improvement)\",\n",
        "        \"Hierarchical chunking: Paragraph-level retrieval, section-level context (50% improvement)\",\n",
        "        \"Smart overlap: Semantic coherence at boundaries prevents information loss\",\n",
        "        \"High retrieval recall \u2260 good answers; context completeness matters more\",\n",
        "    ],\n",
        "    \"Retrieval Optimization\": [\n",
        "        \"FAISS IVF indexing: 20x speedup for large corpora (1.5M+ vectors)\",\n",
        "        \"Metadata co-location: Avoid separate DB fetches, <5ms vs 100ms\",\n",
        "        \"Embedding caching: 35% cache hit rate saves 50ms per cached query\",\n",
        "        \"Two-tier retrieval: Filter by metadata first, reduces search space 15x\",\n",
        "        \"Profile before optimizing: 80% of gains come from fixing the bottleneck\",\n",
        "    ],\n",
        "    \"Hybrid Search\": [\n",
        "        \"BM25: Excels at keyword matching (technical terms, exact phrases)\",\n",
        "        \"Semantic: Excels at concept matching (paraphrases, synonyms)\",\n",
        "        \"Alpha tuning: 0.3 (keyword-heavy), 0.5 (balanced), 0.7 (concept-heavy)\",\n",
        "        \"Reciprocal rank fusion: Simple and effective for combining scores\",\n",
        "    ],\n",
        "    \"Re-ranking & MMR\": [\n",
        "        \"Two-stage retrieval: Fast bi-encoder (top-50) \u2192 Slow cross-encoder (top-5)\",\n",
        "        \"Re-ranking improves precision by 10-30% with 20-50ms latency cost\",\n",
        "        \"MMR (Maximal Marginal Relevance): Reduces redundancy, improves coverage\",\n",
        "        \"Lambda parameter: 0.7-0.8 for good relevance/diversity balance\",\n",
        "    ],\n",
        "    \"Evaluation Framework\": [\n",
        "        \"Retrieval metrics: Recall, Precision, MRR, NDCG\",\n",
        "        \"Generation metrics: Relevance, Faithfulness, Groundedness, Completeness\",\n",
        "        \"Use LLM-as-judge for generation quality (correlation with human: 0.85+)\",\n",
        "        \"Offline: Golden dataset with 95% pass rate before deployment\",\n",
        "        \"Online: Shadow evaluation (1% sample), real-time regression detection\",\n",
        "        \"Deployment: Regression tests \u2192 Shadow \u2192 Canary (5%) \u2192 Gradual (100%)\",\n",
        "    ],\n",
        "    \"Production Principles\": [\n",
        "        \"Measure everything: Latency (P50/P95/P99), cost, quality, user satisfaction\",\n",
        "        \"Catch regressions early: Pre-deployment tests + real-time monitoring\",\n",
        "        \"Optimize for bottlenecks: Profile first, optimize second\",\n",
        "        \"Balance tradeoffs: Latency vs accuracy, cost vs quality\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "for section, points in summary.items():\n",
        "    print(f\"\\n{section}:\")\n",
        "    for point in points:\n",
        "        print(f\"  - {point}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"\\nINTERVIEW QUESTIONS SUMMARY:\")\n",
        "print(\"  - Chunking & Context: Incomplete answers despite high recall\")\n",
        "print(\"  - Performance: FAISS optimization from 800ms to <200ms P95\")\n",
        "print(\"  - Evaluation: Complete framework with offline and online evaluation\")\n",
        "print(\"  Total: 3 advanced questions (2 Senior, 1 Staff level)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"  1. Implement sentence-window retrieval for better context\")\n",
        "print(\"  2. Profile your retrieval pipeline and optimize bottlenecks\")\n",
        "print(\"  3. Build golden dataset for regression testing (100+ examples)\")\n",
        "print(\"  4. Set up shadow evaluation for 1% of production traffic\")\n",
        "print(\"  5. Move to Module 3: LangChain (chains, agents, evaluation)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section 2: Hybrid Search - Combining BM25 and Dense Vectors\n\nHybrid search improves retrieval by combining:\n- **BM25 (Lexical)**: Keyword matching, good for exact terms\n- **Dense Vectors (Semantic)**: Meaning-based, good for paraphrases\n- **Fusion**: Reciprocal Rank Fusion (RRF) or weighted combination"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from rank_bm25 import BM25Okapi\nimport numpy as np\nfrom typing import List, Tuple, Dict\nfrom sentence_transformers import SentenceTransformer\n\nclass HybridRetriever:\n    '''Combines BM25 and dense retrieval for better results'''\n    \n    def __init__(self, embedding_model='all-MiniLM-L6-v2'):\n        self.embedding_model = SentenceTransformer(embedding_model)\n        self.documents = []\n        self.embeddings = None\n        self.bm25 = None\n    \n    def index(self, documents: List[str]):\n        '''Index documents for both BM25 and vector search'''\n        self.documents = documents\n        \n        # BM25 index\n        tokenized = [doc.lower().split() for doc in documents]\n        self.bm25 = BM25Okapi(tokenized)\n        \n        # Vector index\n        print(f'Encoding {len(documents)} documents...')\n        self.embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n    \n    def retrieve_bm25(self, query: str, top_k=5) -> List[Tuple[int, float]]:\n        '''BM25 retrieval'''\n        tokenized_query = query.lower().split()\n        scores = self.bm25.get_scores(tokenized_query)\n        top_indices = np.argsort(scores)[::-1][:top_k]\n        return [(idx, scores[idx]) for idx in top_indices]\n    \n    def retrieve_vector(self, query: str, top_k=5) -> List[Tuple[int, float]]:\n        '''Dense vector retrieval'''\n        query_embedding = self.embedding_model.encode([query])[0]\n        \n        # Cosine similarity\n        similarities = np.dot(self.embeddings, query_embedding) / (\n            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)\n        )\n        \n        top_indices = np.argsort(similarities)[::-1][:top_k]\n        return [(idx, similarities[idx]) for idx in top_indices]\n    \n    def retrieve_hybrid(self, query: str, top_k=5, alpha=0.5) -> List[Tuple[int, float, Dict]]:\n        '''Hybrid retrieval with weighted fusion\n        \n        Args:\n            alpha: Weight for BM25 (0=all vector, 1=all BM25)\n        '''\n        # Get results from both\n        bm25_results = self.retrieve_bm25(query, top_k * 2)\n        vector_results = self.retrieve_vector(query, top_k * 2)\n        \n        # Normalize scores to [0,1]\n        bm25_scores = np.array([s for _, s in bm25_results])\n        if bm25_scores.max() > 0:\n            bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n        \n        vector_scores = np.array([s for _, s in vector_results])\n        vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min() + 1e-10)\n        \n        # Combine scores\n        combined = {}\n        for (idx, _), norm_score in zip(bm25_results, bm25_scores):\n            combined[idx] = {'bm25': norm_score, 'vector': 0.0}\n        \n        for (idx, _), norm_score in zip(vector_results, vector_scores):\n            if idx in combined:\n                combined[idx]['vector'] = norm_score\n            else:\n                combined[idx] = {'bm25': 0.0, 'vector': norm_score}\n        \n        # Calculate combined score\n        for idx in combined:\n            combined[idx]['combined'] = alpha * combined[idx]['bm25'] + (1 - alpha) * combined[idx]['vector']\n        \n        # Sort and return top-k\n        ranked = sorted(combined.items(), key=lambda x: x[1]['combined'], reverse=True)[:top_k]\n        return [(idx, scores['combined'], scores) for idx, scores in ranked]\n    \n    def retrieve_rrf(self, query: str, top_k=5, k=60) -> List[Tuple[int, float]]:\n        '''Reciprocal Rank Fusion (RRF)\n        \n        RRF formula: score = sum(1 / (k + rank_i))\n        where rank_i is the rank in result set i\n        '''\n        bm25_results = self.retrieve_bm25(query, top_k * 2)\n        vector_results = self.retrieve_vector(query, top_k * 2)\n        \n        # Calculate RRF scores\n        rrf_scores = {}\n        \n        # BM25 ranks\n        for rank, (idx, _) in enumerate(bm25_results):\n            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)\n        \n        # Vector ranks\n        for rank, (idx, _) in enumerate(vector_results):\n            rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)\n        \n        # Sort by RRF score\n        ranked = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n        return ranked\n\n# Test hybrid retrieval\ntest_docs = [\n    'Python is a high-level programming language with dynamic typing.',\n    'Machine learning models require large datasets for training.',\n    'The quick brown fox jumps over the lazy dog.',\n    'Natural language processing enables computers to understand human language.',\n    'Deep learning uses neural networks with multiple layers.',\n    'Python is widely used for data science and ML applications.',\n    'Vector databases store embeddings for similarity search.',\n    'BM25 is a ranking function used in information retrieval.',\n    'Transformers are the architecture behind modern LLMs.',\n    'RAG combines retrieval with generation for better accuracy.',\n]\n\nprint('Initializing Hybrid Retriever...')\nretriever = HybridRetriever()\nretriever.index(test_docs)\n\ntest_queries = [\n    'Python machine learning',\n    'neural networks NLP',\n    'information retrieval ranking',\n]\n\nprint('\\n' + '=' * 90)\nfor query in test_queries:\n    print(f'\\nQuery: {query}')\n    print('-' * 90)\n    \n    # Compare methods\n    bm25_only = retriever.retrieve_hybrid(query, top_k=3, alpha=1.0)\n    vector_only = retriever.retrieve_hybrid(query, top_k=3, alpha=0.0)\n    hybrid = retriever.retrieve_hybrid(query, top_k=3, alpha=0.5)\n    rrf = retriever.retrieve_rrf(query, top_k=3)\n    \n    print('BM25 Only:')\n    for rank, (idx, score, _) in enumerate(bm25_only, 1):\n        print(f'  {rank}. [{score:.3f}] {test_docs[idx][:60]}...')\n    \n    print('\\nVector Only:')\n    for rank, (idx, score, _) in enumerate(vector_only, 1):\n        print(f'  {rank}. [{score:.3f}] {test_docs[idx][:60]}...')\n    \n    print('\\nHybrid (50/50):')\n    for rank, (idx, score, _) in enumerate(hybrid, 1):\n        print(f'  {rank}. [{score:.3f}] {test_docs[idx][:60]}...')\n    \n    print('\\nRRF:')\n    for rank, (idx, score) in enumerate(rrf, 1):\n        print(f'  {rank}. [{score:.3f}] {test_docs[idx][:60]}...')\n\nprint('\\n' + '=' * 90)\nprint('KEY INSIGHT: Hybrid search often outperforms single method by combining strengths')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Interview Questions: RAG Systems\n\n### For Senior/Staff Engineers\n\nThese questions test production RAG design and optimization."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "rag_interview_questions = [\n    {\n        'level': 'Senior',\n        'question': 'Your RAG system has 40% precision but 90% recall. Users complain about too many irrelevant chunks. Walk through your debugging and optimization process.',\n        'answer': '''\n**Problem Analysis:**\nHigh recall but low precision means we're retrieving too many chunks, including many irrelevant ones.\n\n**Root Causes:**\n1. **top_k too high**: Retrieving 10+ chunks when only 2-3 are relevant\n2. **Poor embedding model**: Not capturing semantic nuances\n3. **Bad chunking**: Chunks too small or too large, losing context\n4. **No reranking**: Taking vector similarity at face value\n5. **Retrieval threshold too low**: Including low-confidence matches\n\n**Systematic Debugging (3-day process):**\n\n**Day 1: Measure Current Performance**\n```python\ndef evaluate_retrieval(test_cases: List[dict]):\n    metrics = {\n        'precision_at_k': [],\n        'recall_at_k': [],\n        'mrr': [],  # Mean Reciprocal Rank\n        'ndcg': []  # Normalized Discounted Cumulative Gain\n    }\n    \n    for case in test_cases:\n        query = case['query']\n        relevant_doc_ids = set(case['relevant_docs'])\n        \n        # Current retrieval\n        retrieved = retriever.retrieve(query, top_k=10)\n        retrieved_ids = [r['doc_id'] for r in retrieved]\n        \n        # Precision@k\n        relevant_retrieved = len(set(retrieved_ids[:k]) & relevant_doc_ids)\n        metrics['precision_at_k'].append(relevant_retrieved / k)\n        \n        # Recall@k\n        metrics['recall_at_k'].append(relevant_retrieved / len(relevant_doc_ids))\n        \n        # MRR: 1 / rank_of_first_relevant\n        for i, doc_id in enumerate(retrieved_ids, 1):\n            if doc_id in relevant_doc_ids:\n                metrics['mrr'].append(1 / i)\n                break\n    \n    return {k: np.mean(v) for k, v in metrics.items()}\n\n# Baseline\nbaseline = evaluate_retrieval(test_set)\nprint(f\"Baseline P@5: {baseline['precision_at_k']:.2%}\")\nprint(f\"Baseline Recall: {baseline['recall_at_k']:.2%}\")\n```\n\n**Day 2: Try Quick Wins**\n\n**Fix 1: Reduce top_k (Immediate, +15% precision)**\n```python\n# Before: top_k=10\n# After: top_k=5, only high-confidence matches\nretrieved = retriever.retrieve(query, top_k=5)\n```\n\n**Fix 2: Add Similarity Threshold (+20% precision)**\n```python\ndef retrieve_with_threshold(query: str, top_k=5, min_similarity=0.7):\n    results = retriever.retrieve(query, top_k=top_k * 2)\n    \n    # Filter by threshold\n    filtered = [r for r in results if r['similarity'] >= min_similarity]\n    \n    # If too few, lower threshold slightly\n    if len(filtered) < 2:\n        filtered = results[:2]  # Always return at least 2\n    \n    return filtered[:top_k]\n```\n\n**Fix 3: Implement Reranking (+30% precision)**\n```python\nfrom sentence_transformers import CrossEncoder\n\nclass Reranker:\n    def __init__(self):\n        # Cross-encoder is more accurate but slower\n        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n    \n    def rerank(self, query: str, documents: List[dict], top_k=5) -> List[dict]:\n        # Over-retrieve with vector search\n        candidates = retriever.retrieve(query, top_k=top_k * 3)\n        \n        # Rerank with cross-encoder\n        pairs = [(query, doc['text']) for doc in candidates]\n        scores = self.model.predict(pairs)\n        \n        # Sort by reranker scores\n        for doc, score in zip(candidates, scores):\n            doc['rerank_score'] = score\n        \n        ranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n        return ranked[:top_k]\n\nreranker = Reranker()\nreranked = reranker.rerank(query, initial_results, top_k=5)\n```\n\n**Day 3: Improve Chunking & Embeddings**\n\n**Fix 4: Better Chunking Strategy (+10% both metrics)**\n```python\n# Before: Fixed 512 tokens\n# After: Semantic chunking with overlap\n\nclass SemanticChunker:\n    def __init__(self, target_size=400, overlap=50):\n        self.target_size = target_size\n        self.overlap = overlap\n    \n    def chunk(self, text: str) -> List[str]:\n        # Split by paragraphs first\n        paragraphs = text.split('\\\\n\\\\n')\n        \n        chunks = []\n        current_chunk = []\n        current_size = 0\n        \n        for para in paragraphs:\n            para_size = len(para)\n            \n            if current_size + para_size > self.target_size and current_chunk:\n                # Create chunk\n                chunks.append('\\\\n\\\\n'.join(current_chunk))\n                \n                # Overlap: keep last paragraph\n                if len(current_chunk) > 1:\n                    current_chunk = current_chunk[-1:]\n                    current_size = len(current_chunk[0])\n                else:\n                    current_chunk = []\n                    current_size = 0\n            \n            current_chunk.append(para)\n            current_size += para_size\n        \n        if current_chunk:\n            chunks.append('\\\\n\\\\n'.join(current_chunk))\n        \n        return chunks\n```\n\n**Fix 5: Use Domain-Specific Embeddings (+15% both metrics)**\n```python\n# Option A: Fine-tune embeddings on your domain\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\n\ndef finetune_embeddings(train_examples: List[tuple]):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    # Create training examples\n    examples = []\n    for query, pos_doc, neg_doc in train_examples:\n        examples.append(InputExample(texts=[query, pos_doc], label=1.0))\n        examples.append(InputExample(texts=[query, neg_doc], label=0.0))\n    \n    # Train\n    train_loss = losses.CosineSimilarityLoss(model)\n    model.fit(\n        train_objectives=[(examples, train_loss)],\n        epochs=3,\n        warmup_steps=100\n    )\n    \n    return model\n\n# Option B: Use larger, better embedding model\n# 'all-mpnet-base-v2' or 'intfloat/e5-large-v2'\n```\n\n**Combined Solution:**\n```python\nclass OptimizedRAG:\n    def __init__(self):\n        self.chunker = SemanticChunker(target_size=400)\n        self.embedder = SentenceTransformer('all-mpnet-base-v2')  # Better model\n        self.reranker = Reranker()\n        self.min_similarity = 0.7\n    \n    def retrieve(self, query: str, top_k=5):\n        # Step 1: Over-retrieve\n        candidates = vector_search(query, top_k=top_k * 3)\n        \n        # Step 2: Filter by threshold\n        filtered = [c for c in candidates if c['similarity'] >= self.min_similarity]\n        \n        # Step 3: Rerank\n        reranked = self.reranker.rerank(query, filtered, top_k=top_k)\n        \n        return reranked\n```\n\n**Expected Results:**\n- Precision@5: 40% \u2192 85% (+45%)\n- Recall: 90% \u2192 75% (slight drop acceptable)\n- User satisfaction: Much higher (fewer irrelevant results)\n\n**Key Takeaway:**\nHigh recall is easy (just retrieve more), but production needs high precision.\nReranking is the most impactful single improvement.\n        ''',\n    },\n    {\n        'level': 'Senior',\n        'question': 'Design a multi-tenant RAG system where Company A cannot see Company B's documents, but some documents are shared across tenants. Include performance considerations.',\n        'answer': '''\n**Multi-Tenant RAG Architecture:**\n\n**1. Data Model:**\n```python\n@dataclass\nclass Document:\n    doc_id: str\n    text: str\n    embedding: np.ndarray\n    visibility: List[str]  # ['tenant_a', 'public']\n    created_by_tenant: str\n    access_level: str  # 'private', 'shared', 'public'\n```\n\n**2. Namespace-Based Isolation:**\n```python\nclass MultiTenantVectorDB:\n    '''Tenant isolation via namespaces + metadata filtering'''\n    \n    def __init__(self):\n        self.client = chromadb.Client()\n        # Option A: One collection per tenant (simple but doesn't scale)\n        # Option B: One collection with metadata filtering (recommended)\n        self.collection = self.client.get_or_create_collection('multi_tenant_docs')\n    \n    def ingest(self, doc: Document, tenant_id: str):\n        '''Ingest with tenant metadata'''\n        metadata = {\n            'tenant_id': tenant_id,\n            'visibility': ','.join(doc.visibility),\n            'access_level': doc.access_level,\n            'doc_id': doc.doc_id,\n        }\n        \n        self.collection.add(\n            ids=[doc.doc_id],\n            embeddings=[doc.embedding.tolist()],\n            documents=[doc.text],\n            metadatas=[metadata]\n        )\n    \n    def retrieve(self, query: str, tenant_id: str, top_k=5) -> List[dict]:\n        '''Retrieve with tenant isolation'''\n        query_embedding = embed(query)\n        \n        # Over-retrieve to account for filtering\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k * 5  # Over-retrieve\n        )\n        \n        # Filter by tenant access\n        filtered = []\n        for i in range(len(results['ids'][0])):\n            metadata = results['metadatas'][0][i]\n            visibility = set(metadata['visibility'].split(','))\n            \n            # Can access if:\n            # 1. Created by this tenant\n            # 2. Explicitly shared with this tenant\n            # 3. Public\n            if (metadata['tenant_id'] == tenant_id or \n                tenant_id in visibility or \n                'public' in visibility):\n                \n                filtered.append({\n                    'text': results['documents'][0][i],\n                    'metadata': metadata,\n                    'similarity': 1 - results['distances'][0][i]\n                })\n                \n                if len(filtered) >= top_k:\n                    break\n        \n        return filtered\n```\n\n**3. Performance Optimization:**\n\n**Problem:** Over-retrieving then filtering is slow at scale.\n\n**Solution A: Index Partitioning**\n```python\nclass PartitionedVectorDB:\n    '''Separate indices per tenant for performance'''\n    \n    def __init__(self):\n        self.tenant_collections = {}  # tenant_id -> collection\n        self.shared_collection = None  # For shared/public docs\n    \n    def get_or_create_tenant_collection(self, tenant_id: str):\n        if tenant_id not in self.tenant_collections:\n            self.tenant_collections[tenant_id] = chromadb.Client().get_or_create_collection(\n                f'tenant_{tenant_id}'\n            )\n        return self.tenant_collections[tenant_id]\n    \n    def retrieve(self, query: str, tenant_id: str, top_k=5):\n        query_embedding = embed(query)\n        \n        # Parallel retrieval from:\n        # 1. Tenant's private collection\n        # 2. Shared collection\n        private_results = self.tenant_collections[tenant_id].query(\n            query_embeddings=[query_embedding],\n            n_results=top_k\n        )\n        \n        shared_results = self.shared_collection.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k,\n            where={'visibility': tenant_id}  # Pre-filter\n        )\n        \n        # Merge and rerank\n        all_results = merge_results(private_results, shared_results)\n        return all_results[:top_k]\n```\n\n**Solution B: Caching with Tenant Isolation**\n```python\nimport hashlib\nfrom functools import lru_cache\n\nclass TenantAwareCache:\n    '''Cache with tenant isolation'''\n    \n    def __init__(self, redis_client):\n        self.redis = redis_client\n        self.ttl = 3600  # 1 hour\n    \n    def cache_key(self, query: str, tenant_id: str) -> str:\n        '''Generate cache key with tenant context'''\n        query_hash = hashlib.sha256(query.encode()).hexdigest()[:16]\n        return f'retrieve:{tenant_id}:{query_hash}'\n    \n    def get(self, query: str, tenant_id: str) -> Optional[List[dict]]:\n        key = self.cache_key(query, tenant_id)\n        cached = self.redis.get(key)\n        if cached:\n            return json.loads(cached)\n        return None\n    \n    def set(self, query: str, tenant_id: str, results: List[dict]):\n        key = self.cache_key(query, tenant_id)\n        self.redis.setex(key, self.ttl, json.dumps(results))\n    \n    def invalidate_tenant(self, tenant_id: str):\n        '''Invalidate all cache for tenant (e.g., after new doc upload)'''\n        pattern = f'retrieve:{tenant_id}:*'\n        keys = self.redis.keys(pattern)\n        if keys:\n            self.redis.delete(*keys)\n```\n\n**4. Audit Logging:**\n```python\nclass TenantAuditLog:\n    '''Track who accessed what documents'''\n    \n    def __init__(self):\n        self.logs = []\n    \n    def log_access(self, tenant_id: str, query: str, retrieved_docs: List[str]):\n        self.logs.append({\n            'timestamp': datetime.utcnow().isoformat(),\n            'tenant_id': tenant_id,\n            'query_hash': hashlib.sha256(query.encode()).hexdigest()[:16],\n            'num_docs': len(retrieved_docs),\n            'doc_ids': retrieved_docs,\n        })\n    \n    def detect_unauthorized_access(self):\n        '''Detect attempts to access other tenant's data'''\n        # Analyze patterns for anomalies\n        pass\n```\n\n**5. Complete System:**\n```python\nclass MultiTenantRAG:\n    def __init__(self):\n        self.vector_db = PartitionedVectorDB()\n        self.cache = TenantAwareCache(redis_client)\n        self.audit_log = TenantAuditLog()\n        self.rate_limiter = TenantRateLimiter()\n    \n    def retrieve(self, query: str, tenant_id: str, user_id: str, top_k=5):\n        # Rate limiting per tenant\n        if not self.rate_limiter.allow(tenant_id):\n            raise RateLimitExceeded()\n        \n        # Check cache\n        cached = self.cache.get(query, tenant_id)\n        if cached:\n            self.audit_log.log_access(tenant_id, query, [d['doc_id'] for d in cached])\n            return cached\n        \n        # Retrieve with tenant isolation\n        results = self.vector_db.retrieve(query, tenant_id, top_k)\n        \n        # Cache results\n        self.cache.set(query, tenant_id, results)\n        \n        # Audit log\n        self.audit_log.log_access(tenant_id, query, [d['doc_id'] for d in results])\n        \n        return results\n```\n\n**Performance Benchmarks:**\n- Latency: < 100ms (with cache), < 500ms (without cache)\n- Throughput: 1000+ queries/sec\n- Cost: $0.001 per query\n\n**Security Checklist:**\n- [x] Tenant isolation via metadata/namespaces\n- [x] Audit logging for compliance\n- [x] Rate limiting per tenant\n- [x] Cache isolation (tenant_id in cache key)\n- [x] No cross-tenant data leakage\n- [x] Shared docs explicitly marked\n        ''',\n    },\n]\n\nfor i, qa in enumerate(rag_interview_questions, 1):\n    print(f'\\n{'=' * 100}')\n    print(f'Q{i} [{qa[\"level\"]} Level]')\n    print('=' * 100)\n    print(f'\\n{qa[\"question\"]}\\n')\n    print('ANSWER:')\n    print(qa['answer'])\n    print()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}